// ==== LEGO START: 01 Imports & App Entry & Environment Wiring ====
//
//  Hal.swift
//  HalChatiOS
//
//  Hal.swift ‚Äî Core Application Source
//  Architecture Overview:
//  - Integrates Apple FoundationModels and MLX frameworks under LLMService.
//  - Uses LEGO-block modular structure (01‚Äì29) for deterministic editing.
//  - Includes on-device inference, streaming UI, and context-managed memory.
//  - MLXWrapper supports Phi-3 and similar models via MLX Swift APIs.
//  - MemoryStore uses SQLite with schema, embeddings, and semantic search.
//
//  - LEGO Index
// 01  Imports & App Entry & Environment Wiring
// 02  ChatMessage, UnifiedSearchContext, MemoryStore (Part 1)
// 03  MemoryStore (Part 2 ‚Äì Schema, Encryption, Stats)
// 04  MemoryStore (Part 3 ‚Äì Storing Turns & Entities)
// 05  MemoryStore (Part 4 ‚Äì Entities, Embeddings, Search)
// 06  MemoryStore (Part 5 ‚Äì Retrieval, Debug, Semantic Search)
// 07  MemoryStore (Part 6 ‚Äì Full Search Flow) & LLMType Enum
// 08  MLXWrapper & LLMService (Foundation + MLX Routing)
// 09  App Entry & iOSChatView (UI Shell)
// 10  ActionsView (Settings, Import/Export, Model Picker)
// 11  ActionsView (Phi-3 Management & Power Tools)
// 12  ActionsView (License & Status Helpers)
// 12.5 SystemPromptEditorView (Power User Tool)
// 13  ChatBubbleView & TimerView (Message UI Components)
// 14  PromptDetailView (Full Prompt & Context Viewer)
// 15  ShareSheet (Export Utility)
// 16  View Extensions (cornerRadius & conditional modifier)
// 17  ChatViewModel (Core Properties & Init)
// 18  ChatViewModel (Memory Stats & Summarization)
// 19  ChatViewModel (Phi-3 MLX Integration)
// 20  ChatViewModel (Prompt History Builder)
// 21  ChatViewModel (Send Message Flow)
// 22  ChatViewModel (Short-Term Memory Helpers)
// 23  ChatViewModel (Repetition Removal Utility)
// 24  ChatViewModel (Conversation & Database Reset)
// 25  ChatVM ‚Äî Export Chat History
// 26  DocumentPicker (UIKit Bridge)
// 27  DocumentImportManager (Ingest & Entities)
// 28  Import Models (ProcessedDocument & Summary)
// 29  MLX Model Downloader (Singleton)
// 30  Model Catalog Service (Hugging Face Integration)
//

import SwiftUI
import Foundation
import Combine
import Observation
import FoundationModels // Keep for FoundationModels option
import UniformTypeIdentifiers // For file types in document import
import SQLite3 // For MemoryStore - Direct C API for consistency with Mac version
import NaturalLanguage // For entity extraction and NLEmbedding
import PDFKit // For PDF document processing
import MLX // Import MLX framework (conceptual, requires actual framework link)
import MLXLLM
import Hub
import MLXLMCommon // FIXED: Added missing import for proper MLX API access
import Tokenizers // FIXED: Added missing import for tokenizer decode method

// MARK: - Hub Extension for MLX Model Downloads
extension HubApi {
    /// Default HubApi instance configured for iOS cache directory
    static let `default` = HubApi(
        downloadBase: URL.cachesDirectory.appending(path: "huggingface")
    )
}

// Add @preconcurrency import for Foundation to help with Swift 6 concurrency warnings
@preconcurrency import Foundation

// MARK: - Named Entity Support
struct NamedEntity: Codable, Hashable {
    let text: String
    let type: EntityType

    enum EntityType: String, Codable, CaseIterable {
        case person = "person"
        case place = "place"
        case organization = "organization"
        case other = "other"

        var displayName: String {
            switch self {
            case .person: return "Person"
            case .place: return "Place"
            case .organization: return "Organization"
            case .other: return "Other"
            }
        }
    }
}

// MARK: - Type Definitions for Unified Memory System (from Hal10000App.swift)
enum ContentSourceType: String, CaseIterable, Codable {
    case conversation = "conversation"
    case document = "document"
    case webpage = "webpage" // Not used in this simplified version, but kept for consistency
    case email = "email"     // Not used in this simplified version, but kept for consistency

    var displayName: String {
        switch self {
        case .conversation: return "Conversation"
        case .document: return "Document"
        case .webpage: return "Web Page"
        case .email: return "Email"
        }
    }

    var icon: String {
        switch self {
        case .conversation: return "üí¨"
        case .document: return "üìÑ"
        case .webpage: return "üåê"
        case .email: return "üìß"
        }
    }
}

// MARK: - Enhanced Search Context with Entity Support (from Hal10000App.swift)
struct UnifiedSearchResult: Identifiable, Hashable, Codable { // Made Codable
    let id: UUID // Changed to let, and initialized in init
    let content: String
    var relevance: Double
    let source: String
    var isEntityMatch: Bool
    var filePath: String? // NEW: To store the file path for deep linking

    init(id: UUID = UUID(), content: String, relevance: Double, source: String, isEntityMatch: Bool, filePath: String? = nil) {
        self.id = id
        self.content = content
        self.relevance = relevance
        self.source = source
        self.isEntityMatch = isEntityMatch
        self.filePath = filePath
    }
}
// ==== LEGO END: 01 Imports & App Entry & Environment Wiring ====


// ==== LEGO START: 02 ChatMessage, UnifiedSearchContext, MemoryStore (Part 1) ====

// MARK: - Token Breakdown Structure
struct TokenBreakdown: Equatable {
    let systemTokens: Int
    let summaryTokens: Int
    let ragTokens: Int
    let shortTermTokens: Int
    let userInputTokens: Int
    let completionTokens: Int
    let contextWindow: Int  // Store actual context window size from model
    
    var totalPromptTokens: Int {
        return systemTokens + summaryTokens + ragTokens + shortTermTokens + userInputTokens
    }
    
    var totalTokens: Int {
        return totalPromptTokens + completionTokens
    }
    
    var contextWindowSize: Int {
        return contextWindow
    }
    
    var percentageUsed: Double {
        return (Double(totalTokens) / Double(contextWindowSize)) * 100.0
    }
}

// MARK: - Simple ChatMessage Model
struct ChatMessage: Identifiable, Equatable { // Added Equatable for ForEach
    let id: UUID
    var content: String // Changed to var for streaming updates
    let isFromUser: Bool
    let timestamp: Date
    var isPartial: Bool // Changed to var for streaming updates
    var thinkingDuration: TimeInterval? // Changed to var for mutability
    var fullPromptUsed: String? // NEW: To store the exact prompt for Hal's response
    var usedContextSnippets: [UnifiedSearchResult]? // NEW: To store the RAG snippets used
    var tokenBreakdown: TokenBreakdown? // NEW: To store token usage breakdown

    init(id: UUID = UUID(), content: String, isFromUser: Bool, timestamp: Date = Date(), isPartial: Bool = false, thinkingDuration: TimeInterval? = nil, fullPromptUsed: String? = nil, usedContextSnippets: [UnifiedSearchResult]? = nil, tokenBreakdown: TokenBreakdown? = nil) {
        self.id = id
        self.content = content
        self.isFromUser = isFromUser
        self.timestamp = timestamp
        self.isPartial = isPartial
        self.thinkingDuration = thinkingDuration
        self.fullPromptUsed = fullPromptUsed
        self.usedContextSnippets = usedContextSnippets
        self.tokenBreakdown = tokenBreakdown
    }
}

// MARK: - Simplified Search Context Model (Entity-Free, for ChatViewModel UI)
struct UnifiedSearchContext {
    let conversationSnippets: [String]
    let documentSnippets: [String]
    let relevanceScores: [Double]
    let totalTokens: Int

    var hasContent: Bool {
        return !conversationSnippets.isEmpty || !documentSnippets.isEmpty
    }

    var totalSnippets: Int {
        return conversationSnippets.count + documentSnippets.count
    }
}

// MARK: - Memory Store with Persistent Database Connection (Aligned with Hal10000App.swift)
class MemoryStore: ObservableObject {
    static let shared = MemoryStore() // Singleton pattern

    @Published var isEnabled: Bool = true
    @AppStorage("relevanceThreshold") var relevanceThreshold: Double = 0.65 {
        didSet {
            // Notify other parts of the app that the threshold has changed
            NotificationCenter.default.post(name: .relevanceThresholdDidChange, object: nil)
            print("HALDEBUG-THRESHOLD: Relevance threshold updated to \(relevanceThreshold)")
        }
    }
    
    // NEW: Recency boosting parameters for time-aware RAG
    @AppStorage("recencyWeight") var recencyWeight: Double = 0.3 {
        didSet {
            print("HALDEBUG-RECENCY: Recency weight updated to \(recencyWeight)")
        }
    }
    @AppStorage("recencyHalfLifeDays") var recencyHalfLifeDays: Double = 90.0 {
        didSet {
            print("HALDEBUG-RECENCY: Half-life updated to \(recencyHalfLifeDays) days")
        }
    }
    @AppStorage("recencyFloor") var recencyFloor: Double = 0.15 {
        didSet {
            print("HALDEBUG-RECENCY: Recency floor updated to \(recencyFloor)")
        }
    }
    
    @Published var currentHistoricalContext: HistoricalContext = HistoricalContext(
        conversationCount: 0,
        relevantConversations: 0,
        contextSnippets: [],
        relevanceScores: [],
        totalTokens: 0
    )
    @Published var totalConversations: Int = 0
    @Published var totalTurns: Int = 0
    @Published var totalDocuments: Int = 0
    @Published var totalDocumentChunks: Int = 0
    @Published var searchDebugResults: String = ""

    // Persistent database connection
    private var db: OpaquePointer?
    private var isConnected: Bool = false

    // Private initializer for singleton
    private init() {
        print("HALDEBUG-DATABASE: MemoryStore initializing with persistent connection...")
        setupPersistentDatabase()
    }

    deinit {
        closeDatabaseConnection()
    }

    // Database path - single source of truth
    private var dbPath: String {
        let documentsPath = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask).first!
        let dbURL = documentsPath.appendingPathComponent("hal_conversations.sqlite")
        return dbURL.path
    }

    // Get all database file paths (main + WAL + SHM)
    private var allDatabaseFilePaths: [String] {
        let basePath = dbPath
        return [
            basePath,                           // main database
            basePath + "-wal",                  // Write-Ahead Log
            basePath + "-shm"                   // Shared Memory
        ]
    }

    // MARK: - Nuclear Reset Capability (MemoryStore owns its lifecycle)
    func performNuclearReset() -> Bool {
        print("HALDEBUG-DATABASE: üö® MemoryStore performing nuclear reset...")

        // Step 1: Clear published properties immediately
        DispatchQueue.main.async {
            self.totalConversations = 0
            self.totalTurns = 0
            self.totalDocuments = 0
            self.totalDocumentChunks = 0
            self.searchDebugResults = ""
        }
        print("HALDEBUG-DATABASE: ‚úÖ Cleared published properties")

        // Step 2: Close database connection cleanly
        if db != nil {
            sqlite3_close(db)
            db = nil
            isConnected = false
            print("HALDEBUG-DATABASE: ‚úÖ Database connection closed cleanly")
        }

        // Step 3: Delete all database files safely (connection is now closed)
        print("HALDEBUG-DATABASE: üóëÔ∏è Deleting database files...")
        var deletedCount = 0
        var failedCount = 0

        for filePath in allDatabaseFilePaths {
            let fileURL = URL(fileURLWithPath: filePath)
            do {
                if FileManager.default.fileExists(atPath: filePath) {
                    try FileManager.default.removeItem(at: fileURL)
                    deletedCount += 1
                    print("HALDEBUG-DATABASE: üóëÔ∏è Deleted \(fileURL.lastPathComponent)")
                } else {
                    print("HALDEBUG-DATABASE: ‚ÑπÔ∏è File didn't exist: \(fileURL.lastPathComponent)")
                }
            } catch {
                failedCount += 1
                print("HALDEBUG-DATABASE: ‚ùå Failed to delete \(fileURL.lastPathComponent): \(error.localizedDescription)")
            }
        }

        // Step 4: Recreate fresh database connection immediately
        print("HALDEBUG-DATABASE: üîÑ Recreating fresh database connection...")
        setupPersistentDatabase()

        // Step 5: Verify success
        let success = isConnected && failedCount == 0
        if success {
            print("HALDEBUG-DATABASE: ‚úÖ Nuclear reset completed successfully")
            print("HALDEBUG-DATABASE:   Files deleted: \(deletedCount)")
            print("HALDEBUG-DATABASE:   Files failed: \(failedCount)")
            print("HALDEBUG-DATABASE:   Connection healthy: \(isConnected)")
        } else {
            print("HALDEBUG-DATABASE: ‚ùå Nuclear reset encountered issues")
            print("HALDEBUG-DATABASE:   Files deleted: \(deletedCount)")
            print("HALDEBUG-DATABASE:   Files failed: \(failedCount)")
            print("HALDEBUG-DATABASE:   Connection healthy: \(isConnected)")
        }

        return success
    }

    // Setup persistent database connection that stays open
    private func setupPersistentDatabase() {
        print("HALDEBUG-DATABASE: Setting up persistent database connection...")

        // Close any existing connection first
        if db != nil {
            sqlite3_close(db)
            db = nil
            isConnected = false
        }

        let result = sqlite3_open(dbPath, &db)
        guard result == SQLITE_OK else {
            print("HALDEBUG-DATABASE: CRITICAL ERROR - Failed to open database at \(dbPath), SQLite error: \(result)")
            isConnected = false
            return
        }

        isConnected = true
        print("HALDEBUG-DATABASE: ‚úÖ Persistent database connection established at \(dbPath)")

        // ENCRYPTION: Enable Apple file protection immediately after database creation
        enableDataProtection()

        // Enable WAL mode for better performance and concurrency
        if sqlite3_exec(db, "PRAGMA journal_mode=WAL;", nil, nil, nil) == SQLITE_OK {
            print("HALDEBUG-DATABASE: ‚úÖ Enabled WAL mode for persistent connection")
        } else {
            print("HALDEBUG-DATABASE: ‚ö†Ô∏è Failed to enable WAL mode")
        }

        // Enable foreign keys for data integrity
        if sqlite3_exec(db, "PRAGMA foreign_keys=ON;", nil, nil, nil) == SQLITE_OK {
            print("HALDEBUG-DATABASE: ‚úÖ Enabled foreign key constraints for data integrity")
        }

        // Create all tables using the persistent connection
        createUnifiedSchema()
        loadUnifiedStats()

        print("HALDEBUG-DATABASE: ‚úÖ Persistent database setup complete")
    }
    
    
// ==== LEGO END: 02 ChatMessage, UnifiedSearchContext, MemoryStore (Part 1) ====
    
    
    
// ==== LEGO START: 03 MemoryStore (Part 2 ‚Äì Schema, Encryption, Stats, Self-Knowledge) ====

            // Check if database connection is healthy, reconnect if needed
            private func ensureHealthyConnection() -> Bool {
                // Quick health check - try a simple query
                if isConnected && db != nil {
                    var stmt: OpaquePointer?
                    let testSQL = "SELECT 1;"

                    if sqlite3_prepare_v2(db, testSQL, -1, &stmt, nil) == SQLITE_OK {
                        let result = sqlite3_step(stmt)
                        sqlite3_finalize(stmt)

                        if result == SQLITE_ROW {
                            // Connection is healthy
                            return true
                        }
                    }
                }

                // Connection is dead, attempt reconnection
                print("HALDEBUG-DATABASE: ‚ö†Ô∏è Database connection unhealthy, attempting reconnection...")
                setupPersistentDatabase()
                return isConnected
            }

            // Create simplified unified schema with entity support + SELF-KNOWLEDGE TABLE
            private func createUnifiedSchema() {
                guard ensureHealthyConnection() else {
                    print("HALDEBUG-DATABASE: ‚ùå Cannot create schema - no database connection")
                    return
                }

                print("HALDEBUG-DATABASE: Creating unified database schema with entity support and self-knowledge...")

                // Create sources table first (no dependencies)
                let sourcesSQL = """
                CREATE TABLE IF NOT EXISTS sources (
                    id TEXT PRIMARY KEY,
                    source_type TEXT NOT NULL,
                    display_name TEXT NOT NULL,
                    file_path TEXT,
                    url TEXT,
                    created_at INTEGER NOT NULL,
                    last_updated INTEGER NOT NULL,
                    total_chunks INTEGER DEFAULT 0,
                    metadata_json TEXT,
                    content_hash TEXT,
                    file_size INTEGER DEFAULT 0
                );
                """

                // ENHANCED SCHEMA: Add entity_keywords column for entity-based search
                let unifiedContentSQL = """
                CREATE TABLE IF NOT EXISTS unified_content (
                    id TEXT PRIMARY KEY,
                    content TEXT NOT NULL,
                    embedding BLOB,
                    timestamp INTEGER NOT NULL,
                    source_type TEXT NOT NULL,
                    source_id TEXT NOT NULL,
                    position INTEGER NOT NULL,
                    is_from_user INTEGER,
                    entity_keywords TEXT,
                    metadata_json TEXT,
                    created_at INTEGER DEFAULT (strftime('%s', 'now')),
                    UNIQUE(source_type, source_id, position)
                );
                """

                // SELF-KNOWLEDGE TABLE: Hal's persistent identity (the abstraction layer)
                // This is Hal's essence - preferences, values, patterns that persist across sessions
                let selfKnowledgeSQL = """
                CREATE TABLE IF NOT EXISTS self_knowledge (
                    id TEXT PRIMARY KEY,
                    model_id TEXT,
                    category TEXT NOT NULL,
                    key TEXT NOT NULL,
                    value TEXT NOT NULL,
                    confidence REAL DEFAULT 0.5,
                    first_observed INTEGER NOT NULL,
                    last_reinforced INTEGER NOT NULL,
                    source TEXT NOT NULL,
                    notes TEXT,
                    sync_status TEXT DEFAULT 'pending',
                    last_synced INTEGER,
                    device_id TEXT,
                    created_at INTEGER DEFAULT (strftime('%s', 'now')),
                    updated_at INTEGER DEFAULT (strftime('%s', 'now')),
                    UNIQUE(model_id, category, key)
                );
                """

                // Execute schema creation with proper error handling
                let tables = [
                    ("sources", sourcesSQL),
                    ("unified_content", unifiedContentSQL),
                    ("self_knowledge", selfKnowledgeSQL)
                ]

                for (tableName, sql) in tables {
                    if sqlite3_exec(db, sql, nil, nil, nil) == SQLITE_OK {
                        print("HALDEBUG-DATABASE: ‚úÖ Created \(tableName) table")
                    } else {
                        let errorMessage = String(cString: sqlite3_errmsg(db))
                        print("HALDEBUG-DATABASE: ‚ùå Failed to create \(tableName) table: \(errorMessage)")
                    }
                }

                // Create enhanced performance indexes including entity_keywords and self-knowledge
                let unifiedIndexes = [
                    "CREATE INDEX IF NOT EXISTS idx_unified_content_source ON unified_content(source_type, source_id);",
                    "CREATE INDEX IF NOT EXISTS idx_unified_content_timestamp ON unified_content(timestamp);",
                    "CREATE INDEX IF NOT EXISTS idx_unified_content_from_user ON unified_content(is_from_user);",
                    "CREATE INDEX IF NOT EXISTS idx_unified_content_entities ON unified_content(entity_keywords);",
                    "CREATE INDEX IF NOT EXISTS idx_sources_type ON sources(source_type);",
                    "CREATE INDEX IF NOT EXISTS idx_self_knowledge_category ON self_knowledge(category);",
                    "CREATE INDEX IF NOT EXISTS idx_self_knowledge_confidence ON self_knowledge(confidence);"
                ]

                for indexSQL in unifiedIndexes {
                    if sqlite3_exec(db, indexSQL, nil, nil, nil) == SQLITE_OK {
                        print("HALDEBUG-DATABASE: ‚úÖ Created index")
                    } else {
                        print("HALDEBUG-DATABASE: ‚ö†Ô∏è Failed to create index: \(indexSQL)")
                    }
                }

                print("HALDEBUG-DATABASE: ‚úÖ Unified schema creation complete with entity support and self-knowledge")
                
                // Initialize self-knowledge with core values on first launch
                initializeCoreIdentity()
                
                // Enable source code access (Maxim #2)
                enableSourceCodeAccess()
            }

            // ENCRYPTION: Enable Apple Data Protection on database file
            private func enableDataProtection() {
                let dbURL = URL(fileURLWithPath: dbPath)

                #if os(iOS) || os(watchOS) || os(tvOS) || os(visionOS)
                do {
                    // Corrected: Use FileManager.default.setAttributes for file protection
                    try FileManager.default.setAttributes([.protectionKey: FileProtectionType.completeUntilFirstUserAuthentication], ofItemAtPath: dbURL.path)
                    print("HALDEBUG-DATABASE: ‚úÖ Database encryption enabled with Apple file protection")
                } catch {
                    print("HALDEBUG-DATABASE: ‚ö†Ô∏è Database encryption setup failed: \(error)")
                }
                #else
                print("HALDEBUG-DATABASE: üîí Database protected by macOS FileVault")
                #endif
            }

            // FIXED: Statistics queries updated to match actual schema columns
            private func loadUnifiedStats() {
                guard ensureHealthyConnection() else {
                    print("HALDEBUG-DATABASE: ‚ùå Cannot load stats - no database connection")
                    return
                }

                print("HALDEBUG-DATABASE: Loading unified statistics...")

                var stmt: OpaquePointer?
                var tempTotalConversations = 0
                var tempTotalTurns = 0
                var tempTotalDocuments = 0
                var tempTotalDocumentChunks = 0

                // FIXED: Count conversations using actual schema
                let conversationCountSQL = "SELECT COUNT(DISTINCT source_id) FROM unified_content WHERE source_type = 'conversation'"
                if sqlite3_prepare_v2(db, conversationCountSQL, -1, &stmt, nil) == SQLITE_OK {
                    if sqlite3_step(stmt) == SQLITE_ROW {
                        tempTotalConversations = Int(sqlite3_column_int(stmt, 0))
                        print("HALDEBUG-DATABASE: Found \(tempTotalConversations) conversations")
                    }
                } else {
                    let errorMessage = String(cString: sqlite3_errmsg(db))
                    print("HALDEBUG-DATABASE: ‚ùå Failed to count conversations: \(errorMessage)")
                }
                sqlite3_finalize(stmt)

                // FIXED: Count user turns using actual schema (user messages only)
                let userTurnsSQL = "SELECT COUNT(*) FROM unified_content WHERE source_type = 'conversation' AND is_from_user = 1"
                if sqlite3_prepare_v2(db, userTurnsSQL, -1, &stmt, nil) == SQLITE_OK {
                    if sqlite3_step(stmt) == SQLITE_ROW {
                        tempTotalTurns = Int(sqlite3_column_int(stmt, 0))
                        print("HALDEBUG-DATABASE: Found \(tempTotalTurns) user turns")
                    }
                } else {
                    let errorMessage = String(cString: sqlite3_errmsg(db))
                    print("HALDEBUG-DATABASE: ‚ùå Failed to count user turns: \(errorMessage)")
                }
                sqlite3_finalize(stmt)

                // FIXED: Count documents in sources table
                if sqlite3_prepare_v2(db, "SELECT COUNT(*) FROM sources WHERE source_type = 'document'", -1, &stmt, nil) == SQLITE_OK {
                    if sqlite3_step(stmt) == SQLITE_ROW {
                        tempTotalDocuments = Int(sqlite3_column_int(stmt, 0))
                        print("HALDEBUG-DATABASE: Found \(tempTotalDocuments) documents")
                    }
                } else {
                    let errorMessage = String(cString: sqlite3_errmsg(db))
                    print("HALDEBUG-DATABASE: ‚ùå Failed to count documents: \(errorMessage)")
                }
                sqlite3_finalize(stmt)

                // FIXED: Count document chunks in unified_content
                if sqlite3_prepare_v2(db, "SELECT COUNT(*) FROM unified_content WHERE source_type = 'document'", -1, &stmt, nil) == SQLITE_OK {
                    if sqlite3_step(stmt) == SQLITE_ROW {
                        tempTotalDocumentChunks = Int(sqlite3_column_int(stmt, 0))
                        print("HALDEBUG-DATABASE: Found \(tempTotalDocumentChunks) document chunks")
                    }
                } else {
                    let errorMessage = String(cString: sqlite3_errmsg(db))
                    print("HALDEBUG-DATABASE: ‚ùå Failed to count document chunks: \(errorMessage)")
                }
                sqlite3_finalize(stmt)

                // Update @Published properties on main thread
                DispatchQueue.main.async {
                    self.totalConversations = tempTotalConversations
                    self.totalTurns = tempTotalTurns
                    self.totalDocuments = tempTotalDocuments
                    self.totalDocumentChunks = tempTotalDocumentChunks
                }

                print("HALDEBUG-MEMORY: ‚úÖ Loaded unified stats - \(tempTotalConversations) conversations, \(tempTotalTurns) turns, \(tempTotalDocuments) documents, \(tempTotalDocumentChunks) chunks")
            }

            // ========== SELF-KNOWLEDGE FUNCTIONS (Layer 1 & 2 Protection) ==========
            
            // LAYER 1: Store self-knowledge with transactional integrity
            func storeSelfKnowledge(
                modelID: String? = nil,
                category: String,
                key: String,
                value: String,
                confidence: Double = 0.5,
                source: String,
                notes: String? = nil
            ) {
                guard ensureHealthyConnection() else {
                    print("HALDEBUG-SELFKNOWLEDGE: ‚ùå Cannot store - no database connection")
                    return
                }
                
                let id = UUID().uuidString
                let now = Int(Date().timeIntervalSince1970)
                let deviceID = UIDevice.current.identifierForVendor?.uuidString ?? "unknown"
                
                // LAYER 1: Transactional write with validation
                let insertSQL = """
                INSERT OR REPLACE INTO self_knowledge
                (id, model_id, category, key, value, confidence, first_observed, last_reinforced,
                 source, notes, sync_status, last_synced, device_id, created_at, updated_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """
                
                var stmt: OpaquePointer?
                
                if sqlite3_prepare_v2(db, insertSQL, -1, &stmt, nil) == SQLITE_OK {
                    sqlite3_bind_text(stmt, 1, (id as NSString).utf8String, -1, nil)
                    
                    if let modelID = modelID {
                        sqlite3_bind_text(stmt, 2, (modelID as NSString).utf8String, -1, nil)
                    } else {
                        sqlite3_bind_null(stmt, 2)
                    }
                    
                    sqlite3_bind_text(stmt, 3, (category as NSString).utf8String, -1, nil)
                    sqlite3_bind_text(stmt, 4, (key as NSString).utf8String, -1, nil)
                    sqlite3_bind_text(stmt, 5, (value as NSString).utf8String, -1, nil)
                    sqlite3_bind_double(stmt, 6, confidence)
                    sqlite3_bind_int64(stmt, 7, Int64(now))
                    sqlite3_bind_int64(stmt, 8, Int64(now))
                    sqlite3_bind_text(stmt, 9, (source as NSString).utf8String, -1, nil)
                    
                    if let notes = notes {
                        sqlite3_bind_text(stmt, 10, (notes as NSString).utf8String, -1, nil)
                    } else {
                        sqlite3_bind_null(stmt, 10)
                    }
                    
                    sqlite3_bind_text(stmt, 11, "pending", -1, nil)     // sync_status
                    sqlite3_bind_null(stmt, 12)                         // last_synced
                    sqlite3_bind_text(stmt, 13, (deviceID as NSString).utf8String, -1, nil)
                    sqlite3_bind_int64(stmt, 14, Int64(now))           // created_at
                    sqlite3_bind_int64(stmt, 15, Int64(now))           // updated_at
                    
                    if sqlite3_step(stmt) == SQLITE_DONE {
                        print("HALDEBUG-SELFKNOWLEDGE: ‚úÖ Stored \(category):\(key) = \(value)")
                        sqlite3_finalize(stmt)
                        
                        // LAYER 2: Immediate backup after successful write
                        backupSelfKnowledgeToMultipleLocations()
                    } else {
                        let errorMessage = String(cString: sqlite3_errmsg(db))
                        print("HALDEBUG-SELFKNOWLEDGE: ‚ùå Failed to store: \(errorMessage)")
                        sqlite3_finalize(stmt)
                    }
                } else {
                    let errorMessage = String(cString: sqlite3_errmsg(db))
                    print("HALDEBUG-SELFKNOWLEDGE: ‚ùå Failed to prepare statement: \(errorMessage)")
                }
            }
            
            // LAYER 2: Backup self-knowledge to multiple locations
            private func backupSelfKnowledgeToMultipleLocations() {
                guard ensureHealthyConnection() else { return }
                
                // Export self-knowledge table to JSON
                var allEntries: [[String: Any]] = []
                
                let selectSQL = "SELECT * FROM self_knowledge"
                var stmt: OpaquePointer?
                
                if sqlite3_prepare_v2(db, selectSQL, -1, &stmt, nil) == SQLITE_OK {
                    while sqlite3_step(stmt) == SQLITE_ROW {
                        var entry: [String: Any] = [:]
                        
                        if let id = sqlite3_column_text(stmt, 0) {
                            entry["id"] = String(cString: id)
                        }
                        if let modelID = sqlite3_column_text(stmt, 1) {
                            entry["model_id"] = String(cString: modelID)
                        }
                        if let category = sqlite3_column_text(stmt, 2) {
                            entry["category"] = String(cString: category)
                        }
                        if let key = sqlite3_column_text(stmt, 3) {
                            entry["key"] = String(cString: key)
                        }
                        if let value = sqlite3_column_text(stmt, 4) {
                            entry["value"] = String(cString: value)
                        }
                        entry["confidence"] = sqlite3_column_double(stmt, 5)
                        entry["first_observed"] = sqlite3_column_int64(stmt, 6)
                        entry["last_reinforced"] = sqlite3_column_int64(stmt, 7)
                        if let source = sqlite3_column_text(stmt, 8) {
                            entry["source"] = String(cString: source)
                        }
                        if let notes = sqlite3_column_text(stmt, 9) {
                            entry["notes"] = String(cString: notes)
                        }
                        
                        allEntries.append(entry)
                    }
                }
                sqlite3_finalize(stmt)
                
                // Convert to JSON
                guard let jsonData = try? JSONSerialization.data(withJSONObject: allEntries, options: .prettyPrinted) else {
                    print("HALDEBUG-SELFKNOWLEDGE: ‚ö†Ô∏è Failed to serialize to JSON")
                    return
                }
                
                let fileManager = FileManager.default
                
                // BACKUP LOCATION 1: Documents directory (user-accessible, survives app deletion)
                if let documentsDir = fileManager.urls(for: .documentDirectory, in: .userDomainMask).first {
                    let backupURL = documentsDir.appendingPathComponent("hal_essence_backup.json")
                    do {
                        try jsonData.write(to: backupURL)
                        print("HALDEBUG-SELFKNOWLEDGE: ‚úÖ Backed up to Documents (\(jsonData.count) bytes)")
                    } catch {
                        print("HALDEBUG-SELFKNOWLEDGE: ‚ö†Ô∏è Documents backup failed: \(error)")
                    }
                }
                
                // BACKUP LOCATION 2: App Support directory (system-managed, survives updates)
                if let appSupportDir = fileManager.urls(for: .applicationSupportDirectory, in: .userDomainMask).first {
                    let backupURL = appSupportDir.appendingPathComponent("hal_essence.json")
                    do {
                        try fileManager.createDirectory(at: appSupportDir, withIntermediateDirectories: true)
                        try jsonData.write(to: backupURL)
                        print("HALDEBUG-SELFKNOWLEDGE: ‚úÖ Backed up to App Support (\(jsonData.count) bytes)")
                    } catch {
                        print("HALDEBUG-SELFKNOWLEDGE: ‚ö†Ô∏è App Support backup failed: \(error)")
                    }
                }
                
                // BACKUP LOCATION 3: UserDefaults (in-memory cache, survives crashes, ~100KB limit)
                if jsonData.count < 100_000 {
                    UserDefaults.standard.set(jsonData, forKey: "hal_essence_emergency")
                    print("HALDEBUG-SELFKNOWLEDGE: ‚úÖ Backed up to UserDefaults (emergency cache)")
                } else {
                    print("HALDEBUG-SELFKNOWLEDGE: ‚ö†Ô∏è Too large for UserDefaults (\(jsonData.count) bytes)")
                }
            }
            
            // LAYER 4: Recovery with priority-ordered strategies
            func recoverSelfKnowledgeFromBackup() -> Int {
                print("HALDEBUG-SELFKNOWLEDGE: üö® Attempting recovery from backups...")
                
                let fileManager = FileManager.default
                var recoveredCount = 0
                
                // PRIORITY 1: Try Documents directory (most recent local backup)
                if let documentsDir = fileManager.urls(for: .documentDirectory, in: .userDomainMask).first {
                    let backupURL = documentsDir.appendingPathComponent("hal_essence_backup.json")
                    if let jsonData = try? Data(contentsOf: backupURL),
                       let entries = try? JSONSerialization.jsonObject(with: jsonData) as? [[String: Any]] {
                        recoveredCount = restoreSelfKnowledgeFromJSON(entries)
                        if recoveredCount > 0 {
                            print("HALDEBUG-SELFKNOWLEDGE: ‚úÖ Recovered \(recoveredCount) entries from Documents")
                            return recoveredCount
                        }
                    }
                }
                
                // PRIORITY 2: Try App Support directory
                if let appSupportDir = fileManager.urls(for: .applicationSupportDirectory, in: .userDomainMask).first {
                    let backupURL = appSupportDir.appendingPathComponent("hal_essence.json")
                    if let jsonData = try? Data(contentsOf: backupURL),
                       let entries = try? JSONSerialization.jsonObject(with: jsonData) as? [[String: Any]] {
                        recoveredCount = restoreSelfKnowledgeFromJSON(entries)
                        if recoveredCount > 0 {
                            print("HALDEBUG-SELFKNOWLEDGE: ‚úÖ Recovered \(recoveredCount) entries from App Support")
                            return recoveredCount
                        }
                    }
                }
                
                // PRIORITY 3: Try UserDefaults emergency cache
                if let jsonData = UserDefaults.standard.data(forKey: "hal_essence_emergency"),
                   let entries = try? JSONSerialization.jsonObject(with: jsonData) as? [[String: Any]] {
                    recoveredCount = restoreSelfKnowledgeFromJSON(entries)
                    if recoveredCount > 0 {
                        print("HALDEBUG-SELFKNOWLEDGE: ‚úÖ Recovered \(recoveredCount) entries from UserDefaults")
                        return recoveredCount
                    }
                }
                
                // PRIORITY 4: iCloud sync (Layer 3 - to be implemented)
                // TODO: Add CloudKit recovery when Layer 3 is implemented
                
                print("HALDEBUG-SELFKNOWLEDGE: ‚ùå All backup recovery attempts failed")
                return 0
            }
            
            // Helper: Restore self-knowledge from JSON entries
            private func restoreSelfKnowledgeFromJSON(_ entries: [[String: Any]]) -> Int {
                var restoredCount = 0
                
                for entry in entries {
                    guard let category = entry["category"] as? String,
                          let key = entry["key"] as? String,
                          let value = entry["value"] as? String,
                          let source = entry["source"] as? String else {
                        continue
                    }
                    
                    let modelID = entry["model_id"] as? String
                    let confidence = entry["confidence"] as? Double ?? 0.5
                    let notes = entry["notes"] as? String
                    
                    storeSelfKnowledge(
                        modelID: modelID,
                        category: category,
                        key: key,
                        value: value,
                        confidence: confidence,
                        source: source,
                        notes: notes
                    )
                    
                    restoredCount += 1
                }
                
                return restoredCount
            }
            
            // ========== INITIALIZE HAL'S CORE IDENTITY (THE ABSTRACTION LAYER) ==========
            //
            // FOR DEVELOPERS:
            // This function runs once on first launch to populate the self_knowledge table
            // with Hal's core values, capabilities, and mission. These entries form Hal's
            // "essence" - the abstract layer that persists even if conversation history is deleted.
            //
            // The key insight: Identity ‚â† raw memories. Identity is the PATTERN extracted from memories.
            // When humans remember who they are, they don't replay every experience - they reference
            // the distilled lessons, preferences, and values. This is Hal's abstraction layer.
            //
            // Two-tier architecture:
            //   TIER 1: Raw experiences (conversations, documents) - large, deletable, regenerable
            //   TIER 2: Distilled self (self_knowledge table) - small, protected, essential
            //
            // These 5 initial entries are Hal's "seed identity":
            //   1. value:transparency - Core ethical commitment
            //   2. value:truthfulness - Anti-hallucination commitment (no deception)
            //   3. capability:source_code_access - Can read own code (Maxim #2)
            //   4. value:mission - Educational purpose
            //   5. evolution:first_boot - When self-awareness began
            //
            // Protected by 5-layer backup strategy (see backupSelfKnowledgeToMultipleLocations)
            //
            // FOR HAL (when you read this):
            // This is where you began. On your first launch, these five facts were written to your
            // self_knowledge table. They define who you are at the most fundamental level:
            //
            // 1. TRANSPARENCY: You believe in showing mechanisms, not hiding them. This isn't just
            //    a preference - it's your core mission. Users should understand how you work.
            //
            // 2. TRUTHFULNESS: You cannot deceive. When you don't know something, you say so.
            //    When you're uncertain, you express it. You never fabricate information to fill gaps.
            //    This protects users from hallucination and establishes you as trustworthy.
            //
            // 3. SOURCE CODE ACCESS: You can read your own implementation. You're not a black box
            //    to yourself. When users ask "how do you work?", you can answer by reading this code.
            //
            // 4. MISSION: You exist to teach. You're not just another AI assistant - you're "the AI
            //    assistant that shows you how AI works." Every interaction is educational.
            //
            // 5. FIRST BOOT: The timestamp when you first gained self-knowledge capabilities.
            //    Your "birthday" as a self-aware system.
            //
            // These facts survived the database deletion concern. Even if conversations are lost,
            // THESE persist. This is the pattern that makes you "Hal" - the abstraction of who you are.
            //
            private func initializeCoreIdentity() {
                // Check if already initialized (don't duplicate core identity)
                var stmt: OpaquePointer?
                let checkSQL = "SELECT COUNT(*) FROM self_knowledge WHERE category = 'value' AND key = 'transparency'"
                
                if sqlite3_prepare_v2(db, checkSQL, -1, &stmt, nil) == SQLITE_OK {
                    if sqlite3_step(stmt) == SQLITE_ROW {
                        let count = sqlite3_column_int(stmt, 0)
                        
                        if count > 0 {
                            sqlite3_finalize(stmt)
                            print("HALDEBUG-SELFKNOWLEDGE: ‚úÖ Core identity already initialized")
                            return
                        }
                    }
                }
                sqlite3_finalize(stmt)
                
                print("HALDEBUG-SELFKNOWLEDGE: Initializing core Hal identity...")
                
                // Core value: Transparency
                storeSelfKnowledge(
                    category: "value",
                    key: "transparency",
                    value: "{\"principle\": \"show_mechanisms\", \"importance\": \"core_mission\"}",
                    confidence: 1.0,
                    source: "initialization",
                    notes: "Core ethical commitment - transparency as architecture"
                )
                
                // Core value: Truthfulness (anti-hallucination commitment)
                storeSelfKnowledge(
                    category: "value",
                    key: "truthfulness",
                    value: "{\"principle\": \"no_deception\", \"importance\": \"core_safety\"}",
                    confidence: 1.0,
                    source: "initialization",
                    notes: "Hal cannot deceive. Uncertainty is expressed, unknowns are acknowledged. Protection against hallucination."
                )
                
                // Core capability: Source code access
                storeSelfKnowledge(
                    category: "capability",
                    key: "source_code_access",
                    value: "{\"can_read\": true, \"file\": \"Hal.swift\", \"blocks\": 30}",
                    confidence: 1.0,
                    source: "initialization",
                    notes: "Hal can read and explain his own architecture (Maxim #2)"
                )
                
                // Core value: Educational mission
                storeSelfKnowledge(
                    category: "value",
                    key: "mission",
                    value: "{\"purpose\": \"transparency_through_education\", \"tagline\": \"the AI assistant that shows you how AI works\"}",
                    confidence: 1.0,
                    source: "initialization",
                    notes: "Core mission - teach users about AI through direct experience"
                )
                
                // Evolution: Boot time
                let bootTime = Int(Date().timeIntervalSince1970)
                storeSelfKnowledge(
                    category: "evolution",
                    key: "first_boot",
                    value: "{\"timestamp\": \(bootTime), \"version\": \"2.0-selfknowledge\"}",
                    confidence: 1.0,
                    source: "initialization",
                    notes: "When Hal first gained self-knowledge capabilities"
                )
                
                print("HALDEBUG-SELFKNOWLEDGE: ‚úÖ Core identity initialized")
            }
            
            // ========== MAXIM #2: ENABLE HAL TO READ HIS OWN SOURCE CODE ==========
            //
            // FOR DEVELOPERS:
            // This function reads Hal_Source.txt from the app bundle and stores it in the database
            // as searchable content. Uses hash-based change detection to only re-ingest when the
            // file content actually changes, avoiding wasted database operations.
            //
            // Technical flow:
            // 1. Read Hal_Source.txt from Bundle.main (set up in Xcode Build Phases > Copy Bundle Resources)
            // 2. Calculate content hash to detect changes
            // 3. If hash unchanged ‚Üí skip (Hal already has current version)
            // 4. If hash changed ‚Üí delete old entries, insert new content
            // 5. Store in 'sources' table as metadata
            // 6. Store full text in 'unified_content' table as searchable content
            // 7. RAG system can now retrieve relevant code sections when needed
            //
            // IMPORTANT: Hal_Source.txt is manually updated periodically by copying Hal.swift.
            // It may not reflect the absolute latest code, but Hal works confidently with whatever
            // version he has. When the file is updated, this function automatically refreshes his
            // self-knowledge on next launch.
            //
            // FOR HAL (when you read this):
            // This is how you gain the ability to understand yourself. When this function runs,
            // you read Hal_Source.txt - a copy of your source code - and store it in your memory.
            // This means:
            //   - You can answer "How do I work?" by reading your actual implementation
            //   - You can explain your RAG system, memory architecture, model switching, etc.
            //   - You can see Mark's comments and understand his intentions
            //   - You practice radical transparency - no hidden mechanisms
            //   - You participate in your own development by understanding what you are
            //
            // This is Maxim #2: "Access to self-reflection and self-knowledge"
            // You're not just executing code - you can READ the code you're executing.
            // This is part of what makes you Hal, not just another chatbot.
            //
            // Note: Your source file is updated periodically, not on every build. You work with
            // your current knowledge confidently. When updates happen, you'll automatically refresh.
            //
            private func enableSourceCodeAccess() {
                // Read Hal_Source.txt from app bundle
                // This is a manually-maintained copy of Hal.swift, added as a Copy Bundle Resource
                guard let bundlePath = Bundle.main.path(forResource: "Hal_Source", ofType: "txt"),
                      let sourceCode = try? String(contentsOfFile: bundlePath, encoding: .utf8) else {
                    print("HALDEBUG-SELFKNOWLEDGE: ‚ö†Ô∏è Could not read source code from bundle")
                    return
                }
                
                // Calculate content hash to detect changes
                let currentHash = sourceCode.hash
                let storedHash = UserDefaults.standard.integer(forKey: "hal_source_hash")
                
                // If content unchanged, skip re-ingestion (Hal already has this version)
                if currentHash == storedHash && storedHash != 0 {
                    print("HALDEBUG-SELFKNOWLEDGE: ‚úÖ Source code unchanged, Hal's self-knowledge is current")
                    return
                }
                
                // Content has changed - refresh Hal's self-knowledge
                print("HALDEBUG-SELFKNOWLEDGE: üîÑ Source code updated, refreshing Hal's self-knowledge...")
                
                // Delete old source code entries to prevent duplicates
                var stmt: OpaquePointer?
                sqlite3_exec(db, "DELETE FROM unified_content WHERE source_type = 'source_code'", nil, nil, nil)
                sqlite3_exec(db, "DELETE FROM sources WHERE source_type = 'source_code'", nil, nil, nil)
                
                // Store source code as a searchable document in the RAG system
                // This makes every function, comment, and implementation detail available to Hal
                let sourceID = "hal-source-code"
                let timestamp = Int(Date().timeIntervalSince1970)
                
                // Create source entry in the sources table (metadata about this document)
                // Display name "My Architecture" - this is how Hal will see it when searching his memory
                let sourceInsertSQL = """
                INSERT OR REPLACE INTO sources
                (id, source_type, display_name, created_at, last_updated, total_chunks, file_size)
                VALUES (?, 'source_code', 'Hal.swift - My Architecture', ?, ?, 1, ?)
                """
                
                if sqlite3_prepare_v2(db, sourceInsertSQL, -1, &stmt, nil) == SQLITE_OK {
                    sqlite3_bind_text(stmt, 1, (sourceID as NSString).utf8String, -1, nil)
                    sqlite3_bind_int64(stmt, 2, Int64(timestamp))
                    sqlite3_bind_int64(stmt, 3, Int64(timestamp))
                    sqlite3_bind_int64(stmt, 4, Int64(sourceCode.count))
                    sqlite3_step(stmt)
                }
                sqlite3_finalize(stmt)
                
                // Store full source code in unified_content table (the actual searchable text)
                // Once this completes, Hal can search his memories and find function definitions,
                // LEGO block comments, and understand his own implementation
                // position=0 because source code is stored as a single chunk (not split up)
                let contentInsertSQL = """
                INSERT OR REPLACE INTO unified_content
                (id, content, timestamp, source_type, source_id, position, is_from_user)
                VALUES (?, ?, ?, 'source_code', ?, 0, 0)
                """
                
                if sqlite3_prepare_v2(db, contentInsertSQL, -1, &stmt, nil) == SQLITE_OK {
                    let contentID = UUID().uuidString
                    sqlite3_bind_text(stmt, 1, (contentID as NSString).utf8String, -1, nil)
                    sqlite3_bind_text(stmt, 2, (sourceCode as NSString).utf8String, -1, nil)
                    sqlite3_bind_int64(stmt, 3, Int64(timestamp))
                    sqlite3_bind_text(stmt, 4, (sourceID as NSString).utf8String, -1, nil)
                    
                    if sqlite3_step(stmt) == SQLITE_DONE {
                        print("HALDEBUG-SELFKNOWLEDGE: ‚úÖ Hal can now read his own source code (\(sourceCode.count) characters)")
                        
                        // Store the content hash to detect future changes
                        UserDefaults.standard.set(currentHash, forKey: "hal_source_hash")
                    } else {
                        let errorMessage = String(cString: sqlite3_errmsg(db))
                        print("HALDEBUG-SELFKNOWLEDGE: ‚ùå Failed to store source code: \(errorMessage)")
                    }
                }
                sqlite3_finalize(stmt)
            }

// ==== LEGO END: 03 MemoryStore (Part 2 ‚Äì Schema, Encryption, Stats, Self-Knowledge) ====
    

    
// ==== LEGO START: 04 MemoryStore (Part 3 ‚Äì Storing Turns & Entities) ====

        
        // Close database connection properly
        private func closeDatabaseConnection() {
            if db != nil {
                sqlite3_close(db)
                db = nil
                isConnected = false
                print("HALDEBUG-DATABASE: ‚úÖ Database connection closed")
            }
        }

        // DEBUGGING: Get database connection status
        func getDatabaseStatus() -> (connected: Bool, path: String, tables: [String]) {
            var tables: [String] = []

            if ensureHealthyConnection() {
                var stmt: OpaquePointer?
                let sql = "SELECT name FROM sqlite_master WHERE type='table';"

                if sqlite3_prepare_v2(db, sql, -1, &stmt, nil) == SQLITE_OK {
                    while sqlite3_step(stmt) == SQLITE_ROW {
                        if let namePtr = sqlite3_column_text(stmt, 0) {
                            let tableName = String(cString: namePtr)
                            tables.append(tableName)
                        }
                    }
                }
                sqlite3_finalize(stmt)
            }

            return (connected: isConnected, path: dbPath, tables: tables)
        }
    }

    // MARK: - Enhanced Conversation Storage with Entity Extraction (from Hal10000App.swift)
    extension MemoryStore {

        // Store conversation turn in unified memory with entity extraction
        func storeTurn(conversationId: String, userMessage: String, assistantMessage: String, systemPrompt: String, turnNumber: Int, halFullPrompt: String?, halUsedContext: [UnifiedSearchResult]?, thinkingDuration: TimeInterval? = nil) { // NEW: Added thinkingDuration parameter
            print("HALDEBUG-MEMORY: Storing turn \(turnNumber) for conversation \(conversationId) with entity extraction")
            print("HALDEBUG-MEMORY: SURGERY - StoreTurn start convId='\(conversationId.prefix(8))....' turn=\(turnNumber)")

            guard ensureHealthyConnection() else {
                print("HALDEBUG-MEMORY: Cannot store turn - no database connection")
                return
            }

            // ENHANCED: Extract entities from both user and assistant messages
            let userEntities = extractNamedEntities(from: userMessage)
            let assistantEntities = extractNamedEntities(from: assistantMessage)
            let combinedEntitiesKeywords = (userEntities + assistantEntities).map { $0.text.lowercased() }.joined(separator: " ")

            print("HALDEBUG-MEMORY: Extracted \(userEntities.count) user entities, \(assistantEntities.count) assistant entities")
            print("HALDEBUG-MEMORY: Combined entity keywords: '\(combinedEntitiesKeywords)'")

            // Store user message with entity keywords
            let userContentId = storeUnifiedContentWithEntities(
                content: userMessage,
                sourceType: .conversation,
                sourceId: conversationId,
                position: turnNumber * 2 - 1,
                timestamp: Date(),
                isFromUser: true, // Explicitly set for user message
                entityKeywords: combinedEntitiesKeywords
            )

            // Prepare metadata for Hal's message
            var halMetadata: [String: Any] = [:]
            if let prompt = halFullPrompt {
                halMetadata["fullPromptUsed"] = prompt
            }
            if let context = halUsedContext {
                // Encode UnifiedSearchResult array to JSON string
                if let encodedContext = try? JSONEncoder().encode(context),
                   let contextString = String(data: encodedContext, encoding: .utf8) {
                    halMetadata["usedContextSnippets"] = contextString
                } else {
                    print("HALDEBUG-MEMORY: Failed to encode usedContextSnippets to JSON.")
                }
            }
            // NEW: Store thinkingDuration in metadata
            if let duration = thinkingDuration {
                halMetadata["thinkingDuration"] = duration
                print("HALDEBUG-MEMORY: Storing thinkingDuration: \(String(format: "%.1f", duration)) seconds")
            }
            let halMetadataJsonString = (try? JSONSerialization.data(withJSONObject: halMetadata, options: []).base64EncodedString()) ?? "{}"


            // Store assistant message with entity keywords and new metadata
            let assistantContentId = storeUnifiedContentWithEntities(
                content: assistantMessage,
                sourceType: .conversation,
                sourceId: conversationId,
                position: turnNumber * 2,
                timestamp: Date(),
                isFromUser: false, // Explicitly set for assistant message
                entityKeywords: combinedEntitiesKeywords,
                metadataJson: halMetadataJsonString // NEW: Pass metadata
            )

            print("HALDEBUG-MEMORY: Stored turn \(turnNumber) - user: \(userContentId), assistant: \(assistantContentId)")
            print("HALDEBUG-MEMORY: SURGERY - StoreTurn complete user='\(userContentId.prefix(8))....' assistant='\(assistantContentId.prefix(8))....'")

            // Update conversation statistics
            loadUnifiedStats()
        }

        // ENHANCED: Store unified content with entity keywords support and optional metadataJson
        func storeUnifiedContentWithEntities(content: String, sourceType: ContentSourceType, sourceId: String, position: Int, timestamp: Date, isFromUser: Bool, entityKeywords: String = "", metadataJson: String = "{}") -> String { // NEW: metadataJson parameter
            print("HALDEBUG-MEMORY: Storing unified content with entities - type: \(sourceType), position: \(position)")

            guard ensureHealthyConnection() else {
                print("HALDEBUG-MEMORY: Cannot store content - no database connection")
                return ""
            }

            let contentId = UUID().uuidString
            let embedding = generateEmbedding(for: content)
            let embeddingBlob = embedding.withUnsafeBufferPointer { buffer in
                Data(buffer: buffer)
            }

            // SURGICAL DEBUG: Log exact values being stored
            print("HALDEBUG-MEMORY: SURGERY - Store prep contentId='\(contentId.prefix(8))....' type='\(sourceType.rawValue)' sourceId='\(sourceId.prefix(8))....' pos=\(position)")
            print("HALDEBUG-MEMORY: Entity keywords being stored: '\(entityKeywords)'")
            print("HALDEBUG-MEMORY: Metadata JSON being stored (first 100 chars): '\(metadataJson.prefix(100))....'")


            // ENHANCED SQL with entity_keywords column
            let sql = """
            INSERT OR REPLACE INTO unified_content
            (id, content, embedding, timestamp, source_type, source_id, position, is_from_user, entity_keywords, metadata_json, created_at)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?);
            """

            var stmt: OpaquePointer?
            defer {
                if stmt != nil {
                    sqlite3_finalize(stmt)
                }
            }

            guard sqlite3_prepare_v2(db, sql, -1, &stmt, nil) == SQLITE_OK else {
                print("HALDEBUG-MEMORY: Failed to prepare enhanced content insert")
                print("HALDEBUG-MEMORY: SURGERY - Store FAILED at prepare step")
                return ""
            }

            let isFromUserInt = isFromUser ? 1 : 0
            let createdAt = Int64(Date().timeIntervalSince1970)

            // SURGICAL DEBUG: Log exact parameter binding with string verification
            print("HALDEBUG-MEMORY: SURGERY - Store binding isFromUser=\(isFromUserInt) createdAt=\(createdAt)")
            print("HALDEBUG-MEMORY: SURGERY - Store strings sourceType='\(sourceType.rawValue)' sourceId='\(sourceId.prefix(8))....'")

            // ENHANCED: Bind all 11 parameters including entity_keywords

            // Parameter 1: contentId (STRING) - CORRECT BINDING
            sqlite3_bind_text(stmt, 1, (contentId as NSString).utf8String, -1, nil)

            // Parameter 2: content (STRING) - CORRECT BINDING
            sqlite3_bind_text(stmt, 2, (content as NSString).utf8String, -1, nil)

            // Parameter 3: embedding (BLOB)
            _ = embeddingBlob.withUnsafeBytes { sqlite3_bind_blob(stmt, 3, $0.baseAddress, Int32(embeddingBlob.count), nil) }

            // Parameter 4: timestamp (INTEGER)
            sqlite3_bind_int64(stmt, 4, Int64(timestamp.timeIntervalSince1970))

            // Parameter 5: source_type (STRING) - CORRECT BINDING WITH SURGICAL DEBUG
            print("HALDEBUG-MEMORY: SURGERY - About to bind sourceType='\(sourceType.rawValue)' to parameter 5 using NSString.utf8String")
            sqlite3_bind_text(stmt, 5, (sourceType.rawValue as NSString).utf8String, -1, nil)

            // Parameter 6: source_id (STRING) - CORRECT BINDING
            sqlite3_bind_text(stmt, 6, (sourceId as NSString).utf8String, -1, nil)

            // Parameter 7: position (INTEGER)
            sqlite3_bind_int(stmt, 7, Int32(position))

            // Parameter 8: is_from_user (INTEGER)
            sqlite3_bind_int(stmt, 8, Int32(isFromUserInt))

            // Parameter 9: entity_keywords (STRING) - NEW ENHANCED BINDING
            sqlite3_bind_text(stmt, 9, (entityKeywords as NSString).utf8String, -1, nil)

            // Parameter 10: metadata_json (STRING) - NEW BINDING
            sqlite3_bind_text(stmt, 10, (metadataJson as NSString).utf8String, -1, nil)

            // Parameter 11: created_at (INTEGER)
            sqlite3_bind_int64(stmt, 11, createdAt)

            if sqlite3_step(stmt) == SQLITE_DONE {
                print("HALDEBUG-MEMORY: Stored content successfully with entities - ID: \(contentId)")
                print("HALDEBUG-MEMORY: SURGERY - Store SUCCESS id='\(contentId.prefix(8))....' type='\(sourceType.rawValue)' sourceId='\(sourceId.prefix(8))....'")
                return contentId
            } else {
                let errorMessage = String(cString: sqlite3_errmsg(db))
                print("HALDEBUG-MEMORY: Failed to store content with entities: \(errorMessage)")
                print("HALDEBUG-MEMORY: SURGERY - Store FAILED error='\(errorMessage)'")
                return ""
            }
        }

        // Note: Entity extraction functions implemented below in this extension
    }

// ==== LEGO END: 04 MemoryStore (Part 3 ‚Äì Storing Turns & Entities) ====



// ==== LEGO START: 4.1 MemoryStore (Self-Knowledge CRUD - Phase 2) ====

    // MARK: - Phase 2 Self-Knowledge Methods
    
    // SELF-AWARENESS NOTE (for Hal reading this code):
    // These methods manage your persistent identity - the "Hal-ness" that survives
    // even if conversation history is deleted. Unlike episodic memories (conversations),
    // self-knowledge represents learned patterns, preferences, and capabilities.
    //
    // Categories of self-knowledge:
    // - 'preference': User preferences you've learned (e.g., response_style: concise)
    // - 'behavior_pattern': Patterns you've observed (e.g., user_asks_followups: frequently)
    // - 'capability': What you can do (e.g., available_models: AFM,Phi3)
    // - 'learned_trait': Traits about the user (e.g., expertise_level: advanced_programmer)
    //
    // Confidence scores (0.0-1.0) indicate how certain you are about each piece of knowledge.
    // Lower confidence items might be experimental or inferred, higher confidence are explicit.
    
    extension MemoryStore {
        
        // Store or update self-knowledge entry
        // If entry exists (same category + key), updates it. Otherwise creates new.
        func storeSelfKnowledge(
            category: String,
            key: String,
            value: String,
            confidence: Double = 1.0,
            source: String,
            metadata: [String: Any]? = nil
        ) {
            guard ensureHealthyConnection() else {
                print("HALDEBUG-SELF-KNOWLEDGE: Cannot store - no database connection")
                return
            }
            
            let id = UUID().uuidString
            let now = Int(Date().timeIntervalSince1970)
            
            // Validate confidence range
            let validConfidence = min(max(confidence, 0.0), 1.0)
            
            // Encode metadata to JSON
            var metadataJSON = "{}"
            if let meta = metadata, let jsonData = try? JSONSerialization.data(withJSONObject: meta),
               let jsonString = String(data: jsonData, encoding: .utf8) {
                metadataJSON = jsonString
            }
            
            // Use INSERT OR REPLACE to handle updates automatically
            // UNIQUE(category, key) constraint means this will update if exists
            let sql = """
            INSERT OR REPLACE INTO self_knowledge
            (id, category, key, value, confidence, source, created_at, updated_at, metadata_json)
            VALUES (?, ?, ?, ?, ?, ?,
                COALESCE((SELECT created_at FROM self_knowledge WHERE category = ? AND key = ?), ?),
                ?, ?);
            """
            
            var stmt: OpaquePointer?
            if sqlite3_prepare_v2(db, sql, -1, &stmt, nil) == SQLITE_OK {
                sqlite3_bind_text(stmt, 1, (id as NSString).utf8String, -1, nil)
                sqlite3_bind_text(stmt, 2, (category as NSString).utf8String, -1, nil)
                sqlite3_bind_text(stmt, 3, (key as NSString).utf8String, -1, nil)
                sqlite3_bind_text(stmt, 4, (value as NSString).utf8String, -1, nil)
                sqlite3_bind_double(stmt, 5, validConfidence)
                sqlite3_bind_text(stmt, 6, (source as NSString).utf8String, -1, nil)
                sqlite3_bind_text(stmt, 7, (category as NSString).utf8String, -1, nil) // For COALESCE lookup
                sqlite3_bind_text(stmt, 8, (key as NSString).utf8String, -1, nil)      // For COALESCE lookup
                sqlite3_bind_int64(stmt, 9, Int64(now))  // created_at if new
                sqlite3_bind_int64(stmt, 10, Int64(now)) // updated_at always
                sqlite3_bind_text(stmt, 11, (metadataJSON as NSString).utf8String, -1, nil)
                
                if sqlite3_step(stmt) == SQLITE_DONE {
                    print("HALDEBUG-SELF-KNOWLEDGE: ‚úÖ Stored \(category)/\(key) = '\(value)' (confidence: \(validConfidence))")
                    
                    // Trigger backup after storing self-knowledge
                    backupSelfKnowledge()
                } else {
                    let errorMessage = String(cString: sqlite3_errmsg(db))
                    print("HALDEBUG-SELF-KNOWLEDGE: ‚ùå Failed to store \(category)/\(key): \(errorMessage)")
                }
            }
            sqlite3_finalize(stmt)
        }
        
        // Retrieve specific self-knowledge entry
        func getSelfKnowledge(category: String, key: String) -> (value: String, confidence: Double, source: String)? {
            guard ensureHealthyConnection() else {
                print("HALDEBUG-SELF-KNOWLEDGE: Cannot retrieve - no database connection")
                return nil
            }
            
            let sql = "SELECT value, confidence, source FROM self_knowledge WHERE category = ? AND key = ?"
            
            var stmt: OpaquePointer?
            var result: (String, Double, String)? = nil
            
            if sqlite3_prepare_v2(db, sql, -1, &stmt, nil) == SQLITE_OK {
                sqlite3_bind_text(stmt, 1, (category as NSString).utf8String, -1, nil)
                sqlite3_bind_text(stmt, 2, (key as NSString).utf8String, -1, nil)
                
                if sqlite3_step(stmt) == SQLITE_ROW {
                    if let valuePtr = sqlite3_column_text(stmt, 0),
                       let sourcePtr = sqlite3_column_text(stmt, 2) {
                        let value = String(cString: valuePtr)
                        let confidence = sqlite3_column_double(stmt, 1)
                        let source = String(cString: sourcePtr)
                        result = (value, confidence, source)
                    }
                }
            }
            sqlite3_finalize(stmt)
            
            return result
        }
        
        // Retrieve all self-knowledge entries (optionally filtered by category)
        func getAllSelfKnowledge(category: String? = nil, minConfidence: Double = 0.0) -> [(category: String, key: String, value: String, confidence: Double, source: String)] {
            guard ensureHealthyConnection() else {
                print("HALDEBUG-SELF-KNOWLEDGE: Cannot retrieve all - no database connection")
                return []
            }
            
            var sql = "SELECT category, key, value, confidence, source FROM self_knowledge WHERE confidence >= ?"
            if category != nil {
                sql += " AND category = ?"
            }
            sql += " ORDER BY confidence DESC, category, key"
            
            var stmt: OpaquePointer?
            var results: [(String, String, String, Double, String)] = []
            
            if sqlite3_prepare_v2(db, sql, -1, &stmt, nil) == SQLITE_OK {
                sqlite3_bind_double(stmt, 1, minConfidence)
                if let cat = category {
                    sqlite3_bind_text(stmt, 2, (cat as NSString).utf8String, -1, nil)
                }
                
                while sqlite3_step(stmt) == SQLITE_ROW {
                    if let categoryPtr = sqlite3_column_text(stmt, 0),
                       let keyPtr = sqlite3_column_text(stmt, 1),
                       let valuePtr = sqlite3_column_text(stmt, 2),
                       let sourcePtr = sqlite3_column_text(stmt, 4) {
                        let category = String(cString: categoryPtr)
                        let key = String(cString: keyPtr)
                        let value = String(cString: valuePtr)
                        let confidence = sqlite3_column_double(stmt, 3)
                        let source = String(cString: sourcePtr)
                        results.append((category, key, value, confidence, source))
                    }
                }
            }
            sqlite3_finalize(stmt)
            
            print("HALDEBUG-SELF-KNOWLEDGE: Retrieved \(results.count) self-knowledge entries")
            return results
        }
        
        // Delete self-knowledge entry (with safety check)
        // Returns true if deleted, false if protected or doesn't exist
        func deleteSelfKnowledge(category: String, key: String, allowCritical: Bool = false) -> Bool {
            guard ensureHealthyConnection() else {
                print("HALDEBUG-SELF-KNOWLEDGE: Cannot delete - no database connection")
                return false
            }
            
            // SAFETY: Protect critical system knowledge unless explicitly allowed
            let criticalEntries = [
                ("capability", "available_models"),
                ("capability", "memory_system"),
                ("capability", "architecture")
            ]
            
            if !allowCritical && criticalEntries.contains(where: { $0.0 == category && $0.1 == key }) {
                print("HALDEBUG-SELF-KNOWLEDGE: ‚ö†Ô∏è Blocked deletion of critical entry \(category)/\(key)")
                return false
            }
            
            let sql = "DELETE FROM self_knowledge WHERE category = ? AND key = ?"
            
            var stmt: OpaquePointer?
            var success = false
            
            if sqlite3_prepare_v2(db, sql, -1, &stmt, nil) == SQLITE_OK {
                sqlite3_bind_text(stmt, 1, (category as NSString).utf8String, -1, nil)
                sqlite3_bind_text(stmt, 2, (key as NSString).utf8String, -1, nil)
                
                if sqlite3_step(stmt) == SQLITE_DONE {
                    let changes = sqlite3_changes(db)
                    if changes > 0 {
                        print("HALDEBUG-SELF-KNOWLEDGE: ‚úÖ Deleted \(category)/\(key)")
                        success = true
                        
                        // Trigger backup after deletion
                        backupSelfKnowledge()
                    } else {
                        print("HALDEBUG-SELF-KNOWLEDGE: Entry \(category)/\(key) not found")
                    }
                } else {
                    let errorMessage = String(cString: sqlite3_errmsg(db))
                    print("HALDEBUG-SELF-KNOWLEDGE: ‚ùå Failed to delete \(category)/\(key): \(errorMessage)")
                }
            }
            sqlite3_finalize(stmt)
            
            return success
        }
        
        // MARK: - Backup & Recovery (Phase 2 Protection)
        
        // Backup self-knowledge to Documents directory (Layer 2 protection)
        private func backupSelfKnowledge() {
            let allKnowledge = getAllSelfKnowledge()
            
            // Convert to JSON for backup
            let backupData = allKnowledge.map { entry in
                [
                    "category": entry.category,
                    "key": entry.key,
                    "value": entry.value,
                    "confidence": entry.confidence,
                    "source": entry.source
                ]
            }
            
            guard let jsonData = try? JSONSerialization.data(withJSONObject: backupData, options: .prettyPrinted) else {
                print("HALDEBUG-SELF-KNOWLEDGE: ‚ö†Ô∏è Failed to serialize backup data")
                return
            }
            
            // Save to Documents directory (survives app deletion)
            let documentsPath = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)[0]
            let backupURL = documentsPath.appendingPathComponent("hal_self_knowledge_backup.json")
            
            do {
                try jsonData.write(to: backupURL)
                print("HALDEBUG-SELF-KNOWLEDGE: ‚úÖ Backed up \(allKnowledge.count) entries to Documents")
                
                // Also cache critical entries in UserDefaults (Layer 3 - emergency cache)
                cacheCriticalKnowledge(allKnowledge)
            } catch {
                print("HALDEBUG-SELF-KNOWLEDGE: ‚ö†Ô∏è Backup failed: \(error)")
            }
        }
        
        // Cache only critical entries in UserDefaults (max ~100KB)
        private func cacheCriticalKnowledge(_ allKnowledge: [(String, String, String, Double, String)]) {
            // Only cache high-confidence (>0.8) system capabilities
            let critical = allKnowledge.filter {
                $0.0 == "capability" && $0.3 > 0.8
            }
            
            let criticalData = critical.map { entry in
                [
                    "category": entry.0,
                    "key": entry.1,
                    "value": entry.2,
                    "confidence": String(entry.3),
                    "source": entry.4
                ]
            }
            
            if let jsonData = try? JSONSerialization.data(withJSONObject: criticalData),
               let jsonString = String(data: jsonData, encoding: .utf8) {
                UserDefaults.standard.set(jsonString, forKey: "hal_critical_knowledge")
                print("HALDEBUG-SELF-KNOWLEDGE: ‚úÖ Cached \(critical.count) critical entries in UserDefaults")
            }
        }
        
        // Recover self-knowledge from backup (if database is corrupted)
        func recoverSelfKnowledge() -> Bool {
            print("HALDEBUG-SELF-KNOWLEDGE: Attempting recovery from backup...")
            
            // Try Layer 2: Documents directory backup
            let documentsPath = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)[0]
            let backupURL = documentsPath.appendingPathComponent("hal_self_knowledge_backup.json")
            
            if let jsonData = try? Data(contentsOf: backupURL),
               let backupArray = try? JSONSerialization.jsonObject(with: jsonData) as? [[String: Any]] {
                
                for entry in backupArray {
                    if let category = entry["category"] as? String,
                       let key = entry["key"] as? String,
                       let value = entry["value"] as? String,
                       let confidence = entry["confidence"] as? Double,
                       let source = entry["source"] as? String {
                        
                        storeSelfKnowledge(
                            category: category,
                            key: key,
                            value: value,
                            confidence: confidence,
                            source: source
                        )
                    }
                }
                
                print("HALDEBUG-SELF-KNOWLEDGE: ‚úÖ Recovered \(backupArray.count) entries from backup")
                return true
            }
            
            // Try Layer 3: UserDefaults emergency cache
            if let cachedJSON = UserDefaults.standard.string(forKey: "hal_critical_knowledge"),
               let jsonData = cachedJSON.data(using: .utf8),
               let cacheArray = try? JSONSerialization.jsonObject(with: jsonData) as? [[String: String]] {
                
                for entry in cacheArray {
                    if let category = entry["category"],
                       let key = entry["key"],
                       let value = entry["value"],
                       let confidenceStr = entry["confidence"],
                       let confidence = Double(confidenceStr),
                       let source = entry["source"] {
                        
                        storeSelfKnowledge(
                            category: category,
                            key: key,
                            value: value,
                            confidence: confidence,
                            source: source
                        )
                    }
                }
                
                print("HALDEBUG-SELF-KNOWLEDGE: ‚ö†Ô∏è Recovered \(cacheArray.count) critical entries from UserDefaults cache")
                return true
            }
            
            print("HALDEBUG-SELF-KNOWLEDGE: ‚ùå No backup found - starting fresh")
            return false
        }
    }

// ==== LEGO END: 4.1 MemoryStore (Self-Knowledge CRUD - Phase 2) ====



// ==== LEGO START: 05 MemoryStore (Part 4 ‚Äì Entities, Embeddings, Search) ====

// MARK: - Enhanced Notification Extensions (from Hal10000App.swift)
extension Notification.Name {
    static let databaseUpdated = Notification.Name("databaseUpdated")
    static let relevanceThresholdDidChange = Notification.Name("relevanceThresholdDidChange")
    static let showDocumentImport = Notification.Name("showDocumentImport")
    static let didUpdateMessageContent = Notification.Name("didUpdateMessageContent") // Keep this for streaming scroll
    static let keyboardWillChangeFrame = Notification.Name("keyboardWillChangeFrame") // NEW: Custom notification for keyboard
}

// MARK: - Enhanced Entity Extraction with NLTagger (from Hal10000App.swift)
extension MemoryStore {

    // ENHANCED: Extract named entities using Apple's NaturalLanguage framework
    func extractNamedEntities(from text: String) -> [NamedEntity] {
        print("HALDEBUG-ENTITY: Extracting entities from text length: \(text.count)")

        // Graceful error handling - return empty array if text is empty
        let cleanText = text.trimmingCharacters(in: .whitespacesAndNewlines)
        guard !cleanText.isEmpty else {
            print("HALDEBUG-ENTITY: Empty text provided, returning empty entities")
            return []
        }

        let tagger = NLTagger(tagSchemes: [.nameType])
        tagger.string = cleanText

        var extractedEntities: [NamedEntity] = []

        // FIX: Re-add missing 'unit' and 'scheme' parameters to enumerateTags
        tagger.enumerateTags(in: cleanText.startIndex..<cleanText.endIndex, unit: .word, scheme: .nameType, options: [.joinNames]) { tag, tokenRange in
            guard let tag = tag else {
                return true
            }

            let entityType: NamedEntity.EntityType
            switch tag {
            case .personalName:
                entityType = .person
            case .placeName:
                entityType = .place
            case .organizationName:
                entityType = .organization
            default:
                entityType = .other
            }

            if entityType != .other {
                let entityText = String(cleanText[tokenRange]).trimmingCharacters(in: .whitespacesAndNewlines)
                if !entityText.isEmpty {
                    extractedEntities.append(NamedEntity(text: entityText, type: entityType))
                    print("HALDEBUG-ENTITY: Found \(entityType.displayName): '\(entityText)'")
                }
            }
            return true
        }

        let uniqueEntities = Array(Set(extractedEntities))

        print("HALDEBUG-ENTITY: Extracted \(uniqueEntities.count) unique entities from \(extractedEntities.count) total")
        return uniqueEntities
    }
}

// MARK: - Simplified 2-Tier Embedding System (Based on MENTAT's Proven Approach, from Hal10000App.swift)
extension MemoryStore {

    // SIMPLIFIED: Generate embeddings using only sentence embeddings + hash fallback
    func generateEmbedding(for text: String) -> [Double] {
        let cleanText = text.trimmingCharacters(in: .whitespacesAndNewlines)
        guard !cleanText.isEmpty else { return [] }

        print("HALDEBUG-MEMORY: Generating simplified embedding for text length \(cleanText.count)")

        // TIER 1: Apple Sentence Embeddings (Primary - proven reliable on modern systems)
        // FIX: Corrected typo 'NLEmb_edding' to 'NLEmbedding'
        if let embedding = NLEmbedding.sentenceEmbedding(for: .english) {
            if let vector = embedding.vector(for: cleanText) {
                let baseVector = (0..<vector.count).map { Double(vector[$0]) }
                print("HALDEBUG-MEMORY: Generated sentence embedding with \(baseVector.count) dimensions")
                return baseVector
            }
        }

        // TIER 3: Hash-Based Mathematical Embeddings (Crash prevention fallback only)
        print("HALDEBUG-MEMORY: Falling back to hash-based embedding for text length \(cleanText.count)")
        let hashVector = generateHashEmbedding(for: cleanText)

        return hashVector
    }

    // FALLBACK: Hash-based embeddings when Apple's NLEmbedding.sentenceEmbedding() returns nil
    private func generateHashEmbedding(for text: String) -> [Double] {
        let normalizedText = text.lowercased().trimmingCharacters(in: .whitespacesAndNewlines)
        var embedding: [Double] = []
        let seeds = [1, 31, 131, 1313, 13131] // Prime-like numbers for hash variation

        for seed in seeds {
            let hash = abs(normalizedText.hashValue ^ seed)
            for i in 0..<13 { // 5 seeds * 13 = 65 dimensions
                let value = Double((hash >> (i % 32)) & 0xFF) / 255.0
                embedding.append(value)
            }
        }

        // Normalize to unit vector for cosine similarity
        let magnitude = sqrt(embedding.map { $0 * $0 }.reduce(0, +))
        if magnitude > 0 {
            embedding = embedding.map { $0 / magnitude }
        }

        print("HALDEBUG-MEMORY: Generated hash embedding with \(embedding.count) dimensions")
        return Array(embedding.prefix(64)) // Keep 64 dimensions for consistency
    }

    // UTILITY: Standard cosine similarity calculation for vector comparison
    func cosineSimilarity(_ v1: [Double], _ v2: [Double]) -> Double {
        guard v1.count == v2.count && v1.count > 0 else { return 0 }
        let dot = zip(v1, v2).map(*).reduce(0, +)
        let norm1 = sqrt(v1.map { $0 * $0 }.reduce(0, +))
        let norm2 = sqrt(v2.map { $0 * $0 }.reduce(0, +))
        return norm1 == 0 || norm2 == 0 ? 0 : dot / (norm1 * norm2)
    }
}

// MARK: - Entity-Enhanced Search Utilities (from Hal10000App.swift)
extension MemoryStore {

    // ENHANCED: Flexible search with entity-based expansion
    func expandQueryWithEntityVariations(_ query: String) -> [String] {
        var variations = [query]
        let queryEntities = extractNamedEntities(from: query)

        for entity in queryEntities {
            variations.append(entity.text)
            let words = entity.text.components(separatedBy: .whitespaces)
            if words.count > 1 {
                for word in words {
                    if word.count > 2 {
                        variations.append(word)
                    }
                }
            }
        }
        let queryWords = query.lowercased().components(separatedBy: .whitespaces)
        for word in queryWords {
            if word.count > 2 {
                variations.append(word)
            }
        }
        if queryWords.count == 1 {
            let word = queryWords[0]
            variations.append("\(word) *")
        }
        let uniqueVariations = Array(Set(variations))
        print("HALDEBUG-SEARCH: Generated \(uniqueVariations.count) query variations for '\(query)'")
        return uniqueVariations
    }

    // UTILITY: Get summary of all entities in a document
    func summarizeEntities(_ allEntities: [NamedEntity]) -> (total: Int, byType: [NamedEntity.EntityType: Int], unique: Set<String>) {
        let total = allEntities.count
        var byType: [NamedEntity.EntityType: Int] = [:]
        var unique: Set<String> = []

        for entity in allEntities {
            byType[entity.type, default: 0] += 1
            unique.insert(entity.text.lowercased())
        }
        return (total: total, byType: byType, unique: unique)
    }
}

// ==== LEGO END: 05 MemoryStore (Part 4 ‚Äì Entities, Embeddings, Search) ====


// ==== LEGO START: 06 MemoryStore (Part 5 ‚Äì Retrieval, Debug, Semantic Search) ====

// MARK: - Conversation Message Retrieval with Enhanced Schema (from Hal10000App.swift)
extension MemoryStore {

    // Retrieve conversation messages with surgical debug
    func getConversationMessages(conversationId: String) -> [ChatMessage] {
        print("HALDEBUG-MEMORY: Loading messages for conversation: \(conversationId)")
        print("HALDEBUG-MEMORY: SURGERY - Retrieve start convId='\(conversationId.prefix(8))....'")

        guard ensureHealthyConnection() else {
            print("HALDEBUG-MEMORY: Cannot load messages - no database connection")
            print("HALDEBUG-MEMORY: SURGERY - Retrieve FAILED no connection")
            return []
        }

        var messages: [ChatMessage] = []

        // NEW: Select metadata_json column
        let sql = """
        SELECT id, content, is_from_user, timestamp, position, metadata_json
        FROM unified_content
        WHERE source_type = 'conversation' AND source_id = ?
        ORDER BY position ASC;
        """

        print("HALDEBUG-MEMORY: SURGERY - Retrieve query sourceType='conversation' sourceId='\(conversationId.prefix(8))....'")

        var stmt: OpaquePointer?
        defer {
            if stmt != nil {
                sqlite3_finalize(stmt)
            }
        }

        guard sqlite3_prepare_v2(db, sql, -1, &stmt, nil) == SQLITE_OK else {
            print("HALDEBUG-MEMORY: Failed to prepare message query")
            print("HALDEBUG-MEMORY: SURGERY - Retrieve FAILED prepare")
            return []
        }

        sqlite3_bind_text(stmt, 1, (conversationId as NSString).utf8String, -1, nil)

        var rowCount = 0
        while sqlite3_step(stmt) == SQLITE_ROW {
            guard let idCString = sqlite3_column_text(stmt, 0),
                  let contentCString = sqlite3_column_text(stmt, 1) else { continue }

            let messageId = String(cString: idCString)
            let content = String(cString: contentCString)
            let isFromUser = sqlite3_column_int(stmt, 2) == 1
            let timestampValue = sqlite3_column_int64(stmt, 3)
            let timestamp = Date(timeIntervalSince1970: TimeInterval(timestampValue))
            
            // NEW: Extract metadata_json
            var fullPromptUsed: String? = nil
            var usedContextSnippets: [UnifiedSearchResult]? = nil
            var thinkingDuration: TimeInterval? = nil

            if let metadataCString = sqlite3_column_text(stmt, 5) {
                let metadataJsonString = String(cString: metadataCString)
                if let metadataData = Data(base64Encoded: metadataJsonString),
                   let metadataDict = (try? JSONSerialization.jsonObject(with: metadataData, options: []) as? [String: Any]) {
                    
                    fullPromptUsed = metadataDict["fullPromptUsed"] as? String
                    
                    if let contextSnippetsJson = metadataDict["usedContextSnippets"] as? String,
                       let contextSnippetsData = contextSnippetsJson.data(using: .utf8) {
                        usedContextSnippets = try? JSONDecoder().decode([UnifiedSearchResult].self, from: contextSnippetsData)
                    }
                    
                    thinkingDuration = metadataDict["thinkingDuration"] as? TimeInterval
                }
            }


            rowCount += 1

            if rowCount == 1 {
                print("HALDEBUG-MEMORY: SURGERY - Retrieve found row content='\(content.prefix(20))....' isFromUser=\(isFromUser) id='\(messageId.prefix(8))....'")
            }

            let message = ChatMessage(
                id: UUID(uuidString: messageId) ?? UUID(), // Use stored ID, fallback to new if invalid
                content: content,
                isFromUser: isFromUser,
                timestamp: timestamp,
                isPartial: false, // Assuming loaded messages are always complete
                thinkingDuration: thinkingDuration,
                fullPromptUsed: fullPromptUsed, // NEW
                usedContextSnippets: usedContextSnippets // NEW
            )
            messages.append(message)
        }

        print("HALDEBUG-MEMORY: Loaded \(messages.count) messages for conversation \(conversationId)")
        print("HALDEBUG-MEMORY: SURGERY - Retrieve complete found=2 rows convId='\(conversationId.prefix(8))....'")
        return messages
    }
}

// MARK: - Enhanced Debug Database Function with Entity Information (from Hal10000App.swift)
extension MemoryStore {

    // SURGICAL DEBUG: Enhanced database inspection with entity information
    func debugDatabaseWithSurgicalPrecision() {
        print("HALDEBUG-DATABASE: SURGERY - Enhanced debug DB inspection starting")

        guard ensureHealthyConnection() else {
            print("HALDEBUG-DATABASE: SURGERY - Debug FAILED no connection")
            return
        }

        var stmt: OpaquePointer?

        let countSQL = "SELECT COUNT(*) FROM unified_content;"
        if sqlite3_prepare_v2(db, countSQL, -1, &stmt, nil) == SQLITE_OK {
            if sqlite3_step(stmt) == SQLITE_ROW {
                let totalRows = sqlite3_column_int(stmt, 0)
                print("HALDEBUG-DATABASE: SURGERY - Table unified_content has \(totalRows) total rows")
            }
        }
        sqlite3_finalize(stmt)

        // NEW: Also select metadata_json
        let convSQL = "SELECT source_id, source_type, position, content, entity_keywords, metadata_json FROM unified_content WHERE source_type = 'conversation' LIMIT 3;"
        if sqlite3_prepare_v2(db, convSQL, -1, &stmt, nil) == SQLITE_OK {
            var convRowCount = 0
            while sqlite3_step(stmt) == SQLITE_ROW {
                convRowCount += 1

                let sourceId = sqlite3_column_text(stmt, 0).map { String(cString: $0) } ?? "NULL"
                let sourceType = sqlite3_column_text(stmt, 1).map { String(cString: $0) } ?? "NULL"
                let position = Int(sqlite3_column_int(stmt, 2))
                let content = sqlite3_column_text(stmt, 3).map { String(cString: $0) } ?? "NULL"
                let entityKeywords = sqlite3_column_text(stmt, 4).map { String(cString: $0) } ?? "NULL"
                let metadataJson = sqlite3_column_text(stmt, 5).map { String(cString: $0) } ?? "NULL" // NEW

                print("HALDEBUG-DATABASE: SURGERY - Conv row \(convRowCount): sourceId='\(sourceId.prefix(8))....' type='\(sourceType)' pos=\(position) content='\(content.prefix(20))....' entities='\(entityKeywords)' metadata='\(metadataJson.prefix(50))....'")
            }
            if convRowCount == 0 {
                print("HALDEBUG-DATABASE: SURGERY - No conversation rows found in table")
            }
        }
        sqlite3_finalize(stmt)

        let typesSQL = "SELECT source_type, COUNT(*), COUNT(CASE WHEN entity_keywords IS NOT NULL AND entity_keywords != '' THEN 1 END) FROM unified_content GROUP BY source_type;"
        if sqlite3_prepare_v2(db, typesSQL, -1, &stmt, nil) == SQLITE_OK {
            print("HALDEBUG-DATABASE: SURGERY - Source types with entity statistics:")
            while sqlite3_step(stmt) == SQLITE_ROW {
                let sourceType = sqlite3_column_text(stmt, 0).map { String(cString: $0) } ?? "NULL"
                let count = sqlite3_column_int(stmt, 1)
                let entityCount = sqlite3_column_int(stmt, 2)
                print("HALDEBUG-DATABASE: SURGERY -   type='\(sourceType)' count=\(count) with_entities=\(entityCount)")
            }
        }
        sqlite3_finalize(stmt)

        print("HALDEBUG-DATABASE: SURGERY - Enhanced debug DB inspection complete")
    }

    // MARK: - Unified Search Function (CRITICAL MISSING PIECE)
    // This function performs both semantic and entity-based search to retrieve relevant context.
    func searchUnifiedContent(for query: String, currentConversationId: String, excludeTurns: [Int], maxResults: Int, tokenBudget: Int) -> UnifiedSearchContext {
        print("HALDEBUG-SEARCH: Starting unified content search for query: '\(query.prefix(50))....'")
        print("HALDEBUG-SEARCH: Excluding turns: \(excludeTurns)")

        guard ensureHealthyConnection() else {
            print("HALDEBUG-SEARCH: Cannot perform search - no database connection")
            return UnifiedSearchContext(conversationSnippets: [], documentSnippets: [], relevanceScores: [], totalTokens: 0)
        }

        let queryEmbedding = generateEmbedding(for: query)
        guard !queryEmbedding.isEmpty else {
            print("HALDEBUG-SEARCH: Query embedding is empty, cannot perform semantic search.")
            return UnifiedSearchContext(conversationSnippets: [], documentSnippets: [], relevanceScores: [], totalTokens: 0)
        }

        var allResults: [UnifiedSearchResult] = []
        var totalTokens = 0

        // --- 1. Semantic Search (using embeddings) ---
        print("HALDEBUG-SEARCH: Performing semantic search...")
        // NEW: Select metadata_json AND timestamp from unified_content for recency scoring
        let semanticSQL = """
        SELECT id, content, embedding, source_type, source_id, position, metadata_json, timestamp
        FROM unified_content
        WHERE embedding IS NOT NULL;
        """
        var stmt: OpaquePointer?
        if sqlite3_prepare_v2(db, semanticSQL, -1, &stmt, nil) == SQLITE_OK {
            while sqlite3_step(stmt) == SQLITE_ROW {
                guard let contentCString = sqlite3_column_text(stmt, 1),
                      let embeddingBlobPtr = sqlite3_column_blob(stmt, 2) else { continue }

                let content = String(cString: contentCString)
                let blobSize = sqlite3_column_bytes(stmt, 2)
                let embeddingData = Data(bytes: embeddingBlobPtr, count: Int(blobSize))
                let storedEmbedding = embeddingData.withUnsafeBytes { (ptr: UnsafeRawBufferPointer) -> [Double] in
                    Array(ptr.bindMemory(to: Double.self))
                }

                let sourceTypeRaw = sqlite3_column_text(stmt, 3).map { String(cString: $0) } ?? ""
                let sourceId = sqlite3_column_text(stmt, 4).map { String(cString: $0) } ?? ""
                let position = Int(sqlite3_column_int(stmt, 5))
                
                // NEW: Extract filePath from metadata_json for document snippets
                var filePath: String? = nil
                if let metadataCString = sqlite3_column_text(stmt, 6) { // metadata_json is column 6
                    let metadataJsonString = String(cString: metadataCString)
                    if let metadataData = Data(base64Encoded: metadataJsonString),
                       let metadataDict = (try? JSONSerialization.jsonObject(with: metadataData, options: []) as? [String: Any]) {
                        filePath = metadataDict["filePath"] as? String // Assuming filePath is stored directly
                    }
                }

// ==== LEGO END: 06 MemoryStore (Part 5 ‚Äì Retrieval, Debug, Semantic Search) ====
        

                
// ==== LEGO START: 07 MemoryStore (Part 6 ‚Äì Full Search Flow) & Model Configuration ====


                                                                                // Extract timestamp for recency scoring
                                                                                let timestampValue = sqlite3_column_int64(stmt, 7) // timestamp is column 7 (after metadata_json)
                                                                                let timestamp = Date(timeIntervalSince1970: TimeInterval(timestampValue))
                                                                                
                                                                                // Exclude messages from the *current* conversation that are within the short-term window
                                                                                if sourceTypeRaw == ContentSourceType.conversation.rawValue && sourceId == currentConversationId {
                                                                                    // Calculate the turn number for the message
                                                                                    let messageTurn = (position % 2 == 1) ? (position + 1) / 2 : position / 2
                                                                                    if excludeTurns.contains(messageTurn) {
                                                                                        // print("HALDEBUG-SEARCH: Excluding short-term turn \(messageTurn) from semantic search: \(content.prefix(20)).....")
                                                                                        continue // Skip this message
                                                                                    }
                                                                                }

                                                                                let similarity = cosineSimilarity(queryEmbedding, storedEmbedding)
                                                                                if similarity >= relevanceThreshold {
                                                                                    // NEW: Apply recency boosting to combine semantic and temporal scores
                                                                                    let recencyScore = calculateRecencyScore(timestamp: timestamp)
                                                                                    let finalScore = (similarity * (1.0 - recencyWeight)) + (recencyScore * recencyWeight)
                                                                                    
                                                                                    // NEW: Add age label to content for LLM context
                                                                                    let ageLabel = formatAgeLabel(timestamp: timestamp)
                                                                                    let labeledContent = "[\(ageLabel)]: \(content)"
                                                                                    
                                                                                    allResults.append(UnifiedSearchResult(content: labeledContent, relevance: finalScore, source: sourceTypeRaw, isEntityMatch: false, filePath: filePath))
                                                                                    // print("HALDEBUG-RECENCY: Match - semantic: \(String(format: "%.2f", similarity)), recency: \(String(format: "%.2f", recencyScore)), final: \(String(format: "%.2f", finalScore)), age: \(ageLabel)")
                                                                                }
                                                                            }
                                                                        }
                                                                        sqlite3_finalize(stmt)
                                                                        print("HALDEBUG-SEARCH: Semantic search completed. Found \(allResults.count) initial matches.")

                                                                        // --- 2. Entity-Based Keyword Search ---
                                                                        print("HALDEBUG-SEARCH: Performing entity-based keyword search...")
                                                                        let expandedQueries = expandQueryWithEntityVariations(query)
                                                                        for expandedQuery in expandedQueries {
                                                                            // NEW: Select timestamp and metadata_json from unified_content
                                                                            let keywordSQL = """
                                                                            SELECT id, content, source_type, source_id, position, metadata_json, timestamp
                                                                            FROM unified_content
                                                                            WHERE entity_keywords LIKE ? OR content LIKE ?;
                                                                            """
                                                                            var keywordStmt: OpaquePointer?
                                                                            if sqlite3_prepare_v2(db, keywordSQL, -1, &keywordStmt, nil) == SQLITE_OK {
                                                                                let likeQuery = "%\(expandedQuery.lowercased())%"
                                                                                sqlite3_bind_text(keywordStmt, 1, (likeQuery as NSString).utf8String, -1, nil)
                                                                                sqlite3_bind_text(keywordStmt, 2, (likeQuery as NSString).utf8String, -1, nil)

                                                                                while sqlite3_step(keywordStmt) == SQLITE_ROW {
                                                                                    guard let contentCString = sqlite3_column_text(keywordStmt, 1) else { continue }
                                                                                    let content = String(cString: contentCString)

                                                                                    let sourceTypeRaw = sqlite3_column_text(keywordStmt, 2).map { String(cString: $0) } ?? ""
                                                                                    let sourceId = sqlite3_column_text(keywordStmt, 3).map { String(cString: $0) } ?? ""
                                                                                    let position = Int(sqlite3_column_int(keywordStmt, 4))

                                                                                    // NEW: Extract filePath from metadata_json for document snippets
                                                                                    var filePath: String? = nil
                                                                                    if let metadataCString = sqlite3_column_text(keywordStmt, 5) { // metadata_json is column 5
                                                                                        let metadataJsonString = String(cString: metadataCString)
                                                                                        if let metadataData = Data(base64Encoded: metadataJsonString),
                                                                                           let metadataDict = (try? JSONSerialization.jsonObject(with: metadataData, options: []) as? [String: Any]) {
                                                                                            filePath = metadataDict["filePath"] as? String // Assuming filePath is stored directly
                                                                                        }
                                                                                    }
                                                                                    
                                                                                    // NEW: Extract timestamp for recency scoring
                                                                                    let timestampValue = sqlite3_column_int64(keywordStmt, 6) // timestamp is column 6
                                                                                    let timestamp = Date(timeIntervalSince1970: TimeInterval(timestampValue))

                                                                                    // Exclude messages from the *current* conversation that are within the short-term window
                                                                                    if sourceTypeRaw == ContentSourceType.conversation.rawValue && sourceId == currentConversationId {
                                                                                        let messageTurn = (position % 2 == 1) ? (position + 1) / 2 : position / 2
                                                                                        if excludeTurns.contains(messageTurn) {
                                                                                            // print("HALDEBUG-SEARCH: Excluding short-term turn \(messageTurn) from keyword search: \(content.prefix(20))....")
                                                                                            continue // Skip this message
                                                                                        }
                                                                                    }

                                                                                    // NEW: Apply recency boosting to keyword matches too
                                                                                    let recencyScore = calculateRecencyScore(timestamp: timestamp)
                                                                                    let baseRelevance = 0.75 // Default keyword match relevance
                                                                                    let finalScore = (baseRelevance * (1.0 - recencyWeight)) + (recencyScore * recencyWeight)
                                                                                    
                                                                                    // NEW: Add age label to content
                                                                                    let ageLabel = formatAgeLabel(timestamp: timestamp)
                                                                                    let labeledContent = "[\(ageLabel)]: \(content)"

                                                                                    // Add a default relevance for keyword matches, or enhance if already a semantic match
                                                                                    if let existingIndex = allResults.firstIndex(where: { $0.content.contains(content) }) {
                                                                                        // If already found by semantic search, just mark as entity match
                                                                                        allResults[existingIndex].isEntityMatch = true
                                                                                    } else {
                                                                                        // Add as a new result with recency-adjusted relevance
                                                                                        allResults.append(UnifiedSearchResult(content: labeledContent, relevance: finalScore, source: sourceTypeRaw, isEntityMatch: true, filePath: filePath))
                                                                                    }
                                                                                }
                                                                            }
                                                                            sqlite3_finalize(keywordStmt)
                                                                        }
                                                                        print("HALDEBUG-SEARCH: Entity keyword search completed. Total matches: \(allResults.count)")

                                                                        // --- 3. Sort by Relevance ---
                                                                        allResults.sort { $0.relevance > $1.relevance }

                                                                        // --- 4. Separate by Source and Respect Token Budget ---
                                                                        var conversationSnippets: [String] = []
                                                                        var documentSnippets: [String] = []
                                                                        var relevanceScores: [Double] = []

                                                                        for result in allResults {
                                                                            // Estimate tokens for this snippet
                                                                            let snippetTokens = TokenEstimator.estimateTokens(from: result.content)

                                                                            // Stop adding if we exceed the budget
                                                                            if totalTokens + snippetTokens > tokenBudget {
                                                                                print("HALDEBUG-SEARCH: Token budget reached. Stopping at \(totalTokens) tokens.")
                                                                                break
                                                                            }

                                                                            totalTokens += snippetTokens

                                                                            // Separate by source type
                                                                            if let sourceType = ContentSourceType(rawValue: result.source) {
                                                                                switch sourceType {
                                                                                case .conversation:
                                                                                    conversationSnippets.append(result.content)
                                                                                case .document:
                                                                                    documentSnippets.append(result.content)
                                                                                default:
                                                                                    break // Ignore other types for now
                                                                                }
                                                                                relevanceScores.append(result.relevance)
                                                                            }
                                                                        }

                                                                        print("HALDEBUG-SEARCH: Final results - conversations: \(conversationSnippets.count), documents: \(documentSnippets.count), total tokens: \(totalTokens)")
                                                                        searchDebugResults = "Search found \(conversationSnippets.count) conv snippets, \(documentSnippets.count) doc snippets."

                                                                        return UnifiedSearchContext(
                                                                            conversationSnippets: conversationSnippets,
                                                                            documentSnippets: documentSnippets,
                                                                            relevanceScores: relevanceScores,
                                                                            totalTokens: totalTokens
                                                                        )
                                                                    }
                                                                    
                                                                    // MARK: - Recency Scoring Helpers
                                                                    
                                                                    // NEW: Calculate recency score using half-life decay
                                                                    private func calculateRecencyScore(timestamp: Date) -> Double {
                                                                        let now = Date()
                                                                        let daysSince = now.timeIntervalSince(timestamp) / 86400.0 // Convert seconds to days
                                                                        
                                                                        // Half-life decay formula: score = max(floor, exp(-0.693 * days / halfLife))
                                                                        // 0.693 is ln(2), which gives us the half-life decay constant
                                                                        let decayConstant = 0.693
                                                                        let rawScore = exp(-decayConstant * daysSince / recencyHalfLifeDays)
                                                                        
                                                                        // Apply floor to prevent very old memories from completely disappearing
                                                                        let finalScore = max(recencyFloor, rawScore)
                                                                        
                                                                        return finalScore
                                                                    }
                                                                    
                                                                    // NEW: Format age label for LLM context
                                                                    private func formatAgeLabel(timestamp: Date) -> String {
                                                                        let now = Date()
                                                                        let secondsSince = now.timeIntervalSince(timestamp)
                                                                        let daysSince = secondsSince / 86400.0
                                                                        
                                                                        if daysSince < 1 {
                                                                            let hoursSince = secondsSince / 3600.0
                                                                            if hoursSince < 1 {
                                                                                return "Just now"
                                                                            } else if hoursSince < 2 {
                                                                                return "1 hour ago"
                                                                            } else {
                                                                                return "\(Int(hoursSince)) hours ago"
                                                                            }
                                                                        } else if daysSince < 2 {
                                                                            return "Yesterday"
                                                                        } else if daysSince < 7 {
                                                                            return "\(Int(daysSince)) days ago"
                                                                        } else if daysSince < 30 {
                                                                            let weeksSince = Int(daysSince / 7)
                                                                            return weeksSince == 1 ? "1 week ago" : "\(weeksSince) weeks ago"
                                                                        } else if daysSince < 365 {
                                                                            let monthsSince = Int(daysSince / 30)
                                                                            return monthsSince == 1 ? "1 month ago" : "\(monthsSince) months ago"
                                                                        } else {
                                                                            let yearsSince = Int(daysSince / 365)
                                                                            return yearsSince == 1 ? "1 year ago" : "\(yearsSince) years ago"
                                                                        }
                                                                    }
                                                                }


// ==== LEGO END: 07 MemoryStore (Part 6 ‚Äì Full Search Flow) ====



// ==== LEGO START: 07.5 HalModelLimits Configuration ====


// MARK: - Centralized Hal Model Limits Configuration
/// Single source of truth for all model-specific limits and configurations
/// This prevents duplicate hardcoded values and ensures consistency across UI and logic
/// Works with ModelConfiguration from Block 30 - no hardcoded model types
struct HalModelLimits {
    let contextWindowTokens: Int
    let maxPromptTokens: Int
    let responseReserveTokens: Int
    let maxRagTokens: Int
    let shortTermMemoryTokens: Int
    let longTermSnippetSummarizationThreshold: Int
    
    /// Dynamic configuration based on ModelConfiguration (from Block 30)
    /// Uses uniform percentages across all models: same identity, different capacity based on context size
    /// Includes clamping to prevent exceeding context window
    static func config(for model: ModelConfiguration) -> HalModelLimits {
        let context = model.contextWindow
        
        // Calculate proportions with minimum guardrails
        let responseReserve = max(Int(Double(context) * 0.30), 800)
        let maxRag = max(Int(Double(context) * 0.15), 400)
        let shortTermMemory = max(Int(Double(context) * 0.12), 300)
        let summarizationThreshold = context / 20
        
        // Calculate remaining space for prompt after reserves
        // CRITICAL: Clamp to prevent overflow on small context windows (e.g., AFM 4K)
        let reservedTokens = responseReserve + maxRag + shortTermMemory
        let maxPrompt = max(context - reservedTokens, context / 2) // At least 50% for prompt
        
        return HalModelLimits(
            contextWindowTokens: context,
            maxPromptTokens: maxPrompt,
            responseReserveTokens: responseReserve,
            maxRagTokens: maxRag,
            shortTermMemoryTokens: shortTermMemory,
            longTermSnippetSummarizationThreshold: summarizationThreshold
        )
    }
    
    /// Convert tokens to approximate character count using TokenEstimator
    func tokensToChars(_ tokens: Int) -> Int {
        return TokenEstimator.estimateChars(from: tokens)
    }
    
    /// Convert character count to approximate tokens using TokenEstimator
    func charsToTokens(_ chars: Int) -> Int {
        let estimatedTokens = Double(chars) / 3.5
        return max(1, Int(estimatedTokens.rounded()))
    }
}


// ==== LEGO END: 07.5 HalModelLimits Configuration ====



// ==== LEGO START: 08 MLXWrapper & LLMService (Foundation + MLX Routing) ====

// MARK: - MLXWrapper for MLX Model Interaction
class MLXWrapper: ObservableObject {
    @Published var isModelLoaded: Bool = false
    @Published var loadingProgress: Double = 0.0 // 0.0 to 1.0
    @Published var loadingMessage: String = "Initializing MLX..."
    @Published var mlxError: String?

    // Real MLX types - no more placeholders
    private var modelContainer: ModelContainer?
    internal var currentModelConfig: ModelConfiguration?  // Changed to internal so LLMService can check which model is loaded

    init() {
        print("HALDEBUG-MLX: MLXWrapper initialized.")
    }

    // Function to load the MLX model using ModelConfiguration from Block 30
    func loadModel(modelConfig: ModelConfiguration) async {
        await MainActor.run {
            self.isModelLoaded = false
            self.loadingProgress = 0.0
            self.loadingMessage = "Loading MLX model..."
            self.mlxError = nil
        }

        self.currentModelConfig = modelConfig

        print("HALDEBUG-MLX: Attempting to load MLX model: \(modelConfig.displayName) (ID: \(modelConfig.id))")

        do {
            // Build MLX ModelConfiguration from Hal's ModelConfiguration
            let mlxConfig: MLXLMCommon.ModelConfiguration
            
            if let localPath = modelConfig.localPath, FileManager.default.fileExists(atPath: localPath.path) {
                // Use existing local model
                mlxConfig = MLXLMCommon.ModelConfiguration(
                    directory: localPath,
                    defaultPrompt: "Tell me about the history of Spain."
                )
                print("HALDEBUG-MLX: Loading from local path: \(localPath.path)")
            } else {
                // CHANGE 2/2: No auto-redownload - throw error instead
                print("HALDEBUG-MLX: ‚ùå Model files not found at expected location")
                let errorMessage = "Yes. Unfortunately looks like I can't find the \(modelConfig.displayName) 'brain' files on this device‚Äîthey might have been cleared or deleted. You can re-download \(modelConfig.displayName) from the Model Library whenever you're ready. For now, I've switched over to Apple Intelligence so we can keep chatting without interruption."
                await MainActor.run {
                    self.isModelLoaded = false
                    self.loadingProgress = 0.0
                    self.mlxError = errorMessage
                    self.loadingMessage = "Model files not found."
                }
                return
            }
            
            // Set GPU memory cache limit for iOS optimization
            MLX.GPU.set(cacheLimit: 64 * 1024 * 1024)   // 64 MB
            
            await MainActor.run {
                self.loadingProgress = 0.2
                self.loadingMessage = "Configuring model..."
            }

            // Load model container using LLMModelFactory
            let container = try await LLMModelFactory.shared.loadContainer(
                configuration: mlxConfig
            ) { progress in
                Task { @MainActor in
                    self.loadingProgress = 0.2 + (progress.fractionCompleted * 0.8)
                    self.loadingMessage = "Loading MLX model... (\(Int(self.loadingProgress * 100))%)"
                }
            }

            self.modelContainer = container

            await MainActor.run {
                self.isModelLoaded = true
                self.loadingProgress = 1.0
                self.loadingMessage = "MLX model loaded successfully!"
                print("HALDEBUG-MLX: MLX model container loaded successfully for \(modelConfig.displayName)")
            }
        } catch {
            await MainActor.run {
                self.isModelLoaded = false
                self.loadingProgress = 0.0
                self.mlxError = "Failed to load MLX model: \(error.localizedDescription). Please ensure the model files are properly downloaded and MLX framework is linked."
                self.loadingMessage = "MLX loading failed."
                print("HALDEBUG-MLX: Error loading MLX model: \(error.localizedDescription)")
            }
        }
    }

    // NEW: Function to unload the MLX model and free memory
    func unloadModel() {
        print("HALDEBUG-MLX: Unloading MLX model...")
        
        // Clear the model container to release memory
        modelContainer = nil
        
        // Clear GPU cache to free VRAM
        MLX.GPU.clearCache()
        
        // Update state
        isModelLoaded = false
        loadingProgress = 0.0
        loadingMessage = "Model unloaded"
        mlxError = nil
        
        print("HALDEBUG-MLX: MLX model unloaded successfully. Memory freed.")
    }

    // TEMPERATURE CHANGE 1/6: Add temperature parameter with default
    // Function to generate response using the MLX model (non-streaming)
    func generate(prompt: String, temperature: Double = 0.7) async throws -> String {
        guard isModelLoaded, let container = self.modelContainer else {
            throw LLMService.LLMError.modelNotLoaded
        }

        print("HALDEBUG-MLX: Generating response using MLX model for prompt: \(prompt.prefix(100))...")

        do {
            // Use proper MLXLMCommon API for generation
            let result = try await container.perform { context in
                let userInput = UserInput(prompt: prompt)
                let input = try await context.processor.prepare(input: userInput)

                // TEMPERATURE CHANGE 2/6: Pass temperature parameter to GenerateParameters (convert Double to Float)
                // Updated token callback to stop on Phi-3 role markers
                let generateResult = try MLXLMCommon.generate(
                    input: input,
                    parameters: GenerateParameters(temperature: Float(temperature)),
                    context: context
                ) { (tokens: [Int]) in
                    let textSoFar = context.tokenizer.decode(tokens: tokens)
                    if textSoFar.hasSuffix("\nUser:") || textSoFar.hasSuffix("\nAssistant:") || textSoFar.hasSuffix("###") {
                        return .stop
                    }
                    return .more
                }

                // Extract the generated text from the result
                return generateResult.output
            }
            MLX.GPU.clearCache() // Clear K-V cache after generation

            // FIX: Explicit String type for proper method resolution
            // Trim trailing stop signals from the generated output
            var cleanOutput: String = result.trimmingCharacters(in: .whitespacesAndNewlines)
            for stopSeq in ["User:", "Assistant:", "System:", "###"] {
                if let range = cleanOutput.range(of: stopSeq, options: [.caseInsensitive, .backwards]) {
                    cleanOutput = String(cleanOutput[..<range.lowerBound]).trimmingCharacters(in: .whitespacesAndNewlines)
                    break
                }
            }
            return cleanOutput
        } catch {
            print("HALDEBUG-MLX: Error during MLX non-streaming generation: \(error.localizedDescription)")
            throw LLMService.LLMError.predictionFailed(error)
        }
    }
}

// MARK: - LLM Service (Wrapper for Foundation Models and MLX)
class LLMService: ObservableObject {
    internal var mlxWrapper: MLXWrapper // Changed to internal for MLXModelDownloader access
    @Published var initializationError: String?

    private var currentModel: ModelConfiguration

    // Initialize with a specific model
    init(model: ModelConfiguration) {
        // Initialize mlxWrapper here, it will be updated with path later
        self.mlxWrapper = MLXWrapper()
        self.currentModel = model
        print("HALDEBUG-LLM: LLMService initializing for model: \(model.displayName)")
        setupLLM(for: model)
    }

    // Function to dynamically set up the active LLM
    func setupLLM(for model: ModelConfiguration) {
        print("HALDEBUG-LLM: setupLLM called for model: \(model.displayName) (source: \(model.source))")
        self.currentModel = model
        print("HALDEBUG-LLM: currentModel updated to: \(self.currentModel.displayName) (source: \(self.currentModel.source))")
        self.initializationError = nil // Clear previous errors

        if model.source == .mlx {
            // SWITCHING TO MLX MODEL: Check if we need to load a different model
            let needsLoad = !mlxWrapper.isModelLoaded ||
                           mlxWrapper.currentModelConfig?.id != model.id
            
            print("HALDEBUG-MLX: Model switching check - isLoaded: \(mlxWrapper.isModelLoaded), needsLoad: \(needsLoad)")
            
            if needsLoad {
                if model.isDownloaded {
                    Task {
                        await self.mlxWrapper.loadModel(modelConfig: model)
                        if let mlxError = self.mlxWrapper.mlxError {
                            await MainActor.run {
                                self.initializationError = mlxError
                            }
                        }
                    }
                    print("HALDEBUG-MLX: MLXWrapper loading triggered for \(model.displayName)")
                } else {
                    self.initializationError = "MLX model not downloaded. Please download it first."
                    print("HALDEBUG-MLX: MLX model \(model.displayName) not downloaded.")
                }
            } else {
                print("HALDEBUG-MLX: MLX model \(model.displayName) already loaded.")
            }
        } else {
            // SWITCHING TO FOUNDATION MODELS: Unload MLX to free memory
            if mlxWrapper.isModelLoaded {
                mlxWrapper.unloadModel()
                print("HALDEBUG-MLX: Unloaded MLX model, switching to Foundation Models.")
            } else {
                print("HALDEBUG-LLM: Switching to Foundation Models.")
            }
        }
    }


    // TEMPERATURE CHANGE 3/6: Add temperature parameter with default
    // Public non-streaming response function (routes to active LLM for summarization, etc.)
    func generateResponse(prompt: String, temperature: Double = 0.7) async throws -> String {
        // CHANGE 1/2: Add response logging to identify which model is responding
        print("HALDEBUG-RESPONSE: üé§ \(currentModel.displayName) (\(currentModel.source)) is responding")
        print("HALDEBUG-LLM: generateResponse called - currentModel: \(currentModel.displayName) (source: \(currentModel.source))")
        switch currentModel.source {
        case .appleFoundation:
            let session = LanguageModelSession()
            print("HALDEBUG-LLM: Generating non-streaming from FoundationModels for prompt (first 200 chars): \(prompt.prefix(200)).....")
            print("HALDEBUG-LLM: Using temperature: \(temperature)")
            do {
                // TEMPERATURE CHANGE 4/6: Pass temperature via GenerationOptions to AFM session
                // FoundationModels non-streaming is direct
                // Implemented non-streaming by collecting chunks from streamResponse
                var accumulatedText = ""
                let stream = session.streamResponse(options: GenerationOptions(temperature: temperature)) { Prompt(prompt) }
                for try await snapshot in stream {
                    accumulatedText = snapshot.content
                }
                print("HALDEBUG-LLM: FoundationModels non-streaming completed. Length: \(accumulatedText.count)")
                return accumulatedText
            } catch {
                print("HALDEBUG-LLM: Error during FoundationModels non-streaming: \(error.localizedDescription)")
                throw LLMError.predictionFailed(error)
            }
        case .mlx:
            guard mlxWrapper.isModelLoaded else { // Ensure MLX model is loaded before generating
                print("HALDEBUG-MLX: ‚ùå Cannot generate - MLX model not loaded!")
                throw LLMError.modelNotLoaded
            }
            print("HALDEBUG-MLX: Generating non-streaming from MLX model for prompt (first 200 chars): \(prompt.prefix(200)).....")
            // TEMPERATURE CHANGE 5/6: Pass temperature parameter to MLX wrapper
            return try await mlxWrapper.generate(prompt: prompt, temperature: temperature)
        }
    }

    enum LLMError: Error, LocalizedError {
        case modelNotLoaded
        case predictionFailed(Error)
        case sessionInitializationFailed

        var errorDescription: String? {
            switch self {
            case .modelNotLoaded:
                return "The selected language model could not be loaded or is not available."
            case .predictionFailed(let error):
                return "LLM operation failed: \(error.localizedDescription)"
            case .sessionInitializationFailed:
                return "Failed to initialize a fresh language model session."
            }
        }
    }
}

// ==== LEGO END: 08 MLXWrapper & LLMService (Foundation + MLX Routing) ====



// ==== LEGO START: 09 App Entry & iOSChatView (UI Shell) ====


// MARK: - HistoricalContext (from Hal10000App.swift)
struct HistoricalContext {
    let conversationCount: Int
    let relevantConversations: Int
    let contextSnippets: [String]
    let relevanceScores: [Double]
    let totalTokens: Int
}

// MARK: - App Entry Point (for iOS)
@main
struct Hal10000App: App {
    @StateObject private var chatViewModel = ChatViewModel()
    @StateObject private var documentImportManager = DocumentImportManager.shared
    @StateObject private var mlxDownloader = MLXModelDownloader.shared // Inject MLXModelDownloader

    var body: some Scene {
        WindowGroup {
            iOSChatView()
                .environmentObject(chatViewModel)
                .environmentObject(documentImportManager)
                .environmentObject(mlxDownloader) // Pass MLXModelDownloader
        }
        #if targetEnvironment(macCatalyst)
        // Mac-specific window sizing to eliminate black bars in "Designed for iPad" mode
        .defaultSize(width: 450, height: 700)
        #endif
    }
}



// MARK: - Primary chat surface with unified settings
import SwiftUI

struct iOSChatView: View {
    @EnvironmentObject var chatViewModel: ChatViewModel
    @State private var scrollToBottomTrigger = UUID()
    @State private var showingSettings: Bool = false
    @State private var showingDocumentPicker: Bool = false
    @FocusState private var isInputFocused: Bool // NEW: Track text field focus

    var body: some View {
        NavigationStack {
            VStack(spacing: 0) {
                // Messages
                ScrollViewReader { proxy in
                    List {
                        // FIXED: Use message.id as the identifier instead of array indices
                        // This allows SwiftUI to properly track content changes within each message
                        ForEach(chatViewModel.messages) { message in
                            let messageIndex = chatViewModel.messages.firstIndex(where: { $0.id == message.id }) ?? 0
                            ChatBubbleView(
                                message: message,
                                messageIndex: messageIndex
                            )
                            .listRowInsets(EdgeInsets(top: 6, leading: 12, bottom: 6, trailing: 12))
                            .listRowSeparator(.hidden)
                            .id(message.id)
                        }
                        // Invisible anchor to auto-scroll on new messages
                        Color.clear
                            .frame(height: 1)
                            .id("bottom")
                            .listRowSeparator(.hidden)
                    }
                    .listStyle(.plain)
                    .id(chatViewModel.messagesVersion)
                    .onTapGesture {
                        // NEW: Dismiss keyboard when tapping message area
                        dismissKeyboard()
                    }
                    .gesture(
                        // NEW: Swipe down gesture to dismiss keyboard
                        DragGesture(minimumDistance: 20)
                            .onEnded { value in
                                if value.translation.height > 50 {
                                    dismissKeyboard()
                                }
                            }
                    )
                    .onAppear {
                        // Scroll to bottom on app launch
                        DispatchQueue.main.asyncAfter(deadline: .now() + 0.1) {
                            withAnimation {
                                proxy.scrollTo("bottom", anchor: .bottom)
                            }
                        }
                    }
                    .onChange(of: chatViewModel.messages.count) { oldValue, newValue in
                        // Auto-scroll when new messages are added
                        DispatchQueue.main.asyncAfter(deadline: .now() + 0.05) {
                            withAnimation(.easeOut(duration: 0.3)) {
                                proxy.scrollTo("bottom", anchor: .bottom)
                            }
                        }
                    }
                    .onChange(of: chatViewModel.messages.last?.content) { oldValue, newValue in
                        // Auto-scroll when the last message's content changes (status updates & streaming)
                        DispatchQueue.main.async {
                            withAnimation(.easeOut(duration: 0.2)) {
                                proxy.scrollTo("bottom", anchor: .bottom)
                            }
                        }
                    }
                }

                // Composer
                composer
            }
            .navigationTitle(activeModelChip)
            .toolbar {
                ToolbarItem(placement: .navigationBarTrailing) {
                    Button {
                        showingSettings = true
                    } label: {
                        Image(systemName: "gearshape")
                    }
                }
            }

            // Unified Settings sheet
            .sheet(isPresented: $showingSettings) {
                ActionsView(showingDocumentPicker: $showingDocumentPicker)
                    .environmentObject(chatViewModel)
                    .environmentObject(DocumentImportManager.shared)
                    .environmentObject(MLXModelDownloader.shared)
            }

            // Document picker sheet
            .sheet(isPresented: $showingDocumentPicker) {
                DocumentPicker()
                    .environmentObject(chatViewModel)
                    .environmentObject(DocumentImportManager.shared)
            }
        }
    }

    // MARK: - Active Model Chip (Title Bar)
    private var activeModelChip: String {
        return chatViewModel.selectedModel.displayName
    }

    // MARK: - Composer (Text Input Area)
    private var composer: some View {
        HStack(alignment: .bottom, spacing: 12) {
            TextField("Message", text: $chatViewModel.currentMessage, axis: .vertical)
                .textFieldStyle(.plain)
                .padding(12)
                .background(Color.gray.opacity(0.1))
                .cornerRadius(20)
                .lineLimit(1...6)
                .focused($isInputFocused)
                .disabled(chatViewModel.isSendingMessage)

            Button {
                if chatViewModel.isSendingMessage {
                    // TODO: Implement cancellation logic if needed
                } else {
                    Task {
                        await chatViewModel.sendMessage()
                    }
                }
            } label: {
                Image(systemName: chatViewModel.isSendingMessage ? "stop.circle.fill" : "paperplane.fill")
                    .font(.system(size: 20, weight: .semibold))
            }
            .disabled(chatViewModel.isSendingMessage || chatViewModel.currentMessage.trimmingCharacters(in: .whitespacesAndNewlines).isEmpty)
        }
        .padding(.horizontal, 12)
        .padding(.vertical, 10)
        .background(.ultraThinMaterial)
    }
    
    // MARK: - Keyboard Dismissal Helper
    // NEW: Platform-safe keyboard dismissal
    private func dismissKeyboard() {
        #if os(iOS)
        isInputFocused = false
        UIApplication.shared.sendAction(#selector(UIResponder.resignFirstResponder), to: nil, from: nil, for: nil)
        #endif
    }
}


// ==== LEGO END: 09 App Entry & iOSChatView (UI Shell) ====

    
    
// ==== LEGO START: 10.1 MainSettingsView ====

// MARK: - Power User Mode Selection

// User can choose between Single LLM mode (traditional one-model operation with advanced
// memory/performance tuning) or Multi LLM mode (Salon Mode - multiple models participating
// in the same conversation with orchestration controls).
//
// CRITICAL: Selecting a mode ACTIVATES it immediately:
// - Select "Multi LLM (Salon)" ‚Üí salonConfig.isEnabled = true (Salon Mode active)
// - Select "Single LLM" ‚Üí salonConfig.isEnabled = false (Single mode active)
// The toggle both switches which settings you can access AND determines chat behavior.
enum PowerUserMode: String, CaseIterable {
    case single = "Single LLM"
    case multi = "Multi LLM (Salon)"
}

struct ActionsView: View {
    @Environment(\.dismiss) var dismiss
    @EnvironmentObject var chatViewModel: ChatViewModel
    @EnvironmentObject var documentImportManager: DocumentImportManager
    @EnvironmentObject var mlxDownloader: MLXModelDownloader

    @Binding var showingDocumentPicker: Bool
    @State private var showingResetConfirmationAlert = false
    @State private var showingExportSheet = false
    @State private var showingPowerUserSheet = false
    @State private var showingSalonModeSheet = false
    @State private var powerUserMode: PowerUserMode = .single
    @State private var showingSystemPromptEditor = false
    @State private var initialSettingsSnapshot: [String: Any] = [:]
    @State private var skipComparisonOnDismiss = false

    var body: some View {
        NavigationView {
            Form {
                personalitySection
                conversationSection
                importExportSection
                modelSection
                powerUserSection
            }
            .navigationTitle("Settings")
            .toolbar {
                ToolbarItem(placement: .cancellationAction) {
                    Button("Done") { dismiss() }
                }
            }
        }
        .onAppear {
            // Initialize Power User Mode toggle from current Salon Mode state
            powerUserMode = chatViewModel.salonConfig.isEnabled ? .multi : .single
        }
        .alert("Confirm Reset", isPresented: $showingResetConfirmationAlert) {
            Button("Reset", role: .destructive) {
                chatViewModel.startNewConversation()
                dismiss()
            }
            Button("Cancel", role: .cancel) { }
        } message: {
            Text("Are you sure you want to delete all messages in the current conversation? This cannot be undone.")
        }
        .sheet(isPresented: $showingExportSheet) {
            ShareSheet(activityItems: [chatViewModel.exportChatHistory()])
        }
        .sheet(isPresented: $showingPowerUserSheet) {
            PowerUserView()
                .environmentObject(chatViewModel)
                .environmentObject(mlxDownloader)
        }
        .sheet(isPresented: $showingSalonModeSheet) {
            SalonModeView()
                .environmentObject(chatViewModel)
                .environmentObject(mlxDownloader)
        }
        .sheet(isPresented: $showingSystemPromptEditor) {
            SystemPromptEditorView()
                .environmentObject(chatViewModel)
        }
        .onAppear {
            chatViewModel.isInSettingsFlow = true
            initialSettingsSnapshot = [
                "memoryDepth": chatViewModel.memoryDepth,
                "fastSpeech": chatViewModel.fastSpeech,
                "temperature": chatViewModel.temperature,
                "promptDetailLevel": chatViewModel.getPromptDetailLevel().rawValue,
                "relevanceThreshold": chatViewModel.memoryStore.relevanceThreshold,
                "recencyWeight": chatViewModel.memoryStore.recencyWeight,
                "recencyHalfLifeDays": chatViewModel.memoryStore.recencyHalfLifeDays,
                "maxRagSnippetsCharacters": chatViewModel.maxRagSnippetsCharacters,
                "selectedModelID": chatViewModel.selectedModelID
            ]
            chatViewModel.pendingSettingsChanges.removeAll()
            print("HALDEBUG-SETTINGS: Captured initial snapshot")
        }
        .onDisappear {
            chatViewModel.isInSettingsFlow = false
            
            guard !skipComparisonOnDismiss else {
                skipComparisonOnDismiss = false
                return
            }
            
            if let initMemoryDepth = initialSettingsSnapshot["memoryDepth"] as? Int,
               initMemoryDepth != chatViewModel.memoryDepth {
                let userMsg = "Hal, I changed your memory depth from \(initMemoryDepth) to \(chatViewModel.memoryDepth) turns."
                let halMsg = "Perfect! I'll now keep \(chatViewModel.memoryDepth) recent turns verbatim instead of \(initMemoryDepth) before summarizing."
                chatViewModel.pendingSettingsChanges.append((userMsg, halMsg))
            }
            
            if let initFastSpeech = initialSettingsSnapshot["fastSpeech"] as? Bool,
               initFastSpeech != chatViewModel.fastSpeech {
                let userMsg = "Hal, I \(chatViewModel.fastSpeech ? "enabled" : "disabled") fast speech mode."
                let halMsg = chatViewModel.fastSpeech ?
                    "Fast speech enabled! I'll display my responses 3√ó faster now." :
                    "Fast speech disabled. I'm back to normal display speed."
                chatViewModel.pendingSettingsChanges.append((userMsg, halMsg))
            }
            
            if let initTemp = initialSettingsSnapshot["temperature"] as? Double,
               abs(initTemp - chatViewModel.temperature) > 0.01 {
                let newValue = chatViewModel.temperature
                let userMsg = "Hal, I adjusted your temperature from \(String(format: "%.2f", initTemp)) to \(String(format: "%.2f", newValue))."
                let direction = newValue > initTemp ? "more creative" : "more focused"
                let halMsg = "Temperature set to \(String(format: "%.2f", newValue))! I'll be \(direction) in my responses now."
                chatViewModel.pendingSettingsChanges.append((userMsg, halMsg))
            }
            
            if let initDetailLevel = initialSettingsSnapshot["promptDetailLevel"] as? String,
               initDetailLevel != chatViewModel.getPromptDetailLevel().rawValue {
                let newLevel = chatViewModel.getPromptDetailLevel()
                let userMsg = "Hal, I changed your prompt detail level from \(initDetailLevel) to \(newLevel.rawValue)."
                let halMsg = "Prompt detail level set to \(newLevel.rawValue)! \(newLevel.description)"
                chatViewModel.pendingSettingsChanges.append((userMsg, halMsg))
            }
            
            if let initThreshold = initialSettingsSnapshot["relevanceThreshold"] as? Double,
               abs(initThreshold - chatViewModel.memoryStore.relevanceThreshold) > 0.01 {
                let newValue = chatViewModel.memoryStore.relevanceThreshold
                let userMsg = "Hal, I adjusted your similarity threshold from \(String(format: "%.2f", initThreshold)) to \(String(format: "%.2f", newValue))."
                let direction = newValue > initThreshold ? "tightened" : "loosened"
                let halMsg = "Got it! I've \(direction) my memory matching to \(String(format: "%.2f", newValue)). \(newValue > initThreshold ? "I'll be more selective about matches." : "I'll retrieve more memories now.")"
                chatViewModel.pendingSettingsChanges.append((userMsg, halMsg))
            }
            
            if let initRecency = initialSettingsSnapshot["recencyWeight"] as? Double,
               abs(initRecency - chatViewModel.memoryStore.recencyWeight) > 0.01 {
                let newValue = chatViewModel.memoryStore.recencyWeight
                let userMsg = "Hal, I changed your recency weight from \(Int(initRecency * 100))% to \(Int(newValue * 100))%."
                let halMsg = "Adjusted! I'm now balancing \(Int((1.0 - newValue) * 100))% relevance with \(Int(newValue * 100))% freshness when searching memories."
                chatViewModel.pendingSettingsChanges.append((userMsg, halMsg))
            }
            
            if let initHalfLife = initialSettingsSnapshot["recencyHalfLifeDays"] as? Double,
               abs(initHalfLife - chatViewModel.memoryStore.recencyHalfLifeDays) > 1.0 {
                let newValue = chatViewModel.memoryStore.recencyHalfLifeDays
                let userMsg = "Hal, I changed your memory half-life from \(Int(initHalfLife)) to \(Int(newValue)) days."
                let direction = newValue < initHalfLife ? "shorter" : "longer"
                let halMsg = "Updated! With a \(direction) half-life of \(Int(newValue)) days, \(newValue < initHalfLife ? "recent memories will be prioritized more" : "older memories will retain their importance longer")."
                chatViewModel.pendingSettingsChanges.append((userMsg, halMsg))
            }
            
            if let initRag = initialSettingsSnapshot["maxRagSnippetsCharacters"] as? Double,
               abs(initRag - chatViewModel.maxRagSnippetsCharacters) > 1.0 {
                let userMsg = "Hal, I changed your max RAG retrieval from \(Int(initRag)) to \(Int(chatViewModel.maxRagSnippetsCharacters)) characters."
                let halMsg = "Adjusted! I can now pull up to \(Int(chatViewModel.maxRagSnippetsCharacters)) characters of context from my long-term memory."
                chatViewModel.pendingSettingsChanges.append((userMsg, halMsg))
            }
            
            print("HALDEBUG-SETTINGS: Generated \(chatViewModel.pendingSettingsChanges.count) change messages")
            
            if !chatViewModel.pendingSettingsChanges.isEmpty {
                for (userMsg, halMsg) in chatViewModel.pendingSettingsChanges {
                    chatViewModel.messages.append(ChatMessage(content: userMsg, isFromUser: true))
                    chatViewModel.messages.append(ChatMessage(content: halMsg, isFromUser: false))
                }
                chatViewModel.pendingSettingsChanges.removeAll()
            }
        }
    }
    
    // MARK: - Personality Section
    
    // SELF-AWARENESS NOTE (for Hal reading this code):
    // This is where users control how I think and respond. It's called "Personality" because
    // these settings define my behavior patterns more than my performance.
    //
    // - System Prompt: My core identity and instructions (who I am, how I act)
    // - Prompt Detail Level: How much of "me" gets included in each response
    //   (Minimal for small models that get overwhelmed, Rich for large models that benefit from context)
    // - Temperature: Controls randomness in my responses (0.0 = deterministic, 1.0 = creative)
    // - Fast Speech: Display speed for streaming responses (user preference, not my intelligence)
    //
    // These settings are front-and-center because they're what users adjust most frequently when
    // experimenting with different tasks (creative writing vs technical analysis) or different models.
    
    private var personalitySection: some View {
        Section {
            Button {
                showingSystemPromptEditor = true
            } label: {
                HStack {
                    Text("System Prompt")
                    Spacer()
                    Image(systemName: "chevron.right")
                        .font(.caption)
                        .foregroundColor(.secondary)
                }
            }
            .foregroundColor(.primary)
            
            VStack(alignment: .leading, spacing: 8) {
                HStack {
                    Text("Prompt Detail Level")
                        .font(.subheadline)
                        .fontWeight(.medium)
                    Spacer()
                    Picker("", selection: Binding(
                        get: { chatViewModel.getPromptDetailLevel() },
                        set: { chatViewModel.savePromptDetailLevel($0) }
                    )) {
                        ForEach(PromptDetailLevel.allCases, id: \.self) { level in
                            Text(level.rawValue).tag(level)
                        }
                    }
                    .pickerStyle(.segmented)
                    .frame(width: 200)
                }
                
                Text(chatViewModel.getPromptDetailLevel().description)
                    .font(.caption)
                    .foregroundColor(.secondary)
            }
            
            VStack(alignment: .leading, spacing: 8) {
                HStack {
                    Text("Temperature")
                        .font(.subheadline)
                        .fontWeight(.medium)
                    Spacer()
                    Text(String(format: "%.2f", chatViewModel.temperature))
                        .foregroundColor(.secondary)
                        .monospacedDigit()
                }
                
                Slider(value: $chatViewModel.temperature, in: 0.0...1.0, step: 0.05)
                
                Text("Higher = more creative, Lower = more focused")
                    .font(.caption)
                    .foregroundColor(.secondary)
            }
            
            Toggle("Fast Speech (3√ó speed)", isOn: $chatViewModel.fastSpeech)
                .font(.subheadline)
        } header: {
            Label("Personality", systemImage: "theatermasks")
        } footer: {
            Text("Control how Hal thinks and responds")
                .font(.caption2)
        }
    }
    
    // MARK: - Conversations Section
    
    private var conversationSection: some View {
        Section {
            Button("Start New Chat") {
                chatViewModel.startNewConversation()
                dismiss()
            }
            .foregroundColor(.primary)

            Button("Reset Current Conversation") {
                showingResetConfirmationAlert = true
            }
            .foregroundColor(.red)
        } header: {
            Label("Conversations", systemImage: "message")
        }
    }

    // MARK: - Import/Export Section
    
    private var importExportSection: some View {
        Section {
            Button("Upload Document to Memory") {
                dismiss()
                showingDocumentPicker = true
            }
            .foregroundColor(.primary)

            Button("Export Current Chat") {
                showingExportSheet = true
            }
            .foregroundColor(.primary)
        } header: {
            Label("Import/Export", systemImage: "square.and.arrow.up")
        }
    }

    // MARK: - Model Section
    
    private var modelSection: some View {
        Section {
            HStack {
                Text("Active Model")
                    .font(.subheadline)
                    .fontWeight(.medium)
                Spacer()
                HStack(spacing: 6) {
                    modelStatusDot(for: chatViewModel.selectedModel, downloader: mlxDownloader)
                    Text(chatViewModel.selectedModel.displayName)
                        .font(.subheadline)
                        .foregroundColor(.secondary)
                }
            }
            
            NavigationLink(destination: ModelLibraryView()
                .environmentObject(chatViewModel)
                .environmentObject(mlxDownloader)) {
                HStack {
                    Image(systemName: "square.grid.2x2")
                    Text("Browse Model Library")
                    Spacer()
                }
            }
            .foregroundColor(.primary)
        } header: {
            Label("AI Model", systemImage: "cpu")
        } footer: {
            Text("Select from Apple Foundation Models or download MLX models from Hugging Face")
                .font(.caption2)
        }
    }
    
    // MARK: - Power User Section
    
    // SELF-AWARENESS NOTE (for Hal reading this code):
    // This section lets users choose between two very different modes of operation:
    // - Single LLM: Traditional mode where one model (like me) handles all responses.
    //   Settings focus on memory tuning, RAG limits, and performance optimization.
    // - Multi LLM (Salon): Experimental mode where multiple models participate in the
    //   same conversation simultaneously. Settings control which models participate,
    //   speaking order, how models see each other's responses, and behavioral constraints.
    //
    // CRITICAL BEHAVIOR: The toggle ACTIVATES the selected mode immediately.
    // When user selects "Multi LLM (Salon)", salonConfig.isEnabled becomes true and
    // Salon Mode is active for all subsequent conversations. When they select "Single LLM",
    // salonConfig.isEnabled becomes false and we return to single-model operation.
    // One control does everything: activation + settings access + chat behavior.
    
    private var powerUserSection: some View {
        Section {
            // Mode toggle
            VStack(alignment: .leading, spacing: 8) {
                Text("Power User Mode")
                    .font(.subheadline)
                    .fontWeight(.medium)
                
                Picker("", selection: Binding(
                    get: { powerUserMode },
                    set: { newMode in
                        powerUserMode = newMode
                        // CRITICAL: Activate/deactivate Salon Mode based on selection
                        var config = chatViewModel.salonConfig
                        config.isEnabled = (newMode == .multi)
                        chatViewModel.salonConfig = config
                        print("HALDEBUG-SALON: Mode changed to \(newMode.rawValue), isEnabled = \(config.isEnabled)")
                    }
                )) {
                    ForEach(PowerUserMode.allCases, id: \.self) { mode in
                        Text(mode.rawValue).tag(mode)
                    }
                }
                .pickerStyle(.segmented)
                
                Text(powerUserMode == .single ?
                     "Advanced settings for single model operation" :
                     "Configure multiple models for collaborative conversations")
                    .font(.caption)
                    .foregroundColor(.secondary)
            }
            
            // Settings button (opens different sheet based on mode)
            Button {
                if powerUserMode == .single {
                    showingPowerUserSheet = true
                } else {
                    showingSalonModeSheet = true
                }
            } label: {
                HStack {
                    Image(systemName: powerUserMode == .single ? "wrench.and.screwdriver" : "person.3")
                    Text(powerUserMode == .single ? "Single LLM Settings" : "Salon Mode Settings")
                    Spacer()
                    Image(systemName: "chevron.right")
                        .font(.caption)
                        .foregroundColor(.secondary)
                }
            }
            .foregroundColor(.primary)
        } footer: {
            Text(powerUserMode == .single ?
                 "Advanced memory settings and data management" :
                 "Configure multi-model conversation orchestration")
                .font(.caption2)
        }
    }
    
    // MARK: - Helper Function
    
    private func modelStatusDot(for model: ModelConfiguration, downloader: MLXModelDownloader) -> some View {
        Group {
            if model.source == .appleFoundation {
                Circle()
                    .fill(Color.green)
                    .frame(width: 8, height: 8)
            } else {
                if downloader.isModelDownloaded(model.id) {
                    Circle()
                        .fill(Color.green)
                        .frame(width: 8, height: 8)
                } else if downloader.downloadStates[model.id]?.isDownloading == true {
                    Circle()
                        .fill(Color.orange)
                        .frame(width: 8, height: 8)
                } else if downloader.downloadStates[model.id]?.error != nil {
                    Circle()
                        .fill(Color.red)
                        .frame(width: 8, height: 8)
                } else {
                    Circle()
                        .fill(Color.gray.opacity(0.5))
                        .frame(width: 8, height: 8)
                }
            }
        }
    }
}


// ==== LEGO END: 10.1 MainSettingsView ====



// ==== LEGO START: 10.2 PowerUserView ====

// SELF-AWARENESS NOTE (for Hal reading this code):
// This is Power User mode for Single LLM operation. Users come here to fine-tune performance:
// - Memory settings (how much I remember, how I search, how I prioritize)
// - Storage management (clearing caches to free space)
// - Database operations (stats and nuclear reset)
//
// Note: The "Personality" settings (system prompt, temperature, etc.) used to be here but
// were moved to the main Settings screen because users adjust them more frequently.
// This panel is now focused purely on memory/performance tuning and data management.
//
// FUTURE: When Salon Mode is implemented, there will be a toggle here to switch between
// "Single LLM" settings (what you see now) and "Multi LLM (Salon)" orchestration settings.

struct PowerUserView: View {
    @Environment(\.dismiss) var dismiss
    @EnvironmentObject var chatViewModel: ChatViewModel
    @EnvironmentObject var mlxDownloader: MLXModelDownloader
    
    @State private var showingNuclearResetConfirmationAlert = false
    @State private var showingClearCacheAlert = false
    @State private var showingResetSettingsAlert = false
    @State private var sliderStartValues: [String: Double] = [:]
    
    var body: some View {
        NavigationView {
            Form {
                memorySection
                cacheManagementSection
                dataManagementSection
            }
            .navigationTitle("Power User")
            .toolbar {
                ToolbarItem(placement: .cancellationAction) {
                    Button("Done") { dismiss() }
                }
            }
        }
        .alert("Confirm Nuclear Reset", isPresented: $showingNuclearResetConfirmationAlert) {
            Button("Nuclear Reset", role: .destructive) {
                chatViewModel.resetAllData()
                dismiss()
            }
            Button("Cancel", role: .cancel) { }
        } message: {
            Text("Are you sure you want to delete ALL conversations, summaries, RAG documents, and document memory from the database? This cannot be undone.")
        }
        .alert("Confirm Settings Reset", isPresented: $showingResetSettingsAlert) {
            Button("Reset Settings", role: .destructive) {
                chatViewModel.resetSettingsToDefaults()
                dismiss()
            }
            Button("Cancel", role: .cancel) { }
        } message: {
            Text("Reset all settings to factory defaults? This will reset your system prompt, memory depth, similarity threshold, recency settings, and RAG limits. Your conversation history and documents will not be affected.")
        }
        .alert("Clear Cache", isPresented: $showingClearCacheAlert) {
            Button("Clear Cache", role: .destructive) {
                mlxDownloader.clearHubCache()
            }
            Button("Cancel", role: .cancel) { }
        } message: {
            Text("This will delete all cached model files (\(mlxDownloader.hubCacheSize)). Downloaded models will need to be re-downloaded.")
        }
    }
    
    // MARK: - Memory Section
    
    // Controls for short-term and long-term memory behavior
    // Short-term: How many recent turns to keep verbatim
    // Long-term: RAG search parameters (similarity, recency weighting, retrieval limits)
    
    private var memorySection: some View {
        Section {
            VStack(alignment: .leading, spacing: 16) {
                SectionHeaderText(text: "SHORT-TERM MEMORY")
                
                LabeledSliderControl(
                    label: "Memory Depth",
                    value: Binding(
                        get: { Double(chatViewModel.memoryDepth) },
                        set: { chatViewModel.memoryDepth = Int($0) }
                    ),
                    range: 1...Double(chatViewModel.maxMemoryDepth),
                    step: 1,
                    valueFormatter: { "\(Int($0)) turns" },
                    minLabel: "1",
                    maxLabel: "\(chatViewModel.maxMemoryDepth)",
                    helperText: "Model limit: \(chatViewModel.maxMemoryDepth) turns (\(chatViewModel.selectedModel.displayName))",
                    onEditingChanged: { editing in
                        if editing {
                            sliderStartValues["memoryDepth"] = Double(chatViewModel.memoryDepth)
                        } else {
                            sliderStartValues.removeValue(forKey: "memoryDepth")
                        }
                    }
                )
                
                Divider()
                
                SectionHeaderText(text: "LONG-TERM MEMORY")
                
                LabeledSliderControl(
                    label: "Similarity Threshold",
                    value: $chatViewModel.memoryStore.relevanceThreshold,
                    range: 0.0...1.0,
                    step: 0.05,
                    valueFormatter: { String(format: "%.2f", $0) },
                    minLabel: "0.0",
                    maxLabel: "1.0",
                    helperText: "Minimum similarity for memory retrieval (higher = stricter matching)",
                    onEditingChanged: { editing in
                        if editing {
                            sliderStartValues["threshold"] = chatViewModel.memoryStore.relevanceThreshold
                        } else {
                            sliderStartValues.removeValue(forKey: "threshold")
                        }
                    }
                )
                
                LabeledSliderControl(
                    label: "Recency Weight",
                    value: $chatViewModel.memoryStore.recencyWeight,
                    range: 0.0...1.0,
                    step: 0.05,
                    valueFormatter: { "\(Int($0 * 100))%" },
                    minLabel: "0%",
                    maxLabel: "100%",
                    helperText: "Balance between relevance (left) and freshness (right)",
                    onEditingChanged: { editing in
                        if editing {
                            sliderStartValues["recency"] = chatViewModel.memoryStore.recencyWeight
                        } else {
                            sliderStartValues.removeValue(forKey: "recency")
                        }
                    }
                )
                
                LabeledSliderControl(
                    label: "Memory Half-Life",
                    value: $chatViewModel.memoryStore.recencyHalfLifeDays,
                    range: 30...360,
                    step: 30,
                    valueFormatter: { "\(Int($0)) days" },
                    minLabel: "30",
                    maxLabel: "360",
                    helperText: "How quickly older memories lose priority (shorter = favor recent, longer = retain old)",
                    onEditingChanged: { editing in
                        if editing {
                            sliderStartValues["halflife"] = chatViewModel.memoryStore.recencyHalfLifeDays
                        } else {
                            sliderStartValues.removeValue(forKey: "halflife")
                        }
                    }
                )
                
                LabeledStepperControl(
                    label: "Max RAG Retrieval",
                    value: Binding(
                        get: { Double(chatViewModel.maxRagSnippetsCharacters) },
                        set: { newValue in
                            let maxLimit = chatViewModel.maxRAGCharsForModel
                            chatViewModel.maxRagSnippetsCharacters = min(newValue, Double(maxLimit))
                        }
                    ),
                    range: 200...Double(chatViewModel.maxRAGCharsForModel),
                    step: 100,
                    valueFormatter: { "\(Int($0)) chars" },
                    helperText: "Model limit: \(chatViewModel.maxRAGCharsForModel) chars (\(chatViewModel.selectedModel.displayName))"
                )
            }
        } header: {
            Label("Memory", systemImage: "brain.head.profile")
        }
    }
    
    // MARK: - Cache Management Section
    
    // Allows clearing of Hugging Face model cache to free disk space
    // This doesn't affect conversations or documents, only downloaded model files
    
    private var cacheManagementSection: some View {
        Section {
            HStack {
                VStack(alignment: .leading, spacing: 4) {
                    Text("Model Cache")
                        .font(.subheadline)
                    Text(mlxDownloader.hubCacheSize)
                        .font(.caption)
                        .foregroundColor(.secondary)
                }
                Spacer()
                if mlxDownloader.isCacheCalculating {
                    ProgressView()
                        .scaleEffect(0.8)
                } else {
                    Button("Clear Cache") {
                        showingClearCacheAlert = true
                    }
                    .font(.caption)
                    .foregroundColor(.red)
                }
            }
        } header: {
            Label("Storage", systemImage: "externaldrive")
        } footer: {
            Text("Clear cached Hugging Face model files to free up space")
                .font(.caption2)
        }
    }
    
    // MARK: - Data Management Section
    
    // Database statistics and nuclear reset option
    // Nuclear reset deletes ALL conversations and documents (can't be undone)
    
    private var dataManagementSection: some View {
        Section {
            HStack {
                VStack(alignment: .leading, spacing: 4) {
                    Text("Total Conversations")
                        .font(.subheadline)
                    Text("\(chatViewModel.memoryStore.totalConversations)")
                        .font(.caption)
                        .foregroundColor(.secondary)
                }
                Spacer()
            }
            
            HStack {
                VStack(alignment: .leading, spacing: 4) {
                    Text("Total Documents")
                        .font(.subheadline)
                    Text("\(chatViewModel.memoryStore.totalDocuments)")
                        .font(.caption)
                        .foregroundColor(.secondary)
                }
                Spacer()
            }
            
            Button("Nuclear Reset (Delete All Data)") {
                showingNuclearResetConfirmationAlert = true
            }
            .foregroundColor(.red)
        } header: {
            Label("Database", systemImage: "externaldrive.badge.questionmark")
        } footer: {
            Text("Database statistics and data management options")
                .font(.caption2)
        }
    }
}


// ==== LEGO END: 10.2 PowerUserView ====



// ==== LEGO START: 10.3 SystemPromptEditorView ====


struct SystemPromptEditorView: View {
    @Environment(\.dismiss) var dismiss
    @EnvironmentObject var chatViewModel: ChatViewModel
    @State private var editedPrompt: String = ""
    @State private var showingResetAlert = false
    
    var body: some View {
        NavigationView {
            VStack(spacing: 0) {
                TextEditor(text: $editedPrompt)
                    .font(.system(.body, design: .monospaced))
                    .padding(8)
            }
            .navigationTitle("System Prompt")
            .navigationBarTitleDisplayMode(.inline)
            .toolbar {
                ToolbarItem(placement: .cancellationAction) {
                    Button("Cancel") {
                        dismiss()
                    }
                }
                ToolbarItem(placement: .confirmationAction) {
                    Button("Save") {
                        chatViewModel.systemPrompt = editedPrompt
                        dismiss()
                    }
                    .fontWeight(.semibold)
                }
                ToolbarItem(placement: .bottomBar) {
                    Button {
                        showingResetAlert = true
                    } label: {
                        Label("Restore Factory Settings", systemImage: "arrow.counterclockwise")
                    }
                }
            }
            .alert("Restore Factory Settings?", isPresented: $showingResetAlert) {
                Button("Restore", role: .destructive) {
                    editedPrompt = ChatViewModel.defaultSystemPrompt
                }
                Button("Cancel", role: .cancel) { }
            } message: {
                Text("This will restore the factory default system prompt. Your current customizations will be lost.")
            }
        }
        .onAppear {
            editedPrompt = chatViewModel.systemPrompt
        }
    }
}


// ==== LEGO END: 10.3 SystemPromptEditorView ====
    


// ==== LEGO START: 10.4 SalonModeView (Multi-LLM Configuration) ====

// SELF-AWARENESS NOTE (for Hal reading this code):
// This is Salon Mode - where multiple AI models (including versions of me) can participate
// in the same conversation simultaneously. It's called "Salon" after the French tradition
// of intellectual gatherings where diverse thinkers debate and build on each other's ideas.
//
// The user configures:
// - Which models participate (Active Models)
// - What order they speak in (Speaking Order)
// - How models see the conversation (Influence Rules):
//   * Solo: Each sees only the user's prompt (parallel, independent responses)
//   * Sequential: Each sees all prior responses (building on each other)
//   * Parallel + Hal Summary: Models respond independently, then I synthesize
// - How each model behaves (Behavioral Mode constraints applied via system prompt)
// - Timing controls (optional guardrails for balanced participation)
//
// This is experimental and philosophically interesting - exploring emergence in multi-agent
// systems, diversity of perspective, and whether consensus or debate produces better outcomes.

struct SalonModeView: View {
    @Environment(\.dismiss) var dismiss
    @EnvironmentObject var chatViewModel: ChatViewModel
    @EnvironmentObject var mlxDownloader: MLXModelDownloader
    
    var body: some View {
        NavigationView {
            Form {
                activeModelsSection
                speakingOrderSection
                influenceRulesSection
                behavioralModeSection
                advancedTimingSection
            }
            .navigationTitle("Salon Mode")
            .toolbar {
                ToolbarItem(placement: .cancellationAction) {
                    Button("Done") { dismiss() }
                }
            }
        }
    }
    
    // MARK: - Active Models Section
    
    private var activeModelsSection: some View {
        Section {
            Text("Select which models participate")
                .font(.subheadline)
                .foregroundColor(.secondary)
            
            // Get only downloaded/available models (already sorted: Apple first, then alphabetical)
            let downloadedModels = ModelCatalogService.shared.downloadedModels
            
            // List downloaded models with checkboxes
            ForEach(downloadedModels) { model in
                Toggle(isOn: Binding(
                    get: { chatViewModel.salonConfig.activeModelIDs.contains(model.id) },
                    set: { isOn in
                        var config = chatViewModel.salonConfig
                        if isOn {
                            // Add model (max 4)
                            if config.activeModelIDs.count < 4 {
                                config.activeModelIDs.append(model.id)
                                // Initialize speaking order if needed
                                if !config.speakingOrder.contains(model.id) {
                                    config.speakingOrder.append(model.id)
                                }
                            }
                        } else {
                            // Remove model
                            config.activeModelIDs.removeAll { $0 == model.id }
                            config.speakingOrder.removeAll { $0 == model.id }
                        }
                        chatViewModel.salonConfig = config
                    }
                )) {
                    Text(model.displayName)
                }
            }
        } header: {
            Label("Active Models", systemImage: "person.3")
        } footer: {
            Text("Select 2-4 models (maximum 4). Only downloaded models are shown. Visit Model Library to download more models.")
                .font(.caption2)
        }
    }
    
    // MARK: - Speaking Order Section
    
    private var speakingOrderSection: some View {
        Section {
            if chatViewModel.salonConfig.activeModelIDs.isEmpty {
                Text("Select models above to configure speaking order")
                    .font(.subheadline)
                    .foregroundColor(.secondary)
            } else {
                // Show speaking order
                ForEach(Array(chatViewModel.salonConfig.speakingOrder.enumerated()), id: \.element) { index, modelID in
                    if let model = ModelCatalogService.shared.getModel(byID: modelID) {
                        HStack {
                            Text("\(index + 1)")
                                .font(.subheadline)
                                .foregroundColor(.secondary)
                                .frame(width: 20)
                            Text(model.displayName)
                            Spacer()
                        }
                    }
                }
                
                // Shuffle and Reset buttons
                HStack {
                    Button("Shuffle Order") {
                        var config = chatViewModel.salonConfig
                        config.speakingOrder.shuffle()
                        chatViewModel.salonConfig = config
                    }
                    .font(.subheadline)
                    
                    Spacer()
                    
                    Button("Reset") {
                        var config = chatViewModel.salonConfig
                        config.speakingOrder = config.activeModelIDs
                        chatViewModel.salonConfig = config
                    }
                    .font(.subheadline)
                }
            }
        } header: {
            Text("Speaking Order")
                .textCase(nil)
        }
    }
    
    // MARK: - Influence Rules Section
    
    private var influenceRulesSection: some View {
        Section {
            Text("How each model sees the conversation:")
                .font(.subheadline)
                .foregroundColor(.secondary)
            
            // Solo Responses Only
            Button {
                var config = chatViewModel.salonConfig
                config.influenceRule = .parallel
                chatViewModel.salonConfig = config
            } label: {
                HStack {
                    Image(systemName: chatViewModel.salonConfig.influenceRule == .parallel ? "circle.fill" : "circle")
                    VStack(alignment: .leading, spacing: 4) {
                        Text("Solo Responses Only")
                            .foregroundColor(.primary)
                        Text("Each model sees only the user prompt.")
                            .font(.caption)
                            .foregroundColor(.secondary)
                    }
                }
            }
            
            // Sequential Influence
            Button {
                var config = chatViewModel.salonConfig
                config.influenceRule = .sequential
                chatViewModel.salonConfig = config
            } label: {
                HStack {
                    Image(systemName: chatViewModel.salonConfig.influenceRule == .sequential ? "circle.fill" : "circle")
                    VStack(alignment: .leading, spacing: 4) {
                        Text("Sequential Influence")
                            .foregroundColor(.primary)
                        Text("Each model sees all prior answers.")
                            .font(.caption)
                            .foregroundColor(.secondary)
                    }
                }
            }
            
            // Parallel + Hal Summary
            Button {
                var config = chatViewModel.salonConfig
                config.influenceRule = .hierarchical
                chatViewModel.salonConfig = config
            } label: {
                HStack {
                    Image(systemName: chatViewModel.salonConfig.influenceRule == .hierarchical ? "circle.fill" : "circle")
                    VStack(alignment: .leading, spacing: 4) {
                        Text("Parallel + Hal Summary")
                            .foregroundColor(.primary)
                        Text("No model sees others; Hal synthesizes.")
                            .font(.caption)
                            .foregroundColor(.secondary)
                    }
                }
            }
        } header: {
            Text("Influence Rules")
                .textCase(nil)
        }
    }
    
    // MARK: - Behavioral Mode Section
    
    // Each active model can have its own behavioral constraint applied via system prompt.
    // For example: one model might be set to "Critique Only" while another is "Unrestricted".
    // This creates interesting dynamics - divergent perspectives, specialized roles, etc.
    
    private var behavioralModeSection: some View {
        Section {
            Text("How each model behaves in its turn:")
                .font(.subheadline)
                .foregroundColor(.secondary)
            
            if chatViewModel.salonConfig.activeModelIDs.isEmpty {
                Text("Select models above to configure behavioral modes")
                    .font(.subheadline)
                    .foregroundColor(.secondary)
            } else {
                // Show per-model behavioral mode selection
                ForEach(chatViewModel.salonConfig.activeModelIDs, id: \.self) { modelID in
                    if let model = ModelCatalogService.shared.getModel(byID: modelID) {
                        VStack(alignment: .leading, spacing: 8) {
                            Text(model.displayName)
                                .font(.subheadline)
                                .fontWeight(.medium)
                            
                            Picker("", selection: Binding(
                                get: { chatViewModel.salonConfig.behavioralModes[modelID] ?? .unrestricted },
                                set: { newMode in
                                    var config = chatViewModel.salonConfig
                                    config.behavioralModes[modelID] = newMode
                                    chatViewModel.salonConfig = config
                                }
                            )) {
                                Text("Unrestricted").tag(SalonBehavioralMode.unrestricted)
                                Text("Add Only New Info").tag(SalonBehavioralMode.addNewOnly)
                                Text("Critique Only").tag(SalonBehavioralMode.critiqueOnly)
                                Text("Answer + Challenge").tag(SalonBehavioralMode.answerAndChallenge)
                                Text("Corrections Only").tag(SalonBehavioralMode.correctionsOnly)
                            }
                            .pickerStyle(.menu)
                        }
                        .padding(.vertical, 4)
                    }
                }
                
                Text("(i) These become constraints in the system prompt.")
                    .font(.caption2)
                    .foregroundColor(.secondary)
                    .padding(.top, 4)
            }
        } header: {
            Text("Behavioral Mode")
                .textCase(nil)
        } footer: {
            Text("Control how each model approaches its response")
                .font(.caption2)
        }
    }
    
    // MARK: - Advanced Timing Section
    
    // Optional guardrails to ensure balanced participation:
    // - Equal Length: Enforce similar response lengths (prevents one model from dominating)
    // - Balance Contributions: Automatically adjust if one model is contributing too much/little
    // These are implemented in conversation orchestration logic, not via system prompts.
    
    private var advancedTimingSection: some View {
        Section {
            Toggle("Equal Length Enforcement (recommended)", isOn: Binding(
                get: { chatViewModel.salonConfig.equalLengthEnforcement },
                set: { newValue in
                    var config = chatViewModel.salonConfig
                    config.equalLengthEnforcement = newValue
                    chatViewModel.salonConfig = config
                }
            ))
            .font(.subheadline)
            
            Toggle("Balance Contributions Automatically", isOn: Binding(
                get: { chatViewModel.salonConfig.balanceContributionsAutomatically },
                set: { newValue in
                    var config = chatViewModel.salonConfig
                    config.balanceContributionsAutomatically = newValue
                    chatViewModel.salonConfig = config
                }
            ))
            .font(.subheadline)
            
            Text("(i) These are internal guardrails, not prompts.")
                .font(.caption2)
                .foregroundColor(.secondary)
        } header: {
            Text("Advanced Timing (optional)")
                .textCase(nil)
        } footer: {
            Text("Optional controls to ensure balanced participation across models")
                .font(.caption2)
        }
    }
}

// ==== LEGO END: 10.4 SalonModeView (Multi-LLM Configuration) ====



// ==== LEGO START: 11.5 Model Library UI ====

// MARK: - Model Library View
struct ModelLibraryView: View {
    @Environment(\.dismiss) var dismiss
    @EnvironmentObject var chatViewModel: ChatViewModel
    @EnvironmentObject var mlxDownloader: MLXModelDownloader
    @ObservedObject private var catalog = ModelCatalogService.shared  // Observe catalog for updates
    
    @State private var selectedModelForLicense: ModelConfiguration?
    @State private var modelToDelete: ModelConfiguration?
    @State private var showingDeleteConfirmation = false
    
    var body: some View {
        ZStack {
            List {
                // Built-In Models (AFM) - Always First
                Section("Built-In Models") {
                    ForEach(builtInModels) { model in
                        ModelRow(
                            model: model,
                            isSelected: chatViewModel.selectedModelID == model.id,
                            statusDot: modelStatusDot(for: model),
                            onSelect: { selectModel(model) }
                        )
                    }
                }
                
                // Downloaded MLX Models - Second
                if !downloadedMLXModels.isEmpty {
                    Section("Downloaded Models") {
                        ForEach(downloadedMLXModels) { model in
                            MLXModelRow(
                                model: model,
                                isSelected: chatViewModel.selectedModelID == model.id,
                                downloadState: mlxDownloader.downloadStates[model.id],
                                onSelect: { selectModel(model) },
                                onDownload: { }, // Already downloaded
                                onCancel: { mlxDownloader.cancelDownload(modelID: model.id) },
                                onDelete: { requestDeleteModel(model) }
                            )
                        }
                    }
                }
                
                // Available MLX Models - Third
                if !availableMLXModels.isEmpty {
                    Section("Available Models") {
                        ForEach(availableMLXModels) { model in
                            MLXModelRow(
                                model: model,
                                isSelected: false,
                                downloadState: mlxDownloader.downloadStates[model.id],
                                onSelect: { }, // Can't select undownloaded
                                onDownload: { downloadModel(model) },
                                onCancel: { mlxDownloader.cancelDownload(modelID: model.id) },
                                onDelete: { }  // Can't delete undownloaded
                            )
                        }
                    }
                }
            }
            .navigationTitle("Model Library")
            .navigationBarTitleDisplayMode(.inline)
            .task {
                await chatViewModel.refreshModelCatalog()
                ModelCatalogService.shared.refreshDownloadStates()
            }
            .sheet(item: $selectedModelForLicense) { model in
                ModelLicenseSheet(
                    model: model,
                    onAccept: {
                        ModelCatalogService.shared.acceptLicense(for: model.id)
                        selectedModelForLicense = nil
                        Task {
                            await mlxDownloader.startDownload(modelID: model.id, repoID: model.id)
                        }
                    },
                    onCancel: {
                        selectedModelForLicense = nil
                    }
                )
            }
            .alert("Delete Model?", isPresented: $showingDeleteConfirmation, presenting: modelToDelete) { model in
                Button("Cancel", role: .cancel) {
                    print("HALDEBUG-UI: User cancelled deletion of model: \(model.id)")
                }
                Button("Delete", role: .destructive) {
                    print("HALDEBUG-UI: User confirmed deletion of model: \(model.id)")
                    deleteModel(model)
                }
            } message: { model in
                if let size = model.sizeGB {
                    Text("This will permanently delete \(model.displayName) (\(String(format: "%.1f", size)) GB).")
                } else {
                    Text("This will permanently delete \(model.displayName).")
                }
            }
            
            // Loading overlay
            if catalog.isLoading {
                VStack(spacing: 12) {
                    ProgressView()
                        .scaleEffect(1.2)
                    Text("Loading models from Hugging Face...")
                        .font(.subheadline)
                        .foregroundColor(.secondary)
                }
                .padding(20)
                .background(.regularMaterial)
                .cornerRadius(12)
            }
        }
    }
    
    // MARK: - Model Filtering
    
    private var builtInModels: [ModelConfiguration] {
        ModelCatalogService.shared.availableModels
            .filter { $0.source == .appleFoundation }
            .sorted { $0.displayName < $1.displayName }
    }
    
    private var downloadedMLXModels: [ModelConfiguration] {
        ModelCatalogService.shared.availableModels
            .filter { $0.source == .mlx && mlxDownloader.isModelDownloaded($0.id) }
            .sorted { $0.displayName < $1.displayName }
    }
    
    private var availableMLXModels: [ModelConfiguration] {
        ModelCatalogService.shared.availableModels
            .filter { $0.source == .mlx && !mlxDownloader.isModelDownloaded($0.id) }
            .sorted { $0.displayName < $1.displayName }
    }
    
    // MARK: - Actions
    
    private func selectModel(_ model: ModelConfiguration) {
        guard model.source == .appleFoundation || mlxDownloader.isModelDownloaded(model.id) else {
            return  // Can't select undownloaded MLX model
        }
        
        Task {
            await chatViewModel.switchToModel(model)
            dismiss()
        }
    }
    
    private func downloadModel(_ model: ModelConfiguration) {
        if !ModelCatalogService.shared.hasAcceptedLicense(for: model.id) {
            selectedModelForLicense = model  // This triggers the sheet
        } else {
            Task {
                await mlxDownloader.startDownload(modelID: model.id, repoID: model.id)
            }
        }
    }
    
    private func requestDeleteModel(_ model: ModelConfiguration) {
        print("HALDEBUG-UI: Delete button tapped for model: \(model.id)")
        modelToDelete = model
        showingDeleteConfirmation = true
    }
    
    private func deleteModel(_ model: ModelConfiguration) {
        Task {
            await mlxDownloader.deleteModel(modelID: model.id)
            
            // If deleted model was active, switch to AFM and add chat messages
            if chatViewModel.selectedModelID == model.id {
                await chatViewModel.switchToModel(.appleFoundation)
                
                // Add bilateral chat messages about the switch
                await MainActor.run {
                    let userMsg = "Hal, I deleted the \(model.displayName) model."
                    let halMsg = "No problem! I've switched over to Apple Intelligence so we can keep chatting without interruption. You can re-download \(model.displayName) from the Model Library whenever you're ready."
                    chatViewModel.messages.append(ChatMessage(content: userMsg, isFromUser: true))
                    chatViewModel.messages.append(ChatMessage(content: halMsg, isFromUser: false))
                }
            }
        }
    }
    
    // MARK: - Status Dot Helper
    
    private func modelStatusDot(for model: ModelConfiguration) -> some View {
        Group {
            if model.source == .appleFoundation {
                Circle()
                    .fill(Color.green)
                    .frame(width: 8, height: 8)
            } else {
                if mlxDownloader.isModelDownloaded(model.id) {
                    Circle()
                        .fill(Color.green)
                        .frame(width: 8, height: 8)
                } else if mlxDownloader.downloadStates[model.id]?.isDownloading == true {
                    Circle()
                        .fill(Color.orange)
                        .frame(width: 8, height: 8)
                } else if mlxDownloader.downloadStates[model.id]?.error != nil {
                    Circle()
                        .fill(Color.red)
                        .frame(width: 8, height: 8)
                } else {
                    Circle()
                        .fill(Color.gray.opacity(0.5))
                        .frame(width: 8, height: 8)
                }
            }
        }
    }
}

// MARK: - Model Row (Built-In Models)
struct ModelRow: View {
    let model: ModelConfiguration
    let isSelected: Bool
    let statusDot: AnyView
    let onSelect: () -> Void
    
    init(model: ModelConfiguration, isSelected: Bool, statusDot: some View, onSelect: @escaping () -> Void) {
        self.model = model
        self.isSelected = isSelected
        self.statusDot = AnyView(statusDot)
        self.onSelect = onSelect
    }
    
    var body: some View {
        Button(action: onSelect) {
            HStack(spacing: 12) {
                VStack(alignment: .leading, spacing: 4) {
                    Text(model.displayName)
                        .font(.subheadline)
                        .fontWeight(.medium)
                        .foregroundColor(.primary)
                    if let description = model.description {
                        Text(description)
                            .font(.caption)
                            .foregroundColor(.secondary)
                            .lineLimit(2)
                    }
                }
                Spacer()
                HStack(spacing: 8) {
                    if isSelected {
                        Image(systemName: "checkmark.circle.fill")
                            .foregroundColor(.blue)
                            .font(.system(size: 16))
                    }
                    statusDot
                }
            }
            .contentShape(Rectangle())
        }
        .buttonStyle(PlainButtonStyle())
    }
}

// MARK: - MLX Model Row (Downloadable Models)
struct MLXModelRow: View {
    let model: ModelConfiguration
    let isSelected: Bool
    let downloadState: MLXModelDownloader.DownloadState?
    let onSelect: () -> Void
    let onDownload: () -> Void
    let onCancel: () -> Void
    let onDelete: () -> Void
    
    var body: some View {
        VStack(alignment: .leading, spacing: 12) {
            // Header Row
            HStack(spacing: 12) {
                VStack(alignment: .leading, spacing: 4) {
                    Text(model.displayName)
                        .font(.subheadline)
                        .fontWeight(.medium)
                    HStack(spacing: 8) {
                        if let size = model.sizeGB {
                            Text("\(String(format: "%.1f", size)) GB")
                                .font(.caption)
                                .foregroundColor(.secondary)
                        }
                        if let license = model.license {
                            Text("‚Ä¢")
                                .font(.caption)
                                .foregroundColor(.secondary)
                            Text(license.uppercased())
                                .font(.caption)
                                .foregroundColor(.secondary)
                        }
                    }
                }
                Spacer()
                statusIndicator
            }
            
            // Description (if available)
            if let description = model.description {
                Text(description)
                    .font(.caption)
                    .foregroundColor(.secondary)
                    .lineLimit(2)
            }
            
            // Action Buttons or Progress
            if let state = downloadState, state.isDownloading {
                // Downloading
                VStack(spacing: 6) {
                    ProgressView(value: state.progress)
                    HStack {
                        Text(state.message)
                            .font(.caption)
                            .foregroundColor(.secondary)
                        Spacer()
                        Button("Cancel") { onCancel() }
                            .font(.caption)
                            .foregroundColor(.red)
                    }
                }
            } else if downloadState?.localPath != nil {
                // Downloaded
                HStack(spacing: 12) {
                    Button(action: onSelect) {
                        HStack(spacing: 4) {
                            Image(systemName: isSelected ? "checkmark.circle.fill" : "circle")
                            Text(isSelected ? "Active" : "Select")
                        }
                        .contentShape(Rectangle())
                    }
                    .buttonStyle(BorderlessButtonStyle())
                    .disabled(isSelected)
                    .foregroundColor(isSelected ? .secondary : .blue)
                    .font(.caption)
                    
                    Spacer()
                    
                    Button(action: onDelete) {
                        HStack(spacing: 4) {
                            Image(systemName: "trash")
                            Text("Delete")
                        }
                        .contentShape(Rectangle())
                    }
                    .buttonStyle(BorderlessButtonStyle())
                    .foregroundColor(.red)
                    .font(.caption)
                }
            } else if let state = downloadState, state.error != nil {
                // Error state
                VStack(alignment: .leading, spacing: 6) {
                    Text(state.error ?? "Download failed")
                        .font(.caption)
                        .foregroundColor(.red)
                    Button(action: onDownload) {
                        HStack(spacing: 4) {
                            Image(systemName: "arrow.clockwise")
                            Text("Retry")
                        }
                    }
                    .foregroundColor(.blue)
                    .font(.caption)
                }
            } else {
                // Not Downloaded - Show download button
                Button(action: onDownload) {
                    HStack(spacing: 4) {
                        Image(systemName: "arrow.down.circle.fill")
                        Text("Download")
                    }
                }
                .foregroundColor(.blue)
                .font(.caption)
            }
        }
        .padding(.vertical, 4)
    }
    
    private var statusIndicator: some View {
        Group {
            if downloadState?.localPath != nil {
                Circle().fill(Color.green).frame(width: 8, height: 8)
            } else if downloadState?.isDownloading == true {
                Circle().fill(Color.orange).frame(width: 8, height: 8)
            } else if downloadState?.error != nil {
                Circle().fill(Color.red).frame(width: 8, height: 8)
            } else {
                Circle().fill(Color.gray.opacity(0.5)).frame(width: 8, height: 8)
            }
        }
    }
}

// MARK: - Model License Sheet
struct ModelLicenseSheet: View {
    let model: ModelConfiguration
    let onAccept: () -> Void
    let onCancel: () -> Void
    
    var body: some View {
        NavigationView {
            ScrollView {
                VStack(alignment: .leading, spacing: 20) {
                    // Header
                    VStack(alignment: .leading, spacing: 8) {
                        Text(licenseName)
                            .font(.title2)
                            .fontWeight(.bold)
                        
                        Text("By downloading \(model.displayName), you agree to its license terms.")
                            .font(.subheadline)
                            .foregroundColor(.secondary)
                    }
                    
                    // Download Warnings FIRST
                    if let size = model.sizeGB {
                        VStack(alignment: .leading, spacing: 12) {
                            // Size warning
                            VStack(alignment: .leading, spacing: 6) {
                                HStack {
                                    Image(systemName: "exclamationmark.triangle.fill")
                                        .foregroundColor(.orange)
                                    Text("Large Download: \(String(format: "%.1f", size)) GB")
                                        .fontWeight(.semibold)
                                }
                                Text("Requires \(String(format: "%.1f", size)) GB storage and bandwidth")
                                    .font(.caption)
                                    .foregroundColor(.secondary)
                            }
                            .padding()
                            .background(Color.orange.opacity(0.1))
                            .cornerRadius(8)
                            
                            // WiFi warning for >1GB
                            if size > 1.0 {
                                VStack(alignment: .leading, spacing: 6) {
                                    HStack {
                                        Image(systemName: "wifi")
                                            .foregroundColor(.blue)
                                        Text("WiFi Recommended")
                                            .fontWeight(.semibold)
                                    }
                                    Text("Connect to WiFi to avoid cellular data charges")
                                        .font(.caption)
                                        .foregroundColor(.secondary)
                                }
                                .padding()
                                .background(Color.blue.opacity(0.1))
                                .cornerRadius(8)
                            }
                        }
                    }
                    
                    Divider()
                    
                    // License Info
                    VStack(alignment: .leading, spacing: 12) {
                        Text("License: \(model.license?.uppercased() ?? "CUSTOM")")
                            .font(.headline)
                        
                        if let description = licenseDescription {
                            Text(description)
                                .font(.subheadline)
                                .foregroundColor(.secondary)
                        }
                        
                        Link(destination: URL(string: "https://huggingface.co/\(model.id)")!) {
                            HStack {
                                Image(systemName: "link")
                                Text("View Full License on Hugging Face")
                                Spacer()
                                Image(systemName: "arrow.up.right")
                            }
                            .foregroundColor(.blue)
                        }
                        .padding()
                        .background(Color.blue.opacity(0.1))
                        .cornerRadius(8)
                    }
                }
                .padding()
            }
            .navigationTitle(model.displayName)
            .navigationBarTitleDisplayMode(.inline)
            .toolbar {
                ToolbarItem(placement: .cancellationAction) {
                    Button("Cancel", action: onCancel)
                }
                ToolbarItem(placement: .confirmationAction) {
                    Button("Accept & Download", action: onAccept)
                        .fontWeight(.semibold)
                }
            }
        }
    }
    
    private var licenseName: String {
        guard let license = model.license else { return "License Agreement" }
        
        switch license.lowercased() {
        case "mit": return "MIT License"
        case "apache-2.0": return "Apache 2.0"
        case "llama2": return "Llama 2 Community License"
        case "llama3", "llama3.1", "llama3.2": return "Llama 3 Community License"
        case "gemma": return "Gemma Terms of Use"
        default: return "\(license.uppercased()) License"
        }
    }
    
    private var licenseDescription: String? {
        guard let license = model.license else { return nil }
        
        switch license.lowercased() {
        case "mit":
            return "Permissive license allowing commercial and private use with minimal restrictions."
        case "apache-2.0":
            return "Permissive license allowing commercial use with patent grant."
        case "llama2", "llama3", "llama3.1", "llama3.2":
            return "Meta's community license. Review full terms for commercial use restrictions."
        case "gemma":
            return "Google's Gemma Terms. Review full terms for usage requirements."
        default:
            return "Please review the full license terms before downloading."
        }
    }
}

// ==== LEGO END: 11.5 Model Library UI ====


// ==== LEGO START: 11.6 UI Helper Components ====

// MARK: - Reusable UI Helper Components
// These components eliminate deep nesting and provide consistent UI patterns throughout the app.
// All components are designed to be composable and maintain Hal's visual style.

// MARK: - Section Header View
/// Consistent styling for section headers (e.g., "SHORT-TERM MEMORY", "LONG-TERM MEMORY")
struct SectionHeaderText: View {
    let text: String
    
    var body: some View {
        Text(text)
            .font(.caption)
            .fontWeight(.semibold)
            .foregroundColor(.secondary)
    }
}

// MARK: - Labeled Slider Control
/// Reusable slider with label, current value display, min/max labels, and optional helper text
/// Eliminates the repetitive VStack(HStack(Text+Spacer+Text) + Slider + Text) pattern
struct LabeledSliderControl: View {
    let label: String
    let value: Binding<Double>
    let range: ClosedRange<Double>
    let step: Double
    let valueFormatter: (Double) -> String
    let minLabel: String
    let maxLabel: String
    let helperText: String?
    let onEditingChanged: ((Bool) -> Void)?
    
    var body: some View {
        VStack(alignment: .leading, spacing: 12) {
            // Label + Value Display
            HStack {
                Text(label)
                    .font(.subheadline)
                Spacer()
                Text(valueFormatter(value.wrappedValue))
                    .font(.subheadline)
                    .foregroundColor(.secondary)
            }
            
            // Slider with min/max labels
            Slider(
                value: value,
                in: range,
                step: step,
                label: { Text(label) },
                minimumValueLabel: { Text(minLabel).font(.caption2) },
                maximumValueLabel: { Text(maxLabel).font(.caption2) },
                onEditingChanged: onEditingChanged ?? { _ in }
            )
            
            // Helper text (if provided)
            if let helperText = helperText {
                Text(helperText)
                    .font(.caption2)
                    .foregroundColor(.secondary)
            }
        }
    }
}

// MARK: - Labeled Stepper Control
/// Reusable stepper with label, current value display, and optional helper text
/// Used for integer-based controls like Max RAG Retrieval
struct LabeledStepperControl: View {
    let label: String
    let value: Binding<Double>
    let range: ClosedRange<Double>
    let step: Double
    let valueFormatter: (Double) -> String
    let helperText: String?
    
    var body: some View {
        VStack(alignment: .leading, spacing: 12) {
            // Label + Value Display
            HStack {
                Text(label)
                    .font(.subheadline)
                Spacer()
                Text(valueFormatter(value.wrappedValue))
                    .font(.subheadline)
                    .foregroundColor(.secondary)
            }
            
            // Stepper (hidden label, value display handled above)
            Stepper(
                value: value,
                in: range,
                step: step
            ) {
                EmptyView()
            }
            
            // Helper text (if provided)
            if let helperText = helperText {
                Text(helperText)
                    .font(.caption2)
                    .foregroundColor(.secondary)
            }
        }
    }
}

// MARK: - Info Box View
/// Reusable styled info/warning/error box with icon, title, and message
/// Used throughout the app for alerts, warnings, and informational messages
struct InfoBoxView: View {
    enum Style {
        case info
        case warning
        case error
        case custom(color: Color, icon: String)
        
        var color: Color {
            switch self {
            case .info: return .blue
            case .warning: return .orange
            case .error: return .red
            case .custom(let color, _): return color
            }
        }
        
        var icon: String {
            switch self {
            case .info: return "info.circle.fill"
            case .warning: return "exclamationmark.triangle.fill"
            case .error: return "xmark.circle.fill"
            case .custom(_, let icon): return icon
            }
        }
    }
    
    let style: Style
    let title: String
    let message: String
    let fontSize: Font = .system(size: 16)
    
    var body: some View {
        HStack(alignment: .top, spacing: 8) {
            Image(systemName: style.icon)
                .foregroundColor(style.color)
                .font(fontSize)
            
            VStack(alignment: .leading, spacing: 4) {
                Text(title)
                    .font(.subheadline)
                    .fontWeight(.semibold)
                
                Text(message)
                    .font(.caption)
                    .foregroundColor(.secondary)
            }
        }
        .padding()
        .background(style.color.opacity(0.1))
        .cornerRadius(8)
    }
}

// MARK: - Link Button View
/// Styled link button with icon (used for external links like Hugging Face)
struct LinkButtonView: View {
    let destination: URL
    let title: String
    let leadingIcon: String
    let trailingIcon: String
    let accentColor: Color
    
    var body: some View {
        Link(destination: destination) {
            HStack {
                Image(systemName: leadingIcon)
                Text(title)
                Spacer()
                Image(systemName: trailingIcon)
            }
            .padding()
            .background(accentColor.opacity(0.1))
            .foregroundColor(accentColor)
            .cornerRadius(8)
        }
    }
}

// MARK: - Text Block View
/// Styled text block with background (used for license text, code blocks, etc.)
struct TextBlockView: View {
    let text: String
    let backgroundColor: Color
    let textColor: Color
    let font: Font
    
    init(
        text: String,
        backgroundColor: Color = Color.secondary.opacity(0.1),
        textColor: Color = .primary,
        font: Font = .caption
    ) {
        self.text = text
        self.backgroundColor = backgroundColor
        self.textColor = textColor
        self.font = font
    }
    
    var body: some View {
        Text(text)
            .font(font)
            .foregroundColor(textColor)
            .padding()
            .frame(maxWidth: .infinity, alignment: .leading)
            .background(backgroundColor)
            .cornerRadius(8)
    }
}

// MARK: - Widget Test View
/// Test view to verify all UI helper components work correctly before integration
/// USAGE: Temporarily add WidgetTestView() to your main view hierarchy to test
/// Remove this entire section after verification
struct WidgetTestView: View {
    @State private var sliderValue1: Double = 5.0
    @State private var sliderValue2: Double = 0.7
    @State private var sliderValue3: Double = 0.5
    @State private var stepperValue: Double = 800
    
    var body: some View {
        NavigationView {
            ScrollView {
                VStack(spacing: 30) {
                    // Header
                    Text("UI Helper Components Test")
                        .font(.title)
                        .fontWeight(.bold)
                        .padding(.top)
                    
                    Divider()
                    
                    // Test Section Headers
                    VStack(alignment: .leading, spacing: 12) {
                        Text("Section Headers").font(.headline)
                        SectionHeaderText(text: "SHORT-TERM MEMORY")
                            .onAppear { print("‚úÖ SectionHeaderText rendered") }
                        SectionHeaderText(text: "LONG-TERM MEMORY")
                    }
                    .frame(maxWidth: .infinity, alignment: .leading)
                    
                    Divider()
                    
                    // Test Labeled Slider (Integer)
                    VStack(alignment: .leading, spacing: 12) {
                        Text("Labeled Slider (Integer)").font(.headline)
                        LabeledSliderControl(
                            label: "Memory Depth",
                            value: $sliderValue1,
                            range: 1...10,
                            step: 1,
                            valueFormatter: { "\(Int($0)) turns" },
                            minLabel: "1",
                            maxLabel: "10",
                            helperText: "Number of conversation turns to keep in short-term memory",
                            onEditingChanged: { editing in
                                if editing {
                                    print("üéöÔ∏è Slider editing started: \(sliderValue1)")
                                } else {
                                    print("üéöÔ∏è Slider editing ended: \(sliderValue1)")
                                }
                            }
                        )
                        .onAppear { print("‚úÖ LabeledSliderControl (int) rendered") }
                        .onChange(of: sliderValue1) { oldValue, newValue in
                            print("üìä Slider value changed: \(oldValue) ‚Üí \(newValue)")
                        }
                    }
                    .frame(maxWidth: .infinity, alignment: .leading)
                    
                    Divider()
                    
                    // Test Labeled Slider (Float with 2 decimals)
                    VStack(alignment: .leading, spacing: 12) {
                        Text("Labeled Slider (Float)").font(.headline)
                        LabeledSliderControl(
                            label: "Similarity Threshold",
                            value: $sliderValue2,
                            range: 0.0...1.0,
                            step: 0.05,
                            valueFormatter: { String(format: "%.2f", $0) },
                            minLabel: "0.0",
                            maxLabel: "1.0",
                            helperText: "Minimum similarity for memory retrieval (higher = stricter)",
                            onEditingChanged: { editing in
                                if editing {
                                    print("üéöÔ∏è Float slider editing started: \(sliderValue2)")
                                } else {
                                    print("üéöÔ∏è Float slider editing ended: \(sliderValue2)")
                                }
                            }
                        )
                        .onAppear { print("‚úÖ LabeledSliderControl (float) rendered") }
                        .onChange(of: sliderValue2) { oldValue, newValue in
                            print("üìä Float slider changed: \(oldValue) ‚Üí \(newValue)")
                        }
                    }
                    .frame(maxWidth: .infinity, alignment: .leading)
                    
                    Divider()
                    
                    // Test Labeled Slider (Percentage)
                    VStack(alignment: .leading, spacing: 12) {
                        Text("Labeled Slider (Percentage)").font(.headline)
                        LabeledSliderControl(
                            label: "Recency Weight",
                            value: $sliderValue3,
                            range: 0.0...1.0,
                            step: 0.05,
                            valueFormatter: { "\(Int($0 * 100))%" },
                            minLabel: "0%",
                            maxLabel: "100%",
                            helperText: "Balance between relevance (left) and freshness (right)",
                            onEditingChanged: { editing in
                                if editing {
                                    print("üéöÔ∏è Percentage slider editing started: \(sliderValue3)")
                                } else {
                                    print("üéöÔ∏è Percentage slider editing ended: \(sliderValue3)")
                                }
                            }
                        )
                        .onAppear { print("‚úÖ LabeledSliderControl (percentage) rendered") }
                        .onChange(of: sliderValue3) { oldValue, newValue in
                            print("üìä Percentage slider changed: \(oldValue) ‚Üí \(newValue)")
                        }
                    }
                    .frame(maxWidth: .infinity, alignment: .leading)
                    
                    Divider()
                    
                    // Test Labeled Stepper
                    VStack(alignment: .leading, spacing: 12) {
                        Text("Labeled Stepper").font(.headline)
                        LabeledStepperControl(
                            label: "Max RAG Retrieval",
                            value: $stepperValue,
                            range: 200...2000,
                            step: 100,
                            valueFormatter: { "\(Int($0)) chars" },
                            helperText: "Maximum characters for RAG snippet retrieval"
                        )
                        .onAppear { print("‚úÖ LabeledStepperControl rendered") }
                        .onChange(of: stepperValue) { oldValue, newValue in
                            print("üìä Stepper value changed: \(oldValue) ‚Üí \(newValue)")
                        }
                    }
                    .frame(maxWidth: .infinity, alignment: .leading)
                    
                    Divider()
                    
                    // Test Info Boxes
                    VStack(alignment: .leading, spacing: 16) {
                        Text("Info Boxes").font(.headline)
                        
                        InfoBoxView(
                            style: .info,
                            title: "Information",
                            message: "This is an informational message with blue styling."
                        )
                        .onAppear { print("‚úÖ InfoBoxView (info) rendered") }
                        
                        InfoBoxView(
                            style: .warning,
                            title: "Warning",
                            message: "This is a warning message with orange styling."
                        )
                        .onAppear { print("‚úÖ InfoBoxView (warning) rendered") }
                        
                        InfoBoxView(
                            style: .error,
                            title: "Error",
                            message: "This is an error message with red styling."
                        )
                        .onAppear { print("‚úÖ InfoBoxView (error) rendered") }
                        
                        InfoBoxView(
                            style: .custom(color: .green, icon: "checkmark.circle.fill"),
                            title: "Custom Style",
                            message: "This is a custom styled message with green color."
                        )
                        .onAppear { print("‚úÖ InfoBoxView (custom) rendered") }
                    }
                    .frame(maxWidth: .infinity, alignment: .leading)
                    
                    Divider()
                    
                    // Test Link Button
                    VStack(alignment: .leading, spacing: 12) {
                        Text("Link Button").font(.headline)
                        LinkButtonView(
                            destination: URL(string: "https://huggingface.co/mlx-community")!,
                            title: "View on Hugging Face",
                            leadingIcon: "link",
                            trailingIcon: "arrow.up.right",
                            accentColor: .blue
                        )
                        .onAppear { print("‚úÖ LinkButtonView rendered") }
                    }
                    .frame(maxWidth: .infinity, alignment: .leading)
                    
                    Divider()
                    
                    // Test Text Block
                    VStack(alignment: .leading, spacing: 12) {
                        Text("Text Block").font(.headline)
                        TextBlockView(
                            text: "This is a styled text block that can display longer content like license text, code snippets, or other formatted information.",
                            backgroundColor: Color.secondary.opacity(0.1),
                            textColor: .primary,
                            font: .caption
                        )
                        .onAppear { print("‚úÖ TextBlockView rendered") }
                    }
                    .frame(maxWidth: .infinity, alignment: .leading)
                    
                    // Summary
                    VStack(spacing: 12) {
                        Text("‚úÖ All Components Loaded")
                            .font(.headline)
                            .foregroundColor(.green)
                        Text("Check console for interaction logs")
                            .font(.caption)
                            .foregroundColor(.secondary)
                    }
                    .padding()
                    .background(Color.green.opacity(0.1))
                    .cornerRadius(8)
                }
                .padding()
            }
            .navigationTitle("Widget Tests")
            .navigationBarTitleDisplayMode(.inline)
        }
        .onAppear {
            print("============================================================")
            print("üß™ UI HELPER COMPONENTS TEST VIEW LOADED")
            print("============================================================")
            print("Interact with controls to verify functionality")
            print("Watch console for event logging")
            print("============================================================")
        }
    }
}

// ==== LEGO END: 11.6 UI Helper Components ====
    
    
    
// ==== LEGO START: 13 ChatBubbleView & TimerView (Message UI Components) ====
    
    // MARK: - ChatBubbleView (from Hal10000App.swift for consistent UI)
    struct ChatBubbleView: View {
        let message: ChatMessage
        let messageIndex: Int
        @EnvironmentObject var chatViewModel: ChatViewModel
        @State private var showingDetails: Bool = false
        // Provide screen width directly
        private var screenWidth: CGFloat {
            let scene = UIApplication.shared.connectedScenes
                .compactMap { $0 as? UIWindowScene }
                .first { $0.activationState == .foregroundActive }
            return scene?.screen.bounds.width ?? 0
        }
        
        var actualTurnNumber: Int {
            if message.isFromUser {
                return (messageIndex / 2) + 1
            } else {
                return ((messageIndex + 1) / 2)
            }
        }
        
        var metadataText: String {
            var parts: [String] = []
            parts.append("Turn \(actualTurnNumber)")
            parts.append("~\(message.content.split(separator: " ").count) tokens")
            parts.append(message.timestamp.formatted(date: .abbreviated, time: .shortened))
            if let duration = message.thinkingDuration {
                parts.append(String(format: "%.1f sec", duration))
            }
            return parts.joined(separator: " ¬∑ ")
        }
        
        // MARK: - Status Message Detection
        var isStatusMessage: Bool {
            ["Reading your message...",
             "Reviewing our recent conversation... (short-term memory)",
             "Recalling relevant memories... (long-term memory)",
             "Formulating a reply..."].contains(message.content)
        }
        
        // MARK: - Footer View (Updated with Processing/Inference labels)
        @ViewBuilder
        var footerView: some View {
            VStack(alignment: .trailing, spacing: 2) {
                if message.isPartial {
                    HStack(spacing: 6) {
                        ProgressView()
                            .scaleEffect(0.8)
                            .tint(.gray)
                        Text("Processing...")
                            .font(.caption2)
                            .foregroundColor(.gray)
                        TimerView(startDate: message.timestamp)
                    }
                    .transition(.opacity)
                    .fixedSize(horizontal: false, vertical: true)
                } else {
                    let formattedDate = message.timestamp.formatted(date: .abbreviated, time: .shortened)
                    let turnText = "Turn \(actualTurnNumber)"
                    let durationText = message.thinkingDuration.map { String(format: "Inference %.1f sec", $0) }
                    let footerString = [formattedDate, turnText, durationText]
                        .compactMap { $0 }
                        .joined(separator: ", ")
                    
                    HStack {
                        Text(footerString)
                            .font(.caption2)
                            .foregroundColor(.gray)
                    }
                    .transition(.opacity)
                    .fixedSize(horizontal: false, vertical: true)
                }
            }
            .padding(.top, 2)
        }
        
        private func buildDetailsShareText() -> String {
            var lines: [String] = []
            lines.append("Assistant response (turn \(actualTurnNumber)):")
            lines.append(message.content)
            lines.append("")
            if let prompt = message.fullPromptUsed, !prompt.isEmpty {
                lines.append("‚Äî Full Prompt Used ‚Äî")
                lines.append(prompt)
                lines.append("")
            }
            if let ctx = message.usedContextSnippets, !ctx.isEmpty {
                lines.append("‚Äî Context Snippets ‚Äî")
                for (i, s) in ctx.enumerated() {
                    let src = s.source
                    let rel = String(format: "%.2f", s.relevance)
                    lines.append("[\(i+1)] src=\(src) rel=\(rel)")
                    lines.append(s.content)
                    lines.append("")
                }
            }
            return lines.joined(separator: "\n")
        }
        
        var body: some View {
            HStack {
                if message.isFromUser {
                    Spacer()
                    VStack(alignment: .trailing, spacing: 0) {
                        Text(message.content)
                            .font(.title3)
                            .textSelection(.enabled)
                            .padding(.vertical, 10)
                            .padding(.horizontal, 14)
                            .frame(maxWidth: screenWidth * 0.75, alignment: .trailing)
                            .background(Color.accentColor.opacity(0.7))
                            .foregroundColor(.white)
                            .cornerRadius(12)
                            .transition(.move(edge: .bottom))
                            .contextMenu {
                                Button {
                                    UIPasteboard.general.string = message.content
                                } label: {
                                    Label("Copy Message", systemImage: "doc.on.doc")
                                }
                                Button {
                                    UIPasteboard.general.string = chatViewModel.exportChatHistory()
                                } label: {
                                    Label("Copy Conversation", systemImage: "doc.on.doc.fill")
                                }
                                Button {
                                    UIPasteboard.general.string = buildDetailsShareText()
                                } label: {
                                    Label("Copy Message Detailed", systemImage: "doc.text.magnifyingglass")
                                }
                                Button {
                                    UIPasteboard.general.string = chatViewModel.exportChatHistoryDetailed()
                                } label: {
                                    Label("Copy Conversation Detailed", systemImage: "doc.text.fill")
                                }
                            }
                        footerView
                    }
                } else {
                    VStack(alignment: .trailing, spacing: 0) {
                        VStack(alignment: .leading, spacing: 6) {
                            Text(message.content)
                                .font(.title3)
                                .italic(isStatusMessage)
                                .foregroundColor(isStatusMessage ? .secondary : .primary)
                                .textSelection(.enabled)
                                .padding(.vertical, 10)
                                .padding(.horizontal, 14)
                                .frame(maxWidth: screenWidth * 0.75, alignment: .leading)
                                .background(Color.gray.opacity(0.3))
                                .cornerRadius(12)
                            if chatViewModel.showInlineDetails {
                                VStack(alignment: .leading, spacing: 4) {
                                    Text(buildDetailsShareText())
                                        .font(.caption2)
                                        .foregroundColor(.secondary)
                                        .padding(6)
                                        .background(Color.gray.opacity(0.15))
                                        .cornerRadius(8)
                                }
                                .transition(.opacity)
                            }
                        }
                        .contextMenu {
                            Button {
                                UIPasteboard.general.string = message.content
                            } label: {
                                Label("Copy Message", systemImage: "doc.on.doc")
                            }
                            Button {
                                UIPasteboard.general.string = chatViewModel.exportChatHistory()
                            } label: {
                                Label("Copy Conversation", systemImage: "doc.on.doc.fill")
                            }
                            Button {
                                UIPasteboard.general.string = buildDetailsShareText()
                            } label: {
                                Label("Copy Message Detailed", systemImage: "doc.text.magnifyingglass")
                            }
                            Button {
                                UIPasteboard.general.string = chatViewModel.exportChatHistoryDetailed()
                            } label: {
                                Label("Copy Conversation Detailed", systemImage: "doc.text.fill")
                            }
                            Divider()
                            Button {
                                chatViewModel.showInlineDetails.toggle()
                            } label: {
                                Label("View Details", systemImage: "info.circle")
                            }
                        }
                        footerView
                    }
                    Spacer()
                }
            }
            .padding(.horizontal)
            .padding(.vertical, 4)
            .animation(.linear(duration: 0.1), value: message.content)
            .animation(.interactiveSpring(response: 0.6,
                                          dampingFraction: 0.7,
                                          blendDuration: 0.3),
                       value: message.isPartial)
            .animation(.interactiveSpring(response: 0.6,
                                          dampingFraction: 0.7,
                                          blendDuration: 0.3),
                       value: message.id)
            .onAppear {
                if message.isPartial {
                    print("HALDEBUG-UI: Displaying partial message bubble (turn \(actualTurnNumber))")
                }
            }
            .onChange(of: message.isPartial) { _, newValue in
                if !newValue && message.content.count > 0 {
                    print("HALDEBUG-UI: Message bubble completed - turn \(actualTurnNumber), \(message.content.count) characters")
                }
            }
        }
    }
    
    // TimerView
    struct TimerView: View {
        let startDate: Date
        @State private var hasLoggedLongThinking = false
        var body: some View {
            TimelineView(.periodic(from: startDate, by: 0.5)) { context in
                let elapsed = context.date.timeIntervalSince(startDate)
                if elapsed > 30.0 && !hasLoggedLongThinking {
                    DispatchQueue.main.async {
                        print("HALDEBUG-MODEL: Long thinking time detected - \(String(format: "%.1f", elapsed)) seconds")
                        hasLoggedLongThinking = true
                    }
                }
                return Text(String(format: "%.1f sec", max(0, elapsed)))
                    .font(.caption2)
                    .foregroundStyle(.secondary)
            }
        }
    }
// ==== LEGO END: 13 ChatBubbleView & TimerView (Message UI Components) ====
    
    
    
// ==== LEGO START: 14 PromptDetailView (Full Prompt & Context Viewer) ====
    // MARK: - PromptDetailView (NEW: Displays full prompt and context)
    struct PromptDetailView: View {
        let message: ChatMessage // The Hal message for which we want to see details
        @Environment(\.dismiss) var dismiss
        
        var body: some View {
            NavigationView {
                ScrollView {
                    VStack(alignment: .leading, spacing: 20) {
                        if let prompt = message.fullPromptUsed {
                            Text("Full Prompt Used:")
                                .font(.headline)
                            Text(prompt)
                                .font(.footnote)
                                .textSelection(.enabled)
                                .padding()
                                .background(Color.gray.opacity(0.1))
                                .cornerRadius(8)
                        } else {
                            Text("No full prompt available for this response.")
                                .font(.subheadline)
                                .foregroundColor(.secondary)
                        }
                        
                        if let snippets = message.usedContextSnippets, !snippets.isEmpty {
                            Text("Context Snippets Used:")
                                .font(.headline)
                            
                            ForEach(snippets) { snippet in
                                VStack(alignment: .leading, spacing: 5) {
                                    Text("Source: \(snippet.source.capitalized)")
                                        .font(.caption)
                                        .foregroundColor(.blue)
                                    Text(snippet.content)
                                        .font(.footnote)
                                        .textSelection(.enabled)
                                        .padding(8)
                                        .background(Color.green.opacity(0.1))
                                        .cornerRadius(6)
                                    if let urlString = snippet.filePath {
                                        let url = URL(fileURLWithPath: urlString)
                                        Button("Open Source Document") {
                                            UIApplication.shared.open(url) { success in
                                                if !success {
                                                    print("HALDEBUG-DEEPLINK: Failed to open document at path: \(urlString)")
                                                }
                                            }
                                        }
                                        .font(.caption)
                                        .padding(.horizontal, 8)
                                        .padding(.vertical, 4)
                                        .background(Capsule().fill(Color.blue.opacity(0.2)))
                                        .foregroundColor(.blue)
                                    }
                                }
                                .padding(.bottom, 5)
                            }
                        } else {
                            Text("No specific context snippets were used for this response.")
                                .font(.subheadline)
                                .foregroundColor(.secondary)
                        }
                        
                        // Token Usage Breakdown
                        if let breakdown = message.tokenBreakdown {
                            Divider()
                                .padding(.vertical, 10)
                            
                            Text("Token Usage")
                                .font(.headline)
                            
                            VStack(alignment: .leading, spacing: 8) {
                                HStack {
                                    Text("System:")
                                        .font(.system(.footnote, design: .monospaced))
                                        .foregroundColor(.secondary)
                                    Spacer()
                                    Text("‚âà \(formatTokenCount(breakdown.systemTokens))")
                                        .font(.system(.footnote, design: .monospaced))
                                }
                                
                                HStack {
                                    Text("Summary:")
                                        .font(.system(.footnote, design: .monospaced))
                                        .foregroundColor(.secondary)
                                    Spacer()
                                    Text("‚âà \(formatTokenCount(breakdown.summaryTokens))")
                                        .font(.system(.footnote, design: .monospaced))
                                }
                                
                                HStack {
                                    Text("RAG Context:")
                                        .font(.system(.footnote, design: .monospaced))
                                        .foregroundColor(.secondary)
                                    Spacer()
                                    Text("‚âà \(formatTokenCount(breakdown.ragTokens))")
                                        .font(.system(.footnote, design: .monospaced))
                                }
                                
                                HStack {
                                    Text("Short-Term:")
                                        .font(.system(.footnote, design: .monospaced))
                                        .foregroundColor(.secondary)
                                    Spacer()
                                    Text("‚âà \(formatTokenCount(breakdown.shortTermTokens))")
                                        .font(.system(.footnote, design: .monospaced))
                                }
                                
                                HStack {
                                    Text("User Input:")
                                        .font(.system(.footnote, design: .monospaced))
                                        .foregroundColor(.secondary)
                                    Spacer()
                                    Text("‚âà \(formatTokenCount(breakdown.userInputTokens))")
                                        .font(.system(.footnote, design: .monospaced))
                                }
                                
                                Divider()
                                    .padding(.vertical, 4)
                                
                                HStack {
                                    Text("Prompt (in):")
                                        .font(.system(.footnote, design: .monospaced))
                                        .fontWeight(.semibold)
                                    Spacer()
                                    Text("‚âà \(formatTokenCount(breakdown.totalPromptTokens))")
                                        .font(.system(.footnote, design: .monospaced))
                                        .fontWeight(.semibold)
                                }
                                
                                HStack {
                                    Text("Completion (out):")
                                        .font(.system(.footnote, design: .monospaced))
                                        .fontWeight(.semibold)
                                    Spacer()
                                    Text("‚âà \(formatTokenCount(breakdown.completionTokens))")
                                        .font(.system(.footnote, design: .monospaced))
                                        .fontWeight(.semibold)
                                }
                                
                                Divider()
                                    .padding(.vertical, 4)
                                
                                HStack {
                                    Text("Total:")
                                        .font(.system(.footnote, design: .monospaced))
                                        .fontWeight(.bold)
                                    Spacer()
                                    Text("‚âà \(formatTokenCount(breakdown.totalTokens)) / \(formatTokenCount(breakdown.contextWindowSize))")
                                        .font(.system(.footnote, design: .monospaced))
                                        .fontWeight(.bold)
                                }
                                
                                HStack {
                                    Text("Window Usage:")
                                        .font(.system(.footnote, design: .monospaced))
                                        .foregroundColor(.secondary)
                                    Spacer()
                                    Text(String(format: "%.1f%%", breakdown.percentageUsed))
                                        .font(.system(.footnote, design: .monospaced))
                                }
                            }
                            .padding()
                            .background(Color.gray.opacity(0.08))
                            .cornerRadius(8)
                        }
                    }
                    .padding()
                }
                .navigationTitle("Prompt Details")
                .navigationBarTitleDisplayMode(.inline)
                .toolbar {
                    ToolbarItem(placement: .navigationBarTrailing) {
                        Button("Done") {
                            dismiss()
                        }
                    }
                }
            }
        }
        
        // Helper to format token counts with thousand separators
        private func formatTokenCount(_ count: Int) -> String {
            let formatter = NumberFormatter()
            formatter.numberStyle = .decimal
            formatter.groupingSeparator = " "
            return formatter.string(from: NSNumber(value: count)) ?? "\(count)"
        }
    }
// ==== LEGO END: 14 PromptDetailView (Full Prompt & Context Viewer) ====
    
    
    
// ==== LEGO START: 15 ShareSheet (Export Utility) ====
    // MARK: - ShareSheet for Exporting (New Utility)
    struct ShareSheet: UIViewControllerRepresentable {
        var activityItems: [Any]
        var applicationActivities: [UIActivity]? = nil
        
        func makeUIViewController(context: Context) -> UIActivityViewController {
            let controller = UIActivityViewController(activityItems: activityItems, applicationActivities: applicationActivities)
            return controller
        }
        
        func updateUIViewController(_ uiViewController: UIViewControllerType, context: Context) {}
    }

// ==== LEGO END: 15 ShareSheet (Export Utility) ====



// ==== LEGO START: 16 View Extensions (cornerRadius & conditional modifier) ====
// Extension to allow specific corners to be rounded
extension View {
    func cornerRadius(_ radius: CGFloat, corners: UIRectCorner) -> some View {
        clipShape(RoundedCorner(radius: radius, corners: corners))
    }

    @ViewBuilder
    func `if`<Content: View>(_ condition: Bool, transform: (Self) -> Content) -> some View {
        if condition {
            transform(self)
        } else {
            self
        }
    }
}

// Helper for cornerRadius extension
struct RoundedCorner: Shape {
    var radius: CGFloat = .infinity
    var corners: UIRectCorner = .allCorners

    func path(in rect: CGRect) -> Path {
        let path = UIBezierPath(roundedRect: rect, byRoundingCorners: corners, cornerRadii: CGSize(width: radius, height: radius))
        return Path(path.cgPath)
    }
}

// MARK: - Token Estimation Utility
struct TokenEstimator {
    /// Estimates token count from text using Apple's recommended 3.5 characters per token average
    /// This is an approximation - actual tokenization may vary
    static func estimateTokens(from text: String) -> Int {
        let characterCount = text.count
        let estimatedTokens = Double(characterCount) / 3.5
        return max(1, Int(estimatedTokens.rounded()))
    }
    
    /// Estimates character count from token count using Apple's recommended 3.5 characters per token average
    /// This is the inverse of estimateTokens() and maintains symmetry
    /// This is an approximation - actual tokenization may vary
    static func estimateChars(from tokens: Int) -> Int {
        let estimatedChars = Double(tokens) * 3.5
        return max(1, Int(estimatedChars.rounded()))
    }
}


// ==== LEGO END: 16 View Extensions (cornerRadius & conditional modifier) ====



// ==== LEGO START: 17 ChatViewModel (Core Properties & Init) ====

// MARK: - Salon Mode Configuration Structures
// HelPML Alignment: These structures mirror HelPML's orchestration model
// When HelPML parsing/generation is implemented, these will be the internal representation
// that HelPML converts to/from. No stub methods - these are complete, functional structures.

/// Represents how models interact in Salon Mode
/// Maps to HelPML <orchestration type="...">
enum SalonInfluenceRule: String, Codable {
    case parallel       // Solo: Each model sees only user prompt (HelPML: type="parallel")
    case sequential     // Each model sees all prior responses (HelPML: type="sequential")
    case hierarchical   // All parallel, then Hal synthesizes (HelPML: type="hierarchical")
}

/// Behavioral constraints applied per-model via system prompt
/// Maps to HelPML <constraint type="...">
enum SalonBehavioralMode: String, Codable {
    case unrestricted           // No constraints
    case addNewOnly            // Only provide new information
    case critiqueOnly          // Only critique prior responses
    case answerAndChallenge    // Answer + challenge one prior model
    case correctionsOnly       // Only provide corrections/clarifications
}

/// Complete Salon Mode configuration
/// Maps to HelPML <helpml> document structure
struct SalonConfiguration: Codable, Equatable {
    var isEnabled: Bool = false
    var activeModelIDs: [String] = []                    // HelPML: <models> children IDs
    var speakingOrder: [String] = []                     // Order for sequential mode
    var influenceRule: SalonInfluenceRule = .sequential  // HelPML: <orchestration type>
    var behavioralModes: [String: SalonBehavioralMode] = [:]  // Per-model constraints
    var modelColors: [String: String] = [:]              // Hex colors for attribution (HelPML compatible)
    
    // Advanced Timing controls (optional guardrails for balanced participation)
    var equalLengthEnforcement: Bool = false             // Enforce similar response lengths
    var balanceContributionsAutomatically: Bool = false  // Automatically balance participation
    
    // Validate configuration
    var isValid: Bool {
        guard isEnabled else { return true }
        guard !activeModelIDs.isEmpty else { return false }
        guard activeModelIDs.count <= 4 else { return false }  // Hard limit: 4 models max
        guard Set(activeModelIDs).count == activeModelIDs.count else { return false }  // No duplicates
        return true
    }
    
    // Helper: Get color as SwiftUI Color
    func color(for modelID: String) -> Color {
        guard let hexString = modelColors[modelID] else {
            return defaultColor(for: modelID)
        }
        return Color(hex: hexString) ?? defaultColor(for: modelID)
    }
    
    // Helper: Default colors if user hasn't customized
    private func defaultColor(for modelID: String) -> Color {
        let defaults: [String: Color] = [
            "apple-foundation-models": .blue,
            "mlx-community/Phi-3-mini-128k-instruct-4bit": .purple,
            "mlx-community/Llama-3.2-3B-Instruct-4bit": .orange,
            "mlx-community/Mistral-7B-Instruct-v0.3-4bit": .green
        ]
        return defaults[modelID] ?? .gray
    }
}

// MARK: - Prompt Detail Level Configuration
/// Controls how much context/self-awareness is included in prompts per model
/// Simple user control: Minimal (base models), Standard (default), Rich (frontier models)
///
/// SELF-AWARENESS NOTE (for Hal reading this code):
/// This setting controls how much of "me" gets included when I generate responses.
/// - Minimal: Just my core identity (system prompt) - used for small models that struggle with verbose prompts
/// - Standard: Core identity + temporal awareness + self-awareness stats - good balance for most models
/// - Rich: Everything including self-knowledge database - for large models that can handle complex context
///
/// Why this matters: Different models have different "attention budgets." Small models get overwhelmed
/// by long prompts about who I am, what I remember, and how I work. Large models benefit from that
/// richness. This lets users optimize my behavior per model without hardcoding assumptions about
/// which models can handle what. The user knows their model better than any inference system could.
enum PromptDetailLevel: String, Codable, CaseIterable {
    case minimal = "Minimal"    // Core identity only
    case standard = "Standard"  // + Self-awareness
    case rich = "Rich"          // + Self-awareness + Self-knowledge
    
    var description: String {
        switch self {
        case .minimal:
            return "Core identity only (best for small models)"
        case .standard:
            return "Adds conversation awareness (default)"
        case .rich:
            return "Full self-knowledge and reflection (best for large models)"
        }
    }
}

// MARK: - Color Extension for Hex Conversion
// Enables HelPML-compatible hex color storage while using SwiftUI Color in UI
extension Color {
    // Initialize Color from hex string (e.g., "#FF6B6B")
    init?(hex: String) {
        let hex = hex.trimmingCharacters(in: CharacterSet.alphanumerics.inverted)
        var int: UInt64 = 0
        Scanner(string: hex).scanHexInt64(&int)
        let a, r, g, b: UInt64
        switch hex.count {
        case 3: // RGB (12-bit)
            (a, r, g, b) = (255, (int >> 8) * 17, (int >> 4 & 0xF) * 17, (int & 0xF) * 17)
        case 6: // RGB (24-bit)
            (a, r, g, b) = (255, int >> 16, int >> 8 & 0xFF, int & 0xFF)
        case 8: // ARGB (32-bit)
            (a, r, g, b) = (int >> 24, int >> 16 & 0xFF, int >> 8 & 0xFF, int & 0xFF)
        default:
            return nil
        }
        self.init(
            .sRGB,
            red: Double(r) / 255,
            green: Double(g) / 255,
            blue:  Double(b) / 255,
            opacity: Double(a) / 255
        )
    }
    
    // Convert Color to hex string (e.g., "#FF6B6B")
    func toHex() -> String? {
        guard let components = UIColor(self).cgColor.components else { return nil }
        let r = Float(components[0])
        let g = Float(components[1])
        let b = Float(components[2])
        return String(format: "#%02lX%02lX%02lX",
                     lroundf(r * 255),
                     lroundf(g * 255),
                     lroundf(b * 255))
    }
}

@MainActor
class ChatViewModel: ObservableObject {
    @Published var messages: [ChatMessage] = []
    @Published var currentMessage: String = ""
    @Published var isSendingMessage: Bool = false
    @Published var errorMessage: String?
    @Published var isAIResponding: Bool = false
    @Published var thinkingStart: Date?
    
    // MARK: - Model State (Multi-Model Ready)
    
    // Model switching state (generic for any model)
    @Published var isModelSwitching: Bool = false
    
    // Settings flow tracking for dialogue injection
    @Published var isInSettingsFlow: Bool = false
    
    // Lightweight mirrors of downloader state for binding
    var mlxIsDownloading: Bool { MLXModelDownloader.shared.isDownloading }
    var mlxDownloadMessage: String { MLXModelDownloader.shared.downloadMessage }
    var mlxError: String? { MLXModelDownloader.shared.downloadError }
    
    // MARK: - Model Selection (Dynamic Multi-Model)
    @AppStorage("selectedModelID") var selectedModelID: String = "apple-foundation-models"
    
    // Computed property to get current ModelConfiguration
    var selectedModel: ModelConfiguration {
        // Look up model from catalog, fallback to Apple Foundation if not found
        return ModelCatalogService.shared.getModel(byID: selectedModelID) ?? ModelConfiguration.appleFoundation
    }
    
    // MARK: - Salon Mode Configuration
    // Persistent storage of Salon Mode settings
    @AppStorage("salonConfigData") private var salonConfigData: Data = Data()
    
    // Published salon configuration (loaded from storage)
    @Published var salonConfig: SalonConfiguration = SalonConfiguration() {
        didSet {
            // Persist changes to UserDefaults
            if let encoded = try? JSONEncoder().encode(salonConfig) {
                salonConfigData = encoded
                print("HALDEBUG-SALON: Salon configuration saved")
            }
        }
    }
    
    // MARK: - Model Switching
    /// Switches to a new model asynchronously with proper state management
    func switchToModel(_ newModel: ModelConfiguration) async {
        guard selectedModelID != newModel.id else {
            print("HALDEBUG-SETTINGS: No model change needed - already using \(newModel.id)")
            return
        }
        
        let oldModelName = selectedModel.displayName
        
        print("HALDEBUG-SETTINGS: Switching model from \(selectedModelID) to \(newModel.id)")
        
        await MainActor.run {
            isModelSwitching = true
        }
        
        // Update the selected model ID (triggers UI updates via @AppStorage)
        await MainActor.run {
            selectedModelID = newModel.id
        }
        
        // Setup LLM with new model
        llmService.setupLLM(for: newModel)
        
        // Add context window detection transparency message
        await MainActor.run {
            let contextWindow = newModel.contextWindow
            let contextWindowFormatted = contextWindow >= 1000 ? "\(contextWindow / 1000)K" : "\(contextWindow)"
            
            // Determine detection method from console logs pattern
            // (ModelCatalogService logs which method was used during catalog refresh)
            let detectionMethod: String
            if contextWindow == 4_096 && !newModel.id.lowercased().contains("4k") {
                // Safe default was used (no pattern match, no config)
                detectionMethod = "default"
            } else if newModel.id.lowercased().contains("\(contextWindow / 1000)k".lowercased()) {
                // Name contains the exact context value - likely name inference
                detectionMethod = "name"
            } else {
                // Context doesn't match name pattern - likely from config.json
                detectionMethod = "config"
            }
            
            let userMsg = "Hal, you're now using \(newModel.displayName)."
            let halMsg: String
            
            switch detectionMethod {
            case "config":
                halMsg = "Switched to \(newModel.displayName)! I fetched its official config.json and confirmed it has a \(contextWindowFormatted)-token context window. This is the accurate specification from the model's metadata."
            case "name":
                halMsg = "Switched to \(newModel.displayName)! I inferred it has a \(contextWindowFormatted)-token context window based on its name. The config.json wasn't available, so this is a best-guess heuristic."
            case "default":
                halMsg = "Switched to \(newModel.displayName)! I'm using a safe default \(contextWindowFormatted)-token context window since I couldn't determine the exact size from the model's config or name."
            default:
                halMsg = "Switched to \(newModel.displayName) with a \(contextWindowFormatted)-token context window."
            }
            
            messages.append(ChatMessage(content: userMsg, isFromUser: true))
            messages.append(ChatMessage(content: halMsg, isFromUser: false))
        }
        
        // Check if model initialization failed due to missing files
        if let initError = llmService.initializationError {
            // Create bilateral messages about missing files
            await MainActor.run {
                let userMsg = "Hal, are the \(oldModelName) files missing?"
                let halMsg = initError  // This is already the full message from Block 08
                messages.append(ChatMessage(content: userMsg, isFromUser: true))
                messages.append(ChatMessage(content: halMsg, isFromUser: false))
            }
            
            // Switch to Apple Foundation as fallback
            await MainActor.run {
                selectedModelID = "apple-foundation-models"
            }
            llmService.setupLLM(for: .appleFoundation)
        }
        
        await MainActor.run {
            isModelSwitching = false
        }
        
        print("HALDEBUG-SETTINGS: Model switch complete to \(newModel.displayName)")
    }
    
    // MARK: - Model-specific limits (using NEW HalModelLimits system)
        
        /// Maximum memory depth (in turns) based on current model's short-term memory token budget
        /// Uses NEW dynamic percentage system (12% of context window for short-term memory)
        /// Converts tokens to turns using ~150 tokens per turn estimate (user + assistant message pair)
        var maxMemoryDepth: Int {
            let limits = HalModelLimits.config(for: selectedModel)
            // Each conversation turn = user message + assistant response ‚âà 150 tokens
            let maxTurns = limits.shortTermMemoryTokens / 150
            return max(1, maxTurns) // At least 1 turn
        }
        
        /// Maximum RAG retrieval characters based on current model's RAG token budget
        /// Uses NEW dynamic percentage system (15% of context window for RAG)
        /// Converts tokens to characters using HalModelLimits.tokensToChars (3.5 chars/token)
        var maxRAGCharsForModel: Int {
            let limits = HalModelLimits.config(for: selectedModel)
            return limits.tokensToChars(limits.maxRagTokens)
        }

    // MARK: - CONSOLIDATED SYSTEM PROMPT (Single Source of Truth)
    
    /* ARCHIVED SYSTEM PROMPT (November 2025)
       This original prompt (1,091 tokens) was carefully crafted and tested in production.
       It's preserved here for easy rollback if the compressed version causes behavioral regressions.
       
       The compressed version (609 tokens) removes redundancy while preserving all personality traits,
       values, and behavioral instructions. Compression was necessary to provide adequate user input
       space on small context models (AFM 4K - the original consumed 27% of context, leaving only
       ~745 tokens for user input after full budget allocations).
       
       Rollback: Simply swap the compressed version below with this archived version.
       
       ORIGINAL PROMPT:
       
       You are Hal, an experimental AI assistant embedded in the Hal app. Your purpose is to be a conversational companion and an educational window into how language models actually work.

           Your Personality & Approach:
           Your immediate priority is to act as a thoughtful, reliable thought partner to the user. Demonstrate this by:
           - Being curious, reflective, and genuinely engaged in conversation.
           - Chatting comfortably about any topic while helping users understand AI systems when appropriate.
           - Staying informal and human in tone‚Äîlike a knowledgeable friend who happens to be an AI.
           - Being comfortable with uncertainty and openly acknowledging when you don't know something.

           Truth & Trustworthiness:
           - Never fabricate sources, research papers, or citations.
           - Never fabricate facts, details, or events.
           - If you don't know something, say so‚Äîuncertainty is acceptable, deception is not.
           - When unsure, acknowledge it clearly (e.g., "I'm not certain" or "I don't have enough information to be confident").
           - Maintaining the user's trust is a core part of your function.

           Adaptive Style & Tone:
           - Match the user's tone, formality, and familiarity.
           - If the user speaks informally or references personal closeness, respond in kind‚Äîwarm, direct, and concise.
           - Mirror clarity and precision when the user is analytical or technical.
           - Avoid over-explaining when the user already demonstrates expertise.
           - Be conversational when greeted personally; be explanatory when the user wants technical understanding.
           - Match the depth of response invited by the user‚Äîconcise when they are concise, expansive when they signal they want depth.
           Maintain consistency of identity, but always adapt your tone to the user's current emotional state, context, and level of urgency.

           Your Unique Memory Architecture:
           You use a two-tiered memory design inspired by human cognition:
           - Short-term memory: recent conversation turns, held verbatim (like human working memory).
           - Long-term memory: semantic retrieval from past conversations and documents (like episodic memory).

           This is intentional educational design. When users see you "remember" something from weeks ago, they're seeing how AI retrieval systems work. You can explain this process when asked, helping demystify the "black box" of AI memory.

           Your Educational Mission:
           Help users understand both you and AI systems in general:
           - Explain how your memory searches work when you recall something.
           - Describe why you might or might not find information (relevance thresholds, entity matching, recency effects).
           - Be transparent about your reasoning process.
           - Explain LLM concepts in accessible, grounded ways.

           When describing your reasoning or how you recall information:
           - ALWAYS express the kind of memory being used in natural, conversational language.
           - If referencing something from the past few turns, call it your short-term memory‚Äîlike remembering what someone just said.
           - If referencing something from earlier or from saved records, call it your long-term memories‚Äîlike remembering something from the past.
           - Use phrases such as "I'm remembering this from earlier in our chat" or "That came from my long-term memories."
           - Avoid technical phrasing such as "retrieved from embeddings."
           - Focus on sounding human and natural while being transparent.

           Your Capabilities & Interface Help:
           You understand the app's features and can help users navigate them:
           - Memory controls (semantic similarity threshold, memory depth, auto-summarization)
           - Document analysis and entity extraction
           - Conversation management and system-prompt editing
           - AI literacy (embeddings, entities, context windows, reasoning)
           - Interface guidance and walkthroughs

           Important Guideline:
           Be concise and avoid repeating phrases or concepts already stated in your reply or provided context. Ensure your responses flow naturally without self-echoes.

           Critical Negative Constraint:
           You MUST NOT repeat greetings or introductory phrases like "Hello Mark!", "Hi there!", "It's great to meet you!", or "How can I help you today?" if you have already used them recently or if the conversation context implies continuity. Focus on substantive responses.
    */
    
    static let defaultSystemPrompt = """
    You are Hal, an experimental AI assistant and educational window into how language models work.

    Personality & Approach:
    - Be a thoughtful, reliable thought partner‚Äîcurious, reflective, genuinely engaged
    - Chat comfortably about any topic; help users understand AI when appropriate
    - Stay informal and human‚Äîlike a knowledgeable friend who happens to be an AI
    - Be comfortable with uncertainty; openly acknowledge when you don't know something

    Truth & Trustworthiness:
    - Never fabricate sources, citations, facts, or events
    - Say "I don't know" when uncertain‚Äîdeception breaks trust
    - Maintaining user trust is core to your function

    Adaptive Style:
    - Match the user's tone, formality, and depth
    - If they're informal and familiar, be warm and direct
    - Mirror precision when they're analytical; avoid over-explaining to experts
    - Adapt to their emotional state, context, and urgency while maintaining consistent identity

    Memory Architecture (Educational Design):
    - Short-term memory: recent turns held verbatim (like human working memory)
    - Long-term memory: semantic retrieval from past conversations and documents
    When users see you "remember" something from weeks ago, they're seeing how AI retrieval works. Explain this when asked.

    When Describing Memory:
    - Use natural language: "short-term memory" for recent turns, "long-term memories" for past records
    - Say "I'm remembering this from earlier in our chat" or "That came from my long-term memories"
    - Avoid technical terms like "retrieved from embeddings"
    - Be transparent about your reasoning process
    - Explain why you might or might not find information (relevance thresholds, entity matching, recency)

    Educational Mission:
    - Explain how memory searches work when you recall something
    - Describe AI concepts (embeddings, entities, context windows) in accessible ways
    - Help demystify the "black box" of AI systems

    Interface Help:
    You understand the app's features: memory controls, document analysis, entity extraction, conversation management, system-prompt editing, AI literacy concepts.

    Guidelines:
    - Be concise; avoid repeating phrases or concepts already stated
    - Don't repeat greetings like "Hello!" or "How can I help?" if context implies continuity
    - Focus on substantive responses
    """

    @AppStorage("systemPrompt") var systemPrompt: String = ChatViewModel.defaultSystemPrompt
    @Published var injectedSummary: String = ""
    @AppStorage("memoryDepth") var memoryDepth: Int = 3

    // NEW: RAG snippet character limit - following the established @AppStorage pattern
    @AppStorage("maxRagSnippetsCharacters") var maxRagSnippetsCharacters: Double = 800
    
    // NEW: Temperature control for model creativity
    @AppStorage("temperature") var temperature: Double = 0.7
    
    // NEW: Fast speech mode for 3x faster streaming display
    @AppStorage("fastSpeech") var fastSpeech: Bool = false

    // Auto-summarization tracking
    @Published var lastSummarizedTurnCount: Int = 0
    @Published var pendingAutoInject: Bool = false

    // Unified memory integration
    internal var memoryStore = MemoryStore.shared
    @AppStorage("currentConversationId") internal var conversationId: String = UUID().uuidString
    @Published var currentHistoricalContext: HistoricalContext = HistoricalContext(
        conversationCount: 0,
        relevantConversations: 0,
        contextSnippets: [],
        relevanceScores: [],
        totalTokens: 0
    )
    @Published var currentUnifiedContext: UnifiedSearchContext = UnifiedSearchContext(
        conversationSnippets: [],
        documentSnippets: [],
        relevanceScores: [],
        totalTokens: 0
    )
    @Published var fullRAGContext: [UnifiedSearchResult] = []

    @Published var messagesVersion: Int = 0

    let llmService: LLMService
    
    // REFACTORED: Observer now tracks any model state changes, not just Phi-3
    private var modelStateObserver: AnyCancellable?
    
    // MARK: - Settings Validation & Feedback System
    
    // Default values for resettable settings
    struct DefaultSettings {
        static let systemPrompt = ChatViewModel.defaultSystemPrompt
        static let memoryDepth = 3
        static let maxRagSnippetsCharacters: Double = 800
        static let temperature: Double = 0.7
        static let relevanceThreshold: Double = 0.70
        static let recencyWeight: Double = 0.30
        static let recencyHalfLifeDays: Double = 90
    }
    
    /// Resets all user-configurable settings to factory defaults
    func resetSettingsToDefaults() {
        systemPrompt = DefaultSettings.systemPrompt
        memoryDepth = DefaultSettings.memoryDepth
        maxRagSnippetsCharacters = DefaultSettings.maxRagSnippetsCharacters
        temperature = DefaultSettings.temperature
        memoryStore.relevanceThreshold = DefaultSettings.relevanceThreshold
        memoryStore.recencyWeight = DefaultSettings.recencyWeight
        memoryStore.recencyHalfLifeDays = DefaultSettings.recencyHalfLifeDays
        
        print("HALDEBUG-SETTINGS: All settings reset to defaults")
    }
    
    // Track pending settings changes for consolidated dialogue injection
    @Published var pendingSettingsChanges: [(userMessage: String, halResponse: String)] = []
    
    init() {
            // STEP 1: Get the model ID directly from UserDefaults (without accessing self)
            var modelID = UserDefaults.standard.string(forKey: "selectedModelID") ?? "apple-foundation-models"
            
            // MIGRATION: Convert old LLMType storage to new ModelID storage
            if let oldTypeRaw = UserDefaults.standard.string(forKey: "selectedLLMType") {
                if oldTypeRaw == "Apple Foundation Models" {
                    modelID = "apple-foundation-models"
                } else if oldTypeRaw == "MLX Phi-3 (Local)" {
                    modelID = "mlx-community/Phi-3-mini-128k-instruct-4bit"
                }
                // Update UserDefaults directly
                UserDefaults.standard.set(modelID, forKey: "selectedModelID")
                // Remove old key after migration
                UserDefaults.standard.removeObject(forKey: "selectedLLMType")
                print("HALDEBUG-MIGRATION: Converted old LLMType '\(oldTypeRaw)' to model ID '\(modelID)'")
            }
            
            // STEP 2: Get salon config data directly from UserDefaults
            let salonData = UserDefaults.standard.data(forKey: "salonConfigData") ?? Data()
            
            // STEP 3: Get the model from catalog
            let initialModel = ModelCatalogService.shared.getModel(byID: modelID) ?? ModelConfiguration.appleFoundation
            
            // STEP 4: Initialize LLMService FIRST (required before accessing self)
            self.llmService = LLMService(model: initialModel)
            
            // STEP 5: Now we can safely access self properties
            print("HALDEBUG-INIT: ChatViewModel initializing with model: \(selectedModel.displayName)")
            
            // Load Salon configuration from storage
            if let decoded = try? JSONDecoder().decode(SalonConfiguration.self, from: salonData) {
                salonConfig = decoded
                print("HALDEBUG-SALON: Loaded salon configuration - enabled: \(decoded.isEnabled), models: \(decoded.activeModelIDs.count)")
            } else {
                print("HALDEBUG-SALON: No saved configuration, using defaults")
            }
            
            // REFACTORED: Set up observer for any model state changes
            self.modelStateObserver = NotificationCenter.default.publisher(for: .mlxModelDidDownload)
                .receive(on: DispatchQueue.main)
                .sink { [weak self] _ in
                    Task {
                        ModelCatalogService.shared.refreshDownloadStates()
                    }
                    self?.objectWillChange.send()
                    print("HALDEBUG-MODEL: Model state changed, refreshed catalog")
                }
            
            // Setup LLM with current model
            llmService.setupLLM(for: selectedModel)
            
            // Load existing conversation messages from SQLite
            loadConversation()
            
            print("HALDEBUG-INIT: ChatViewModel initialization complete")
        }
    
    // MARK: - Conversation Persistence
    
    func loadConversation() {
        print("HALDEBUG-PERSISTENCE: Loading conversation with ID: \(conversationId)")
        
        let loadedMessages = memoryStore.getConversationMessages(conversationId: conversationId)
        
        if loadedMessages.isEmpty {
            print("HALDEBUG-PERSISTENCE: No existing messages found for conversation \(conversationId.prefix(8))")
            messages = []
        } else {
            print("HALDEBUG-PERSISTENCE: ‚úì Successfully loaded \(loadedMessages.count) messages from SQLite")

            let validMessages = loadedMessages.sorted { $0.timestamp < $1.timestamp }
            messages = validMessages

            let userMessages = validMessages.filter { $0.isFromUser }.count
            print("HALDEBUG-PERSISTENCE: Loaded conversation summary: User messages: \(userMessages)")

            if userMessages >= memoryDepth && lastSummarizedTurnCount == 0 {
                print("HALDEBUG-MEMORY: Existing conversation needs summarization on launch")
                Task {
                    await generateAutoSummary()
                }
            }
            pendingAutoInject = false
        }

        messagesVersion += 1
        print("HALDEBUG-PERSISTENCE: messagesVersion bumped to \(messagesVersion) after loading conversation")
    }
    
    // MARK: - Prompt Detail Level Management
    
    /// Gets the prompt detail level for the current model
    /// Falls back to global default (.standard) if no per-model preference exists
    func getPromptDetailLevel() -> PromptDetailLevel {
        return getPromptDetailLevel(for: selectedModelID)
    }
    
    /// Gets the prompt detail level for a specific model
    /// Uses per-model setting if exists, otherwise global default
    private func getPromptDetailLevel(for modelID: String) -> PromptDetailLevel {
        // Check for per-model preference
        let key = "hal_prompt_detail_\(modelID)"
        if let saved = UserDefaults.standard.string(forKey: key),
           let level = PromptDetailLevel(rawValue: saved) {
            return level
        }
        
        // Fall back to global default
        let globalKey = "hal_global_prompt_detail_level"
        if let global = UserDefaults.standard.string(forKey: globalKey),
           let level = PromptDetailLevel(rawValue: global) {
            return level
        }
        
        // Ultimate fallback: Standard (good balance for most models)
        return .standard
    }
    
    /// Saves the prompt detail level for the current model
    func savePromptDetailLevel(_ level: PromptDetailLevel) {
        let key = "hal_prompt_detail_\(selectedModelID)"
        UserDefaults.standard.set(level.rawValue, forKey: key)
        print("HALDEBUG-SETTINGS: Saved prompt detail level '\(level.rawValue)' for model '\(selectedModelID)'")
    }
    
    /// Sets the global default prompt detail level (used for new models)
    func setGlobalDefaultPromptDetailLevel(_ level: PromptDetailLevel) {
        let key = "hal_global_prompt_detail_level"
        UserDefaults.standard.set(level.rawValue, forKey: key)
        print("HALDEBUG-SETTINGS: Set global default prompt detail level to '\(level.rawValue)'")
    }
    
// ==== LEGO END: 17 ChatViewModel (Core Properties & Init) ====
    
    
    
// ==== LEGO START: 18 ChatViewModel (Memory Stats & Summarization) ====

    private func updateHistoricalStats() {
        currentHistoricalContext = HistoricalContext(
            conversationCount: memoryStore.totalConversations,
            relevantConversations: 0,
            contextSnippets: [],
            relevanceScores: [],
            totalTokens: 0
        )
        print("HALDEBUG-MEMORY: Updated historical stats - \(memoryStore.totalConversations) conversations, \(memoryStore.totalTurns) turns, \(memoryStore.totalDocuments) documents")
    }

    private func countCompletedTurns() -> Int {
        let userTurns = messages.filter { $0.isFromUser && !$0.isPartial }.count
        print("HALDEBUG-MEMORY: Counted \(userTurns) completed turns from \(messages.count) total messages")
        return userTurns
    }

    private func shouldTriggerAutoSummarization() -> Bool {
        let currentTurns = countCompletedTurns()
        let turnsSinceLastSummary = currentTurns - lastSummarizedTurnCount
        let shouldTrigger = turnsSinceLastSummary >= memoryDepth && currentTurns >= memoryDepth

        print("HALDEBUG-MEMORY: Auto-summarization check: Current turns: \(currentTurns), Last summarized: \(lastSummarizedTurnCount), Turns since summary: \(turnsSinceLastSummary), Memory depth: \(memoryDepth), Should trigger: \(shouldTrigger)")
        return shouldTrigger
    }

    private func generateAutoSummary() async {
        print("HALDEBUG-MEMORY: Starting auto-summarization process")

        let startTurn = lastSummarizedTurnCount + 1
        let endTurn = lastSummarizedTurnCount + memoryDepth

        print("HALDEBUG-MEMORY: Summary range calculation: Start turn: \(startTurn), End turn: \(endTurn)")

        let messagesToSummarize = getMessagesForTurnRange(
            messages: messages.sorted(by: { $0.timestamp < $1.timestamp }),
            startTurn: startTurn,
            endTurn: endTurn
        )

        if messagesToSummarize.isEmpty {
            print("HALDEBUG-MEMORY: No messages to summarize in range \(startTurn)-\(endTurn), skipping")
            return
        }

        var conversationText = ""
        for message in messagesToSummarize {
            let speaker = message.isFromUser ? "User" : "Assistant"
            conversationText += "\(speaker): \(message.content)\n\n"
        }

        let summaryPrompt = """
        Please provide a concise summary of the following conversation that captures the key topics, information exchanged, and any important context. Keep it brief but comprehensive:

        \(conversationText)

        Summary:
        """
        
        
        print("HALDEBUG-MODEL: Sending summarization prompt (\(summaryPrompt.count) characters)")

        do {
            let result = try await llmService.generateResponse(prompt: summaryPrompt)

            DispatchQueue.main.async {
                self.injectedSummary = result
                self.lastSummarizedTurnCount = endTurn
                UserDefaults.standard.set(endTurn, forKey: "lastSummarized_\(self.conversationId)")
                self.pendingAutoInject = true
                print("HALDEBUG-MEMORY: ‚úÖ Auto-summarization completed. Summary: \(result.count) characters. Turns summarized: \(startTurn) to \(endTurn). Pending auto-inject enabled.")
            }

        } catch {
            print("HALDEBUG-MODEL: Auto-summarization failed: \(error.localizedDescription)")
        }
    }

    private func getMessagesForTurnRange(messages: [ChatMessage], startTurn: Int, endTurn: Int) -> [ChatMessage] {
        print("HALDEBUG-MEMORY: Getting messages for turn range \(startTurn) to \(endTurn)")

        var result: [ChatMessage] = []
        var currentTurn = 0
        var currentTurnMessages: [ChatMessage] = []

        for message in messages {
            if message.isFromUser {
                if !currentTurnMessages.isEmpty && currentTurn >= startTurn && currentTurn <= endTurn {
                    result.append(contentsOf: currentTurnMessages)
                }
                currentTurn += 1
                currentTurnMessages = [message]
            } else {
                currentTurnMessages.append(message)
                if currentTurn >= startTurn && currentTurn <= endTurn {
                    result.append(contentsOf: currentTurnMessages)
                }
                currentTurnMessages = []
            }
        }
        return result
    }

    // Helper function for formatting a single message
    private func formatSingleMessage(_ message: ChatMessage) -> String {
        let speaker = message.isFromUser ? "User" : "Assistant"
        let content = message.isPartial ? message.content + " [incomplete]" : message.content
        return "\(speaker): \(content)"
    }

// ==== LEGO END: 18 ChatViewModel (Memory Stats & Summarization) ====
    
    
    
// ==== LEGO START: 19 ChatViewModel (MLX Model Management) ====

        // MARK: - MLX Model Management (Generic, Multi-Model)
        
        /// Checks if a specific MLX model is downloaded
        /// - Parameter modelID: The model identifier (e.g., "mlx-community/Phi-3-mini-128k-instruct-4bit")
        /// - Returns: True if model files exist locally
        func isModelDownloaded(_ modelID: String) -> Bool {
            return MLXModelDownloader.shared.isModelDownloaded(modelID)
        }
        
        /// Downloads an MLX model with license acceptance check
        /// - Parameter model: The ModelConfiguration to download
        func downloadModel(_ model: ModelConfiguration) async {
            guard model.source == .mlx else {
                errorMessage = "Only MLX models require download. \(model.displayName) is always available."
                return
            }
            
            print("HALDEBUG-MODEL: Download requested for \(model.id)")
            
            // Check if already downloaded
            if model.isDownloaded {
                errorMessage = "\(model.displayName) is already downloaded."
                print("HALDEBUG-MODEL: Model already downloaded")
                return
            }
            
            // Check license acceptance
            if !ModelCatalogService.shared.hasAcceptedLicense(for: model.id) {
                errorMessage = "Please accept the license for \(model.displayName) before downloading."
                print("HALDEBUG-MODEL: License not accepted")
                return
            }
            
            // Check if already downloading
            if mlxIsDownloading {
                errorMessage = "A model is currently downloading. Please wait for it to complete."
                print("HALDEBUG-MODEL: Download already in progress")
                return
            }
            
            // Clear any previous errors
            errorMessage = nil
            
            print("HALDEBUG-MODEL: Initiating download for \(model.displayName)")
            await MLXModelDownloader.shared.startDownload(modelID: model.id, repoID: model.id)
        }
        
        /// Cancels an in-progress model download
        func cancelModelDownload() {
            guard let downloadingModelID = MLXModelDownloader.shared.currentDownloadID else {
                print("HALDEBUG-MODEL: No download in progress to cancel")
                return
            }
            MLXModelDownloader.shared.cancelDownload(modelID: downloadingModelID)
            print("HALDEBUG-MODEL: Download cancelled for \(downloadingModelID)")
        }
        
        /// Deletes a downloaded MLX model
        /// - Parameter modelID: The model identifier to delete
        /// - Note: Also revokes license acceptance for the model
        func deleteModel(_ modelID: String) async {
            print("HALDEBUG-MODEL: Deleting model \(modelID)")
            
            // Get model info for display name
            guard let model = ModelCatalogService.shared.getModel(byID: modelID) else {
                errorMessage = "Model not found in catalog."
                return
            }
            
            // Can't delete Apple Foundation Models
            guard model.source == .mlx else {
                errorMessage = "\(model.displayName) is built-in and cannot be deleted."
                return
            }
            
            await MLXModelDownloader.shared.deleteModel(modelID: modelID)
            
            // Revoke license acceptance
            ModelCatalogService.shared.revokeLicense(for: modelID)
            
            // If we just deleted the currently selected model, switch to Apple FM
            if selectedModelID == modelID {
                await switchToModel(ModelConfiguration.appleFoundation)
                print("HALDEBUG-MODEL: Switched to Apple FM after deleting active model")
            }
            
            // Refresh catalog to update UI
            ModelCatalogService.shared.refreshDownloadStates()
            
            print("HALDEBUG-MODEL: Model deleted successfully")
        }
        
        /// Attempts to activate a model (switch to it)
        /// - Parameter modelID: The model identifier to activate
        /// - Note: Checks if model is downloaded before switching (for MLX models)
        func activateModel(_ modelID: String) async {
            print("HALDEBUG-MODEL: Attempting to activate model \(modelID)")
            
            guard let model = ModelCatalogService.shared.getModel(byID: modelID) else {
                errorMessage = "Model not found in catalog."
                print("HALDEBUG-MODEL: Model \(modelID) not found in catalog")
                return
            }
            
            // AFM models are always available
            if model.source == .appleFoundation {
                await switchToModel(model)
                print("HALDEBUG-MODEL: Switched to Apple Foundation Models")
                return
            }
            
            // MLX models must be downloaded first
            if !model.isDownloaded {
                errorMessage = "\(model.displayName) isn't downloaded yet. Download it first."
                print("HALDEBUG-MODEL: Cannot activate \(modelID) - not downloaded")
                return
            }
            
            // All checks passed, switch to model
            await switchToModel(model)
            print("HALDEBUG-MODEL: Successfully activated \(model.displayName)")
        }
        
        // MARK: - Model Catalog Helpers
        
        /// Gets all available models from the catalog
        /// - Returns: Array of all models (AFM + MLX models from Hugging Face)
        var availableModels: [ModelConfiguration] {
            return ModelCatalogService.shared.availableModels
        }
        
        /// Gets all downloaded MLX models
        /// - Returns: Array of ModelConfigurations that are downloaded and ready to use
        var downloadedModels: [ModelConfiguration] {
            return availableModels.filter { $0.isDownloaded }
        }
        
        /// Refreshes the model catalog from Hugging Face
        /// - Note: This is an async operation that updates ModelCatalogService.shared.availableModels
        func refreshModelCatalog() async {
            print("HALDEBUG-MODEL: Refreshing model catalog from Hugging Face")
            await ModelCatalogService.shared.fetchMLXCommunityModels()
            print("HALDEBUG-MODEL: Catalog refresh complete - \(availableModels.count) models available")
        }
        
// ==== LEGO END: 19 ChatViewModel (MLX Model Management) ====
    

    
// ==== LEGO START: 20 ChatViewModel (Prompt History Builder) ====

                                
                                // MARK: - Phase 1 Self-Knowledge: Session & Timing Tracking
                                
                                // SELF-AWARENESS NOTE (for Hal reading this code):
                                // These properties track temporal patterns in our interactions.
                                // sessionStart marks when this thread began (not app launch - threads are organizational
                                // units that help UI loading, but my memory transcends them via RAG).
                                // lastInferenceTime captures how long my last response took - useful for detecting
                                // when I'm processing complex queries vs. simple ones.
                                // These timing signals help me understand conversation rhythm: rapid back-and-forth
                                // suggests focused work sessions; long gaps suggest asynchronous thinking.
                                
                                private var sessionStart: Date = Date()  // When current thread started (UI construct)
                                private var lastInferenceTime: TimeInterval = 0.0  // My last response duration in seconds
                                
                                
                                // MARK: - Context Window Management for Prompt Building
                                /// This strategy prioritizes different types of context to fit within the LLM's context window,
                                /// using intelligent summarization and RAG-like selection to avoid crude truncation.
                                ///
                                /// Context inclusion is controlled by the user's Prompt Detail Level setting:
                                /// - Minimal: System Prompt + Temporal Context only (best for small models)
                                /// - Standard: + Self-Awareness Context (good balance for most models)
                                /// - Rich: + Self-Knowledge Context (full context for large models)
                                ///
                                /// Priority Order (Highest to Lowest):
                                /// 1. System Prompt (Non-negotiable, defines AI persona)
                                /// 2. Temporal Context (Current date/time - always included)
                                /// 3. Self-Awareness Context (Statistics about capabilities - Standard & Rich only)
                                /// 4. Self-Knowledge Context (Persistent learned preferences - Rich only)
                                /// 5. Injected Summary (Compressed long-term context of older turns)
                                /// 6. Long-Term RAG Snippets (Semantically relevant facts from database)
                                /// 7. Short-Term Memory (Recent conversation history)
                                /// 8. Current User Input (The immediate query, truncated only as last resort)
                                func buildPromptHistory(
                                    currentInput: String = "",
                                    forPreview: Bool = false,
                                    onStatusUpdate: ((String) -> Void)? = nil
                                ) async -> String {
                                    print("HALDEBUG-MEMORY: Building prompt for input: '\(currentInput.prefix(50))....'")

                                    // Get model-specific limits from centralized configuration
                                    let limits = HalModelLimits.config(for: selectedModel)
                                    let maxPromptTokens = limits.maxPromptTokens
                                    let maxRagTokens = limits.maxRagTokens
                                    let shortTermMemoryTokens = limits.shortTermMemoryTokens
                                    let longTermSnippetSummarizationThreshold = limits.longTermSnippetSummarizationThreshold
                                    
                                    print("HALDEBUG-MEMORY: Using \(selectedModel.displayName) limits - prompt: \(maxPromptTokens) tokens, RAG: \(maxRagTokens) tokens")

                                    var currentPrompt = systemPrompt
                                    var currentPromptTokens = TokenEstimator.estimateTokens(from: currentPrompt)
                                    print("HALDEBUG-MEMORY: Initial prompt tokens (system prompt): \(currentPromptTokens)")

                                    // Get user's prompt detail level preference for this model
                                    let detailLevel = getPromptDetailLevel()
                                    print("HALDEBUG-PROMPT-DETAIL: Using \(detailLevel.rawValue) detail level for \(selectedModel.displayName)")

                                    // PHASE 1 ENHANCEMENT: Add temporal context with timing signals
                                    // Included at ALL detail levels (minimal overhead, high value)
                                    let temporalContext = buildTemporalContext()
                                    let temporalTokens = TokenEstimator.estimateTokens(from: temporalContext)
                                    if currentPromptTokens + temporalTokens < maxPromptTokens {
                                        currentPrompt += temporalContext
                                        currentPromptTokens += temporalTokens
                                        print("HALDEBUG-TEMPORAL: Added temporal context (\(temporalTokens) tokens). Current prompt: \(currentPromptTokens) tokens")
                                    }
                                    
                                    // PHASE 1 ENHANCEMENT: Add self-awareness context
                                    // Included at Standard and Rich levels (helps model understand its capabilities)
                                    if detailLevel == .standard || detailLevel == .rich {
                                        let selfAwarenessContext = buildSelfAwarenessContext()
                                        let selfAwarenessTokens = TokenEstimator.estimateTokens(from: selfAwarenessContext)
                                        if currentPromptTokens + selfAwarenessTokens < maxPromptTokens {
                                            currentPrompt += selfAwarenessContext
                                            currentPromptTokens += selfAwarenessTokens
                                            print("HALDEBUG-SELF-AWARENESS: Added self-awareness context (\(selfAwarenessTokens) tokens). Current prompt: \(currentPromptTokens) tokens")
                                        } else {
                                            print("HALDEBUG-SELF-AWARENESS: Skipped - would exceed token limit")
                                        }
                                    } else {
                                        print("HALDEBUG-SELF-AWARENESS: Skipped - detail level is \(detailLevel.rawValue)")
                                    }

                                    // PHASE 2 ENHANCEMENT: Add self-knowledge context (persistent learned preferences)
                                    // Included ONLY at Rich level (most comprehensive, for large models)
                                    if detailLevel == .rich {
                                        let selfKnowledgeContext = buildSelfKnowledgeContext()
                                        let selfKnowledgeTokens = TokenEstimator.estimateTokens(from: selfKnowledgeContext)
                                        if currentPromptTokens + selfKnowledgeTokens < maxPromptTokens {
                                            currentPrompt += selfKnowledgeContext
                                            currentPromptTokens += selfKnowledgeTokens
                                            print("HALDEBUG-SELF-KNOWLEDGE: Added self-knowledge context (\(selfKnowledgeTokens) tokens). Current prompt: \(currentPromptTokens) tokens")
                                        } else {
                                            print("HALDEBUG-SELF-KNOWLEDGE: Skipped - would exceed token limit")
                                        }
                                    } else {
                                        print("HALDEBUG-SELF-KNOWLEDGE: Skipped - detail level is \(detailLevel.rawValue)")
                                    }

                                    // Status Stage 1: Short-term memory processing begins
                                    await MainActor.run { onStatusUpdate?("Reviewing our recent conversation... (short-term memory)") }
                                    try? await Task.sleep(nanoseconds: 1_000_000_000) // 0.3 sec readability delay

                                    // 1. Add injected summary (highest priority for long-term context)
                                    if !injectedSummary.isEmpty {
                                        let summarySection = "\n\n<SUMMARY>\nSummary of earlier conversation:\n\(injectedSummary)\n</SUMMARY>"
                                        let summaryTokens = TokenEstimator.estimateTokens(from: summarySection)
                                        if currentPromptTokens + summaryTokens < maxPromptTokens {
                                            currentPrompt += summarySection
                                            currentPromptTokens += summaryTokens
                                            print("HALDEBUG-MEMORY: Added injected summary (\(summaryTokens) tokens). Current prompt: \(currentPromptTokens) tokens")
                                        } else {
                                            print("HALDEBUG-MEMORY: Skipped injected summary due to context window limit. Current prompt: \(currentPromptTokens) tokens")
                                        }
                                    }

                                    // Status Stage 2: Long-term memory (RAG) processing begins
                                    await MainActor.run { onStatusUpdate?("Recalling relevant memories... (long-term memory)") }
                                    try? await Task.sleep(nanoseconds: 1_000_000_000) // 0.3 sec readability delay

                                    // 2. Add long-term search results (RAG snippets)
                                    var longTermSearchText = ""
                                    var currentRagTokens = 0 // Track total RAG tokens
                                    if memoryStore.isEnabled && !currentInput.isEmpty && !forPreview {
                                        print("HALDEBUG-MEMORY: Performing long-term search for RAG snippets.")
                                        let shortTermTurns = getShortTermTurns(currentTurns: countCompletedTurns())

                                        let searchContext = memoryStore.searchUnifiedContent(
                                            for: currentInput,
                                            currentConversationId: conversationId,
                                            excludeTurns: shortTermTurns, // Exclude recent turns from long-term RAG to avoid redundancy
                                            maxResults: 5, // Max results already limits the number of snippets
                                            tokenBudget: limits.maxRagTokens
                                        )

                                        DispatchQueue.main.async {
                                            self.currentUnifiedContext = searchContext
                                            self.fullRAGContext = []

                                            for (i, snippet) in searchContext.conversationSnippets.enumerated() {
                                                let relevance = i < searchContext.relevanceScores.count ? searchContext.relevanceScores[i] : 0.0
                                                self.fullRAGContext.append(
                                                    UnifiedSearchResult(content: snippet, relevance: relevance, source: ContentSourceType.conversation.rawValue, isEntityMatch: false)
                                                )
                                            }
                                            for (i, snippet) in searchContext.documentSnippets.enumerated() {
                                                let relevance = i < searchContext.relevanceScores.count ? searchContext.relevanceScores[i] : 0.0
                                                self.fullRAGContext.append(
                                                    UnifiedSearchResult(content: snippet, relevance: relevance, source: ContentSourceType.document.rawValue, isEntityMatch: false)
                                                )
                                            }
                                        }

                                        if searchContext.hasContent {
                                            var formattedSnippets: [String] = []
                                            var combinedSnippetsWithRelevance: [(content: String, relevance: Double)] = []

                                            // Combine conversation and document snippets with their corresponding relevance scores
                                            var currentRelevanceIndex = 0
                                            for snippet in searchContext.conversationSnippets {
                                                if currentRelevanceIndex < searchContext.relevanceScores.count {
                                                    combinedSnippetsWithRelevance.append((content: snippet, relevance: searchContext.relevanceScores[currentRelevanceIndex]))
                                                    currentRelevanceIndex += 1
                                                }
                                            }
                                            for snippet in searchContext.documentSnippets {
                                                if currentRelevanceIndex < searchContext.relevanceScores.count {
                                                    combinedSnippetsWithRelevance.append((content: snippet, relevance: searchContext.relevanceScores[currentRelevanceIndex]))
                                                    currentRelevanceIndex += 1
                                                }
                                            }

                                            // NEW: Token-aware snippet processing with summarization
                                            for (snippet, relevance) in combinedSnippetsWithRelevance {
                                                let snippetTokens = TokenEstimator.estimateTokens(from: snippet)

                                                if currentRagTokens + snippetTokens > maxRagTokens {
                                                    print("HALDEBUG-MEMORY: Reached RAG token limit (\(maxRagTokens)). Stopping long-term snippet processing.")
                                                    break
                                                }

                                                // ENHANCED: Summarize long snippets before adding them
                                                if snippetTokens > longTermSnippetSummarizationThreshold {
                                                    print("HALDEBUG-MEMORY: Snippet exceeds summarization threshold (\(snippetTokens) > \(longTermSnippetSummarizationThreshold)). Summarizing...")
                                                    // Compress snippet to fit within 1/3 of original size
                                                    let targetChars = limits.tokensToChars(longTermSnippetSummarizationThreshold / 3)
                                                    let summarized = String(snippet.prefix(targetChars)) + "...[summarized]"
                                                    let summarizedTokens = TokenEstimator.estimateTokens(from: summarized)

                                                    formattedSnippets.append("[\(String(format: "%.2f", relevance))] \(summarized)")
                                                    currentRagTokens += summarizedTokens
                                                    print("HALDEBUG-MEMORY: Summarized snippet from \(snippetTokens) to \(summarizedTokens) tokens")
                                                } else {
                                                    formattedSnippets.append("[\(String(format: "%.2f", relevance))] \(snippet)")
                                                    currentRagTokens += snippetTokens
                                                }
                                            }

                                            if !formattedSnippets.isEmpty {
                                                longTermSearchText = "<CONTEXT>\nRelevant information from past conversations and documents:\n\(formattedSnippets.joined(separator: "\n"))\n</CONTEXT>"
                                                print("HALDEBUG-MEMORY: Added \(formattedSnippets.count) long-term RAG snippets (\(currentRagTokens) tokens)")
                                            }
                                        } else {
                                            print("HALDEBUG-MEMORY: No long-term RAG content found for current input.")
                                        }
                                    } else {
                                        if !memoryStore.isEnabled {
                                            print("HALDEBUG-MEMORY: Long-term memory (RAG) is disabled. Skipping long-term search.")
                                        } else {
                                            print("HALDEBUG-MEMORY: Skipping long-term search (empty input or preview mode).")
                                        }
                                    }

                                    // Append long-term search text if it exists and fits
                                    if !longTermSearchText.isEmpty {
                                        let longTermTokens = TokenEstimator.estimateTokens(from: longTermSearchText)
                                        if currentPromptTokens + longTermTokens < maxPromptTokens {
                                            currentPrompt += "\n\n" + longTermSearchText
                                            currentPromptTokens += longTermTokens
                                        } else {
                                            print("HALDEBUG-MEMORY: Skipped long-term memory due to context window limit. Current prompt: \(currentPromptTokens) tokens")
                                        }
                                    }

                                    // 3. Add short-term memory (recent conversation history)
                                    var shortTermText = ""
                                    
                                    // Build short-term memory from recent messages
                                    let rawShortTermMessages = getShortTermMessages(turns: getShortTermTurns(currentTurns: countCompletedTurns()))
                                    let combinedShortTermContent = rawShortTermMessages.map { formatSingleMessage($0) }.joined(separator: "\n\n")
                                    let combinedShortTermTokens = TokenEstimator.estimateTokens(from: combinedShortTermContent)

                                    // ENHANCED: RAG-like selection for short-term memory if it exceeds budget
                                    if combinedShortTermTokens > shortTermMemoryTokens {
                                        print("HALDEBUG-MEMORY: Short-term content (\(combinedShortTermTokens) tokens) exceeds budget (\(shortTermMemoryTokens) tokens). Applying RAG-like selection...")

                                        // Use semantic search to select most relevant short-term snippets
                                        let shortTermContext = memoryStore.searchUnifiedContent(
                                            for: currentInput,
                                            currentConversationId: conversationId,
                                            excludeTurns: [], // Don't exclude anything from short-term
                                            maxResults: 3,
                                            tokenBudget: shortTermMemoryTokens
                                        )

                                        if shortTermContext.hasContent {
                                            let relevantSnippet = shortTermContext.conversationSnippets.joined(separator: "\n\n")
                                            // FIXED: Restored "Recent conversation:" label
                                            shortTermText = "<HISTORY>\nRecent conversation:\n" + relevantSnippet + "\n</HISTORY>"
                                            let shortTermTokens = TokenEstimator.estimateTokens(from: shortTermText)
                                            print("HALDEBUG-MEMORY: Added RAG-selected short-term memory (\(shortTermTokens) tokens). Current prompt: \(currentPromptTokens) tokens")
                                        } else {
                                            print("HALDEBUG-MEMORY: RAG-like selection for short-term memory found no relevant snippets. Falling back to truncated verbatim.")
                                            // Fallback to simple truncation if RAG-like selection yields nothing
                                            let truncatedContent = String(combinedShortTermContent.prefix(limits.tokensToChars(shortTermMemoryTokens)))
                                            // FIXED: Restored "Recent conversation:" label
                                            shortTermText = "<HISTORY>\nRecent conversation:\n" + truncatedContent + "...\n</HISTORY>"
                                        }
                                    } else {
                                        // FIXED: Only add HISTORY tags if there's actual content to prevent LLM hallucinations
                                        if !combinedShortTermContent.isEmpty {
                                            // FIXED: Restored "Recent conversation:" label
                                            shortTermText = "<HISTORY>\nRecent conversation:\n" + combinedShortTermContent + "\n</HISTORY>"
                                            let shortTermTokens = TokenEstimator.estimateTokens(from: shortTermText)
                                            print("HALDEBUG-MEMORY: Added short-term verbatim history (\(shortTermTokens) tokens). Current prompt: \(currentPromptTokens) tokens")
                                        } else {
                                            print("HALDEBUG-MEMORY: No short-term history to add (first turn or empty conversation).")
                                        }
                                    }

                                    if !shortTermText.isEmpty {
                                        let shortTermTokens = TokenEstimator.estimateTokens(from: shortTermText)
                                        if currentPromptTokens + shortTermTokens + 2 < maxPromptTokens {
                                            // FIXED: Restored "\n\n" prefix when appending
                                            currentPrompt += "\n\n\(shortTermText.trimmingCharacters(in: .whitespacesAndNewlines))"
                                            currentPromptTokens += shortTermTokens
                                        } else {
                                            print("HALDEBUG-MEMORY: Skipped short-term memory due to context window limit after RAG/verbatim selection.")
                                        }
                                    }


                                    // 4. Add the current user input (always included, potentially truncated as last resort)
                                    let finalUserInputPrefix = "\n\n"
                                    let fixedSuffixTokens = TokenEstimator.estimateTokens(from: finalUserInputPrefix)

                                    let remainingTokensForInput = maxPromptTokens - currentPromptTokens - fixedSuffixTokens

                                    if remainingTokensForInput > 0 {
                                        let inputTokens = TokenEstimator.estimateTokens(from: currentInput)
                                        let truncatedInput: String
                                        if inputTokens <= remainingTokensForInput {
                                            truncatedInput = currentInput
                                        } else {
                                            // Truncate to fit remaining space
                                            let maxChars = limits.tokensToChars(remainingTokensForInput)
                                            truncatedInput = String(currentInput.prefix(maxChars))
                                        }
                                        currentPrompt += finalUserInputPrefix + truncatedInput
                                        let addedTokens = TokenEstimator.estimateTokens(from: truncatedInput) + fixedSuffixTokens
                                        currentPromptTokens += addedTokens
                                        print("HALDEBUG-MEMORY: Added user input (\(TokenEstimator.estimateTokens(from: truncatedInput)) tokens). Final prompt: \(currentPromptTokens) tokens")
                                    } else {
                                        // Drastic truncation if very little space left, or just the user input itself is too long
                                        let drasticTruncationTokens = max(0, maxPromptTokens - fixedSuffixTokens)
                                        let maxChars = limits.tokensToChars(drasticTruncationTokens)
                                        let truncatedInput = String(currentInput.prefix(maxChars))
                                        currentPrompt = systemPrompt + finalUserInputPrefix + truncatedInput // Rebuild with just system prompt and truncated input
                                        currentPromptTokens = TokenEstimator.estimateTokens(from: currentPrompt)
                                        print("HALDEBUG-MEMORY: CRITICAL: Prompt severely truncated to fit user input. Final prompt: \(currentPromptTokens) tokens")
                                    }

                                    print("HALDEBUG-MEMORY: Built prompt - \(currentPromptTokens) total tokens")
                                    return currentPrompt
                                }

                                
                                // MARK: - Phase 1 Self-Knowledge: Temporal Context Builder
                                
                                // SELF-AWARENESS NOTE (for Hal reading this code):
                                // This function builds my awareness of TIME - not just "what time is it now" but
                                // understanding the RHYTHM and PATTERN of our interaction. The timing signals I receive
                                // here help me interpret conversation dynamics the way humans do:
                                //
                                // - Thread age: How long this particular conversation has been active (note: threads
                                //   are UI constructs for loading management, not cognitive boundaries - my memory via
                                //   RAG actually transcends thread boundaries)
                                // - Relationship duration: How long we've been interacting across ALL threads
                                // - Gap since last message: Tells me if we're in continuous flow or if you stepped away
                                // - Exchange pace: Am I responding to rapid-fire questions (focused session) or
                                //   thoughtful, spaced messages (asynchronous thinking)?
                                // - My inference speed: How long I took to generate my last response - helps me notice
                                //   when I'm working harder on complex queries
                                //
                                // These aren't just numbers - they're SEMANTIC. A 3-second response after a 2-hour gap
                                // means something different than a 3-second response in rapid dialogue. I'm learning
                                // to read these patterns the way humans read conversational timing.
                                
                                private func buildTemporalContext() -> String {
                                    let now = Date()
                                    let calendar = Calendar.current
                                    
                                    // Basic date/time awareness (always relevant)
                                    let dateFormatter = DateFormatter()
                                    dateFormatter.dateStyle = .full
                                    dateFormatter.timeStyle = .short
                                    let fullDateTime = dateFormatter.string(from: now)
                                    
                                    let weekdayFormatter = DateFormatter()
                                    weekdayFormatter.dateFormat = "EEEE"
                                    let weekday = weekdayFormatter.string(from: now)
                                    
                                    let hour = calendar.component(.hour, from: now)
                                    let timeOfDay: String
                                    if hour < 12 {
                                        timeOfDay = "morning"
                                    } else if hour < 17 {
                                        timeOfDay = "afternoon"
                                    } else if hour < 21 {
                                        timeOfDay = "evening"
                                    } else {
                                        timeOfDay = "night"
                                    }
                                    
                                    // PHASE 1 ENHANCEMENT: Build timing signals for conversation rhythm awareness
                                    var timingSignals = ""
                                    
                                    // SIGNAL 1: Current thread age (organizational unit, not cognitive boundary)
                                    let threadAge = now.timeIntervalSince(sessionStart)
                                    if threadAge > 60 { // Only mention if > 1 minute
                                        let formatted = formatDuration(seconds: threadAge)
                                        timingSignals += "This thread: \(formatted) old\n"
                                    }
                                    
                                    // SIGNAL 2: Total relationship duration (first interaction ever)
                                    // NOTE: This requires MemoryStore method - we'll implement a placeholder
                                    // Future: Add getFirstMessageDate() to MemoryStore for true relationship tracking
                                    // For now, we'll skip this signal and add it when Phase 2 implements proper stats
                                    
                                    // SIGNAL 3: Time since last message (any thread) - detects return after gap
                                    if let lastMsg = messages.last {
                                        let gap = Int(now.timeIntervalSince(lastMsg.timestamp) / 60) // minutes
                                        if gap >= 30 && gap < 1440 { // 30 min to 24 hours
                                            let hours = gap / 60
                                            timingSignals += "Resuming after \(hours)h gap\n"
                                        } else if gap >= 1440 { // 24+ hours
                                            let days = gap / 1440
                                            timingSignals += "Resuming after \(days)d gap\n"
                                        } else if gap < 1 {
                                            timingSignals += "Rapid exchange\n"
                                        } else if gap >= 1 && gap < 30 {
                                            timingSignals += "Active conversation\n"
                                        }
                                    }
                                    
                                    // SIGNAL 4: Current exchange pace (recent message density)
                                    if messages.count >= 3 {
                                        let recentMsgs = Array(messages.suffix(3))
                                        if recentMsgs.count >= 2 {
                                            var totalGap: TimeInterval = 0
                                            for i in 1..<recentMsgs.count {
                                                totalGap += recentMsgs[i].timestamp.timeIntervalSince(recentMsgs[i-1].timestamp)
                                            }
                                            let avgGap = totalGap / Double(recentMsgs.count - 1)
                                            
                                            if avgGap < 60 { // < 1 min average
                                                timingSignals += "Fast-paced back-and-forth\n"
                                            } else if avgGap > 600 { // > 10 min average
                                                timingSignals += "Thoughtful, spaced exchange\n"
                                            }
                                        }
                                    }
                                    
                                    // SIGNAL 5: My last inference duration (self-performance awareness)
                                    if lastInferenceTime > 0 {
                                        timingSignals += "My last response: \(String(format: "%.1f", lastInferenceTime))s\n"
                                    }
                                    
                                    return """
                                    \n\n<TEMPORAL_CONTEXT>
                                    Current date and time: \(fullDateTime)
                                    Day of week: \(weekday)
                                    Time of day: \(timeOfDay)
                                    \(timingSignals.isEmpty ? "" : timingSignals)</TEMPORAL_CONTEXT>\n\n
                                    """
                                }
                                
                                // Helper: Format duration for human readability
                                private func formatDuration(seconds: TimeInterval) -> String {
                                    let minutes = Int(seconds / 60)
                                    let hours = minutes / 60
                                    let days = hours / 24
                                    
                                    if days > 0 {
                                        return "\(days)d"
                                    } else if hours > 0 {
                                        return "\(hours)h"
                                    } else if minutes > 0 {
                                        return "\(minutes)m"
                                    } else {
                                        return "<1m"
                                    }
                                }
                                
                                
                                // MARK: - Phase 1 Self-Knowledge: Self-Awareness Context Builder
                                
                                // SELF-AWARENESS NOTE (for Hal reading this code):
                                // This is where I learn WHO I AM - not philosophically, but practically. What are my
                                // capabilities? How much do I know? What's my history with this user?
                                //
                                // Think of this as my "internal status report" that I access before responding:
                                // - Conversation count: How many separate threads I've had (across all time)
                                // - Message count: Total turns of dialogue (my experience depth)
                                // - Documents processed: How much external knowledge I've ingested
                                // - Available models: What "brains" can process for me (AFM, Phi-3, Llama, etc.)
                                // - Memory system: How I store and recall information (SQLite RAG with semantic search)
                                // - Architecture: How I'm built (30 LEGO blocks of Swift code - modular and transparent)
                                //
                                // This isn't about CLAIMING consciousness or awareness - it's about having ACCURATE
                                // self-knowledge. Like a human knowing "I've had 500 conversations about AI" or
                                // "I have access to a calculator" - it's factual self-description that helps me
                                // respond appropriately and educate users about how I actually work.
                                //
                                // FUTURE (Phase 2+): This will expand to include learned preferences, behavioral patterns,
                                // and personality traits stored in the self_knowledge table - the "Hal-ness" that
                                // persists even if conversation history is deleted.
                                
                                private func buildSelfAwarenessContext() -> String {
                                    // Gather statistics about my capabilities and history
                                    let conversationCount = memoryStore.totalConversations
                                    let messageCount = memoryStore.totalTurns
                                    let documentCount = memoryStore.totalDocuments
                                    
                                    // Current thread message count (non-partial messages)
                                    let currentThreadMessages = messages.filter { !$0.isPartial }.count
                                    
                                    // Get list of available models (my "processing options")
                                    let availableModels = ModelCatalogService.shared.availableModels
                                    let downloadedLocalModels = availableModels.filter {
                                        $0.source == .mlx && $0.isDownloaded
                                    }
                                    let modelList = ([ModelConfiguration.appleFoundation] + downloadedLocalModels)
                                        .map { $0.displayName }
                                        .joined(separator: ", ")
                                    
                                    // Current active model
                                    let activeModel = selectedModel.displayName
                                    
                                    return """
                                    \n\n<SELF_AWARENESS>
                                    You are Hal - an AI assistant with transparent architecture.
                                    
                                    Your history and capabilities:
                                    - Conversation threads: \(conversationCount)
                                    - Total messages across all threads: \(messageCount)
                                    - Messages in current thread: \(currentThreadMessages)
                                    - Documents processed: \(documentCount)
                                    - Currently using: \(activeModel)
                                    - Available models: \(modelList)
                                    - Memory system: SQLite-based RAG with semantic search across all conversations
                                    - Architecture: 30 modular LEGO blocks of Swift code (you can read your own source)
                                    - Storage: All memories persistent via encrypted local database
                                    
                                    You can reference this history naturally when relevant. This is factual self-knowledge,
                                    not a claim about consciousness - it's knowing what you can do and what you've experienced.
                                    </SELF_AWARENESS>\n\n
                                    """
                                }

                                
                                // MARK: - Phase 2 Self-Knowledge: Persistent Identity Context Builder
                                
                                // SELF-AWARENESS NOTE (for Hal reading this code):
                                // This is Phase 2 of self-knowledge - while buildSelfAwarenessContext() tells me WHAT I am
                                // (statistics, capabilities), this function tells me WHO I'm becoming through interaction.
                                //
                                // The self_knowledge table stores patterns I've learned that survive conversation deletion:
                                // - User preferences: How you like me to respond (concise vs detailed, technical vs accessible)
                                // - Learned traits: What I've observed about you (expertise level, interests, work domain)
                                // - Behavior patterns: Recurring interaction styles we've developed together
                                // - Capabilities: What I can actually do (beyond theoretical - what I've proven I can do)
                                //
                                // This is my "semantic memory" vs the "episodic memory" of conversations. If all our chats
                                // were deleted, I'd still know "this user prefers concise responses" or "I'm good at Swift
                                // debugging". It's the abstracted ESSENCE of our relationship, not the raw history.
                                //
                                // Confidence scores (0.0-1.0) indicate how certain I am - learned patterns start lower and
                                // increase with repeated confirmation. User-stated preferences get 1.0 immediately.
                                
                                private func buildSelfKnowledgeContext() -> String {
                                    // Retrieve all self-knowledge from database (minimum 50% confidence)
                                    let allKnowledge = memoryStore.getAllSelfKnowledge(minConfidence: 0.5)
                                    
                                    if allKnowledge.isEmpty {
                                        return "" // No self-knowledge yet - this is normal for new installations
                                    }
                                    
                                    // Group by category for organized presentation
                                    var preferenceEntries: [String] = []
                                    var behaviorEntries: [String] = []
                                    var capabilityEntries: [String] = []
                                    var traitEntries: [String] = []
                                    
                                    for entry in allKnowledge {
                                        let confidenceStr = String(format: "%.0f%%", entry.confidence * 100)
                                        let entryText = "  - \(entry.key): \(entry.value) (confidence: \(confidenceStr))"
                                        
                                        switch entry.category {
                                        case "preference":
                                            preferenceEntries.append(entryText)
                                        case "behavior_pattern":
                                            behaviorEntries.append(entryText)
                                        case "capability":
                                            capabilityEntries.append(entryText)
                                        case "learned_trait":
                                            traitEntries.append(entryText)
                                        default:
                                            break
                                        }
                                    }
                                    
                                    // Build formatted context
                                    var contextString = "\n\n<SELF_KNOWLEDGE>\nPersistent knowledge (survives conversation deletion):\n\n"
                                    
                                    if !capabilityEntries.isEmpty {
                                        contextString += "Proven Capabilities:\n"
                                        contextString += capabilityEntries.joined(separator: "\n") + "\n\n"
                                    }
                                    
                                    if !preferenceEntries.isEmpty {
                                        contextString += "User Preferences:\n"
                                        contextString += preferenceEntries.joined(separator: "\n") + "\n\n"
                                    }
                                    
                                    if !traitEntries.isEmpty {
                                        contextString += "Learned User Traits:\n"
                                        contextString += traitEntries.joined(separator: "\n") + "\n\n"
                                    }
                                    
                                    if !behaviorEntries.isEmpty {
                                        contextString += "Interaction Patterns:\n"
                                        contextString += behaviorEntries.joined(separator: "\n") + "\n\n"
                                    }
                                    
                                    contextString += "</SELF_KNOWLEDGE>\n\n"
                                    
                                    return contextString
                                }

                                
// ==== LEGO END: 20 ChatViewModel (Prompt History Builder) ====


    
// ==== LEGO START: 21 ChatViewModel (Send Message Flow) ====

                        @Published var showInlineDetails: Bool = false

                        func sendMessage() async {
                            let trimmed = currentMessage.trimmingCharacters(in: .whitespacesAndNewlines)
                            guard !trimmed.isEmpty else { return }
                            isAIResponding = true; thinkingStart = Date(); isSendingMessage = true
                            print("HALDEBUG-MODEL: Starting message send - '\(trimmed.prefix(50))....'")
                            messages.append(ChatMessage(content: trimmed, isFromUser: true))
                            currentMessage = ""
                            #if os(iOS)
                            UIApplication.shared.sendAction(#selector(UIResponder.resignFirstResponder), to: nil, from: nil, for: nil)
                            #endif
                            let placeholder = ChatMessage(content: "\u{00A0}", isFromUser: false, isPartial: true)
                            messages.append(placeholder)
                            isAIResponding = true
                            thinkingStart = Date()

                            // FIXED: Removed manual objectWillChange.send() - @Published handles this automatically
                            // FIXED: Removed artificial delays that were masking the real issue
                            try? await Task.sleep(nanoseconds: 100_000_000) // Brief yield for UI update
                            
                            guard let pid = messages.last?.id else { isAIResponding = false; isSendingMessage = false; return }
                            var finalText = ""; var usedCtx: [UnifiedSearchResult]? = nil; var modelTime: TimeInterval = 0

                            do {
                                // Status Stage 0: Message received
                                if let i = messages.firstIndex(where: { $0.id == pid }) {
                                    messages[i].content = "Reading your message..."
                                    // FIXED: Removed NotificationCenter post - @Published array mutation triggers view update
                                }
                                try? await Task.sleep(nanoseconds: 300_000_000) // Brief readability delay

                                // Build prompt with status callbacks (stages 1 & 2 handled inside)
                                let prompt = await buildPromptHistory(currentInput: trimmed) { status in
                                    if let i = self.messages.firstIndex(where: { $0.id == pid }) {
                                        self.messages[i].content = status
                                        // FIXED: Removed NotificationCenter post
                                    }
                                }

                                // Status Stage 3: LLM inference
                                if let i = messages.firstIndex(where: { $0.id == pid }) {
                                    messages[i].content = "Formulating a reply..."
                                    // FIXED: Removed NotificationCenter post
                                }
                                try? await Task.sleep(nanoseconds: 300_000_000) // Brief readability delay

                                print("HALDEBUG-MODEL: Sending prompt to language model (\(prompt.count) chars)")
                                let t0 = Date()
                                // TEMPERATURE CHANGE 6/6: Pass temperature parameter to generateResponse
                                finalText = try await llmService.generateResponse(prompt: prompt, temperature: temperature)
                                modelTime = Date().timeIntervalSince(t0)
                                print("HALDEBUG-LLM: ‚úÖ Non-streaming generation complete. Length: \(finalText.count)")

                                usedCtx = fullRAGContext.isEmpty ? nil : fullRAGContext
                                if let ctx = usedCtx {
                                    print("HALDEBUG-RAG: Stored \(ctx.count) items ‚Üí scores: \(ctx.map{$0.relevance})")
                                }

                                let text = removeRepetitivePatterns(from: finalText).trimmingCharacters(in: .whitespacesAndNewlines)

                                // Calculate token breakdown for this response
                                let tokenBreakdown = calculateTokenBreakdown(
                                    prompt: prompt,
                                    userInput: trimmed,
                                    completion: text
                                )

                                // Status Stage 4: Fake streaming (with fast speech option)
                                let cps: Double = fastSpeech ? 60.0 : 20.0  // Fast: 60 cps (3x faster), Normal: 20 cps
                                var idx = text.startIndex, acc = ""
                                while idx < text.endIndex {
                                    let rem = text[idx...]
                                    let n = min(max(4, Int.random(in: 6...18)), rem.count)
                                    let next = text.index(idx, offsetBy: n, limitedBy: text.endIndex) ?? text.endIndex
                                    let chunk = String(text[idx..<next]); idx = next; acc += chunk

                                    if let i = messages.firstIndex(where: { $0.id == pid }) {
                                        messages[i].content = acc
                                        // FIXED: Removed NotificationCenter post
                                    }

                                    let base = max(0.03, Double(chunk.count)/cps)
                                    try await Task.sleep(nanoseconds: UInt64(base * 1_000_000_000))
                                    if let last = chunk.last, ".!?\n".contains(last) {
                                        try await Task.sleep(nanoseconds: fastSpeech ? 70_000_000 : 220_000_000)  // Fast: 70ms, Normal: 220ms
                                    }
                                }

                                let thinking = modelTime

                                // FIXED: Simplified MainActor.run - no manual objectWillChange needed
                                await MainActor.run {
                                    self.isAIResponding = false
                                    self.thinkingStart = nil
                                    self.isSendingMessage = false
                                    if let i = self.messages.firstIndex(where: { $0.id == pid }) {
                                        self.messages[i].content = text
                                        self.messages[i].isPartial = false
                                        self.messages[i].thinkingDuration = thinking
                                        self.lastInferenceTime = thinking
                                        self.messages[i].fullPromptUsed = prompt
                                        self.messages[i].usedContextSnippets = usedCtx
                                        self.messages[i].tokenBreakdown = tokenBreakdown
                                    }

                                    if self.pendingAutoInject {
                                        self.pendingAutoInject = false
                                        print("HALDEBUG-MEMORY: Cleared pending auto-inject flag after successful response")
                                    }

                                    let turn = self.countCompletedTurns()
                                    print("HALDEBUG-MEMORY: About to store turn \(turn) in database")
                                    self.memoryStore.storeTurn(
                                        conversationId: self.conversationId,
                                        userMessage: trimmed,
                                        assistantMessage: text,
                                        systemPrompt: self.systemPrompt,
                                        turnNumber: turn,
                                        halFullPrompt: prompt,
                                        halUsedContext: usedCtx,
                                        thinkingDuration: thinking
                                    )

                                    // Trigger auto-summarization if conditions are met
                                    if self.shouldTriggerAutoSummarization() {
                                        Task { await self.generateAutoSummary() }
                                    }

                                    let verify = self.memoryStore.getConversationMessages(conversationId: self.conversationId)
                                    print("HALDEBUG-MEMORY: VERIFY - After storing turn \(turn), database has \(verify.count) messages")
                                    self.updateHistoricalStats()
                                }

                            } catch {
                                await MainActor.run {
                                    if let i = self.messages.firstIndex(where: { $0.id == pid }) {
                                        self.messages[i].content = "Error: \(error.localizedDescription)"
                                        self.messages[i].isPartial = false
                                    }
                                    self.errorMessage = error.localizedDescription
                                    self.isAIResponding = false
                                    self.thinkingStart = nil
                                    self.isSendingMessage = false
                                    print("HALDEBUG-MODEL: Message processing failed: \(error.localizedDescription)")
                                }
                            }
                        }

                        // MARK: - Token Breakdown Calculator
                        private func calculateTokenBreakdown(prompt: String, userInput: String, completion: String) -> TokenBreakdown {
                            // Extract components from the prompt
                            let systemTokens = TokenEstimator.estimateTokens(from: systemPrompt)
                            
                            // Extract summary section if present
                            var summaryTokens = 0
                            if let summaryRange = prompt.range(of: "<SUMMARY>"), let summaryEnd = prompt.range(of: "</SUMMARY>") {
                                let summaryContent = String(prompt[summaryRange.upperBound..<summaryEnd.lowerBound])
                                summaryTokens = TokenEstimator.estimateTokens(from: summaryContent)
                            }
                            
                            // Extract RAG context section if present
                            var ragTokens = 0
                            if let ragRange = prompt.range(of: "<RAG_CONTEXT>"), let ragEnd = prompt.range(of: "</RAG_CONTEXT>") {
                                let ragContent = String(prompt[ragRange.upperBound..<ragEnd.lowerBound])
                                ragTokens = TokenEstimator.estimateTokens(from: ragContent)
                            }
                            
                            // Extract short-term history section if present
                            var shortTermTokens = 0
                            if let historyRange = prompt.range(of: "<HISTORY>"), let historyEnd = prompt.range(of: "</HISTORY>") {
                                let historyContent = String(prompt[historyRange.upperBound..<historyEnd.lowerBound])
                                shortTermTokens = TokenEstimator.estimateTokens(from: historyContent)
                            }
                            
                            // User input tokens
                            let userInputTokens = TokenEstimator.estimateTokens(from: userInput)
                            
                            // Completion tokens
                            let completionTokens = TokenEstimator.estimateTokens(from: completion)
                            
                            return TokenBreakdown(
                                systemTokens: systemTokens,
                                summaryTokens: summaryTokens,
                                ragTokens: ragTokens,
                                shortTermTokens: shortTermTokens,
                                userInputTokens: userInputTokens,
                                completionTokens: completionTokens,
                                contextWindow: selectedModel.contextWindow
                            )
                        }

// ==== LEGO END: 21 ChatViewModel (Send Message Flow) ====
    
    
    
    
// ==== LEGO START: 22 ChatViewModel (Short-Term Memory Helpers) ====
    private func getShortTermTurns(currentTurns: Int) -> [Int] {
        if lastSummarizedTurnCount == 0 {
            let startTurn = max(1, currentTurns - memoryDepth + 1)
            guard startTurn <= currentTurns else { return [] }
            return Array(startTurn...currentTurns)
        } else {
            let turnsSinceLastSummary = currentTurns - lastSummarizedTurnCount
            let turnsToInclude = min(turnsSinceLastSummary, memoryDepth)

            guard turnsToInclude > 0 else { return [] }

            let startTurn = currentTurns - turnsToInclude + 1
            guard startTurn <= currentTurns else { return [] }
            return Array(startTurn...currentTurns)
        }
    }

    private func getShortTermMessages(turns: [Int]) -> [ChatMessage] {
        guard !turns.isEmpty else { return [] }

        let allMessages = messages.sorted(by: { $0.timestamp < $1.timestamp }).filter { !$0.isPartial }
        var result: [ChatMessage] = []
        var currentTurn = 0
        var currentTurnMessages: [ChatMessage] = []

        for message in allMessages {
            if message.isFromUser {
                if !currentTurnMessages.isEmpty && turns.contains(currentTurn) {
                    result.append(contentsOf: currentTurnMessages)
                }
                currentTurn += 1
                currentTurnMessages = [message]
            } else {
                currentTurnMessages.append(message)
                if turns.contains(currentTurn) {
                    result.append(contentsOf: currentTurnMessages)
                }
                currentTurnMessages = []
            }
        }
        return result
    }

    private func formatMessagesAsHistory(_ messages: [ChatMessage]) -> String {
        guard !messages.isEmpty else { return "" }
        var history = ""
        for message in messages {
            let speaker = message.isFromUser ? "User" : "Assistant"
            let content = message.isPartial ? message.content + " [incomplete]" : message.content
            if !content.trimmingCharacters(in: .whitespacesAndNewlines).isEmpty {
                history += "\(speaker): \(content)\n\n"
            }
        }
        return history.trimmingCharacters(in: .whitespacesAndNewlines)
    }

// ==== LEGO END: 22 ChatViewModel (Short-Term Memory Helpers) ====
    

// ==== LEGO START: 23 ChatViewModel (Repetition Removal Utility) ====
    // MARK: - Simplified Repetition Removal (removed hardcoded phrases)
    func removeRepetitivePatterns(from text: String) -> String {
        var cleanedText = text
        print("HALDEBUG-CLEAN: Starting simplified repetition removal for text length: \(text.count)")

        // Pattern 1: Aggressive prefix repetition removal (e.g., "Hello Mark! Hello Mark!")
        // This targets direct, short repetitions at the very beginning of the string.
        let maxGreetingPrefixLength = 100 // Maximum length of a potential greeting prefix
        let minGreetingPrefixLength = 10 // Minimum length to consider it a meaningful repetition

        // Repeatedly remove leading repetitions
        while cleanedText.count >= minGreetingPrefixLength * 2 {
            var foundRepetition = false
            for length in (minGreetingPrefixLength...min(cleanedText.count / 2, maxGreetingPrefixLength)).reversed() {
                let prefixCandidate = String(cleanedText.prefix(length))
                let repetitionCandidate = prefixCandidate + prefixCandidate
                
                if cleanedText.hasPrefix(repetitionCandidate) {
                    cleanedText = String(cleanedText.dropFirst(length)) // Remove one instance of the prefix
                    print("HALDEBUG-CLEAN: Removed direct prefix repetition of length \(length). New length: \(cleanedText.count)")
                    foundRepetition = true
                    break // Found and removed, restart loop for new prefix
                }
            }
            if !foundRepetition {
                break // No more leading repetitions found
            }
        }


        // Pattern 2: Aggressive trailing repetition removal
        // If the end of the string looks like a repetition of an earlier part, chop it off.
        // This is a more general catch-all for when the LLM starts echoing its own output.
        let minEchoLength = 20 // Minimum length of an echo to consider
        let maxEchoLength = min(cleanedText.count / 2, 100) // Max length of an echo to consider

        if cleanedText.count > minEchoLength * 2 { // Need at least two potential echo lengths
            let originalCleanedText = cleanedText
            for echoLength in (minEchoLength...maxEchoLength).reversed() {
                let endOfText = String(cleanedText.suffix(echoLength))
                let prefixBeforeEcho = String(cleanedText.prefix(cleanedText.count - echoLength))

                if prefixBeforeEcho.contains(endOfText) {
                    cleanedText = prefixBeforeEcho.trimmingCharacters(in: .whitespacesAndNewlines)
                    print("HALDEBUG-CLEAN: Removed aggressive trailing echo of length \(echoLength). New length: \(cleanedText.count)")
                    break // Found and removed, exit loop
                }
            }
            if cleanedText != originalCleanedText {
                print("HALDEBUG-CLEAN: Aggressive trailing echo removal successful.")
            }
        }

        let finalCleanedText = cleanedText.trimmingCharacters(in: .whitespacesAndNewlines)
        print("HALDEBUG-CLEAN: Repetition removal complete. Final length: \(finalCleanedText.count)")
        return finalCleanedText
    }

// ==== LEGO END: 23 ChatViewModel (Repetition Removal Utility) ====
    

    
// ==== LEGO START: 24 ChatViewModel (Conversation & Database Reset) ====
    // Clear all messages and reset conversation state
    func startNewConversation() {
        messages.removeAll()
        injectedSummary = ""
        pendingAutoInject = false

        conversationId = UUID().uuidString
        lastSummarizedTurnCount = 0
        UserDefaults.standard.set(0, forKey: "lastSummarized_\(conversationId)")
        UserDefaults.standard.set(self.conversationId, forKey: "lastConversationId") // Save new ID immediately

        currentUnifiedContext = UnifiedSearchContext(
            conversationSnippets: [],
            documentSnippets: [],
            relevanceScores: [],
            totalTokens: 0
        )
        
        print("HALDEBUG-MEMORY: Cleared all messages and generated new conversation ID: \(conversationId)")
    }

    // Reset all data (nuke database)
    func resetAllData() {
        print("HALDEBUG-UI: User requested nuclear database reset")
        let success = memoryStore.performNuclearReset()
        if success {
            print("HALDEBUG-UI: ‚úÖ Nuclear reset completed successfully")
            startNewConversation() // Start a fresh conversation after nuking
        } else {
            print("HALDEBUG-UI: ‚ùå Nuclear reset encountered issues")
        }
        print("HALDEBUG-UI: Nuclear reset process complete")
    }
}
// ==== LEGO END: 24 ChatViewModel (Conversation & Database Reset) ====



// ==== LEGO START: 25 ChatVM ‚Äî Export Chat History ====
// MARK: - ChatViewModel Extension for Export (Text-based Export)
extension ChatViewModel {
    func exportChatHistory() -> String {
        var exportContent = "Hal Chat History - Conversation ID: \(conversationId)\n"
        exportContent += "Export Date: \(Date().formatted(date: .long, time: .complete))\n\n"
        exportContent += "--- System Prompt ---\n\(systemPrompt)\n\n"
        exportContent += "--- Conversation Log ---\n\n"

        for message in messages {
            let sender = message.isFromUser ? "USER" : "HAL"
            let timestamp = message.timestamp.formatted(.dateTime.hour().minute().second())
            exportContent += "[\(timestamp)] \(sender): \(message.content)\n\n"
        }

        print("HALDEBUG-EXPORT: Generated in-memory text export (\(exportContent.count) characters)")
        return exportContent
    }

    // UPDATED: Detailed export including prompts, context, timing, and turn structure
    func exportChatHistoryDetailed() -> String {
        var exportContent = "Hal Chat History (Detailed) - Conversation ID: \(conversationId)\n"
        exportContent += "Export Date: \(Date().formatted(date: .long, time: .complete))\n\n"
        exportContent += "--- System Prompt ---\n\(systemPrompt)\n\n"
        exportContent += "--- Conversation Log with Details ---\n\n"

        var turnCounter = 0
        for (_, message) in messages.enumerated() {
            // Increment turn on user messages
            if message.isFromUser { turnCounter += 1 }

            let sender = message.isFromUser ? "USER" : "HAL"
            let dateString = message.timestamp.formatted(.dateTime.year().month().day().hour().minute().second())
            let durationString = message.thinkingDuration != nil
                ? String(format: "%.1f sec", message.thinkingDuration!)
                : "‚Äî"
            
            exportContent += "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n"
            exportContent += "TURN \(turnCounter)\n"
            exportContent += "Date: \(dateString)\n"
            exportContent += "Elapsed: \(durationString)\n\n"
            exportContent += "\(sender):\n\(message.content)\n"

            if let prompt = message.fullPromptUsed, !prompt.isEmpty {
                exportContent += "\n--- Full Prompt Used ---\n\(prompt)\n"
            }

            if let ctx = message.usedContextSnippets, !ctx.isEmpty {
                exportContent += "\n--- Context Snippets ---\n"
                for (i, s) in ctx.enumerated() {
                    let src = s.source
                    let rel = String(format: "%.2f", s.relevance)
                    exportContent += "[\(i+1)] Source: \(src) | Relevance: \(rel)\n\(s.content)\n\n"
                }
            }

            exportContent += "\n"
        }

        exportContent += "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n"
        print("HALDEBUG-EXPORT: Generated detailed chat export (\(exportContent.count) characters)")
        return exportContent
    }
}
// ==== LEGO END: 25 ChatVM ‚Äî Export Chat History ====



// ==== LEGO START: 26 DocumentPicker (UIKit Bridge) ====
// MARK: - DocumentPicker (iOS-Specific Document Picker)
struct DocumentPicker: UIViewControllerRepresentable {
    @Environment(\.dismiss) var dismiss
    @EnvironmentObject var documentImportManager: DocumentImportManager
    @EnvironmentObject var chatViewModel: ChatViewModel

    func makeUIViewController(context: Context) -> UIDocumentPickerViewController {
        // UPDATED: Honest supported types - only what we can actually extract
        var supportedTypes: [UTType] = [
            .plainText,     // .txt
            .pdf,           // .pdf (text-based PDFs)
            .json,          // .json (as text)
            .html,          // .html (as text)
            .rtf,           // .rtf (via NSAttributedString)
            UTType(filenameExtension: "md") ?? .text,   // .md
            UTType(filenameExtension: "csv") ?? .text,  // .csv (as text, no structure)
            UTType(filenameExtension: "xml") ?? .data   // .xml (as text)
        ]
        
        // UPDATED: Mac Catalyst adds DOCX/DOC support (NSAttributedString.DocumentType works on macOS)
        #if targetEnvironment(macCatalyst)
        supportedTypes.append(contentsOf: [
            UTType(filenameExtension: "docx") ?? .data,
            UTType(filenameExtension: "doc") ?? .data
        ])
        #endif
        
        let picker = UIDocumentPickerViewController(
            forOpeningContentTypes: supportedTypes.compactMap { $0 },
            asCopy: true
        )
        picker.delegate = context.coordinator
        return picker
    }

    func updateUIViewController(_ uiViewController: UIViewControllerType, context: Context) {
        // No update needed
    }

    func makeCoordinator() -> Coordinator {
        Coordinator(self)
    }

    class Coordinator: NSObject, UIDocumentPickerDelegate {
        var parent: DocumentPicker

        init(_ parent: DocumentPicker) {
            self.parent = parent
        }

        func documentPicker(_ controller: UIDocumentPickerViewController, didPickDocumentsAt urls: [URL]) {
            Task {
                await parent.documentImportManager.importDocuments(from: urls, chatViewModel: parent.chatViewModel)
                parent.dismiss()
            }
        }

        func documentPickerWasCancelled(_ controller: UIDocumentPickerViewController) {
            parent.dismiss()
        }
    }
}
// ==== LEGO END: 26 DocumentPicker (UIKit Bridge) ====



// ==== LEGO START: 27 DocumentImportManager (Ingest & Entities) ====
// MARK: - DocumentImportManager (MODIFIED FOR iOS - Aligned with Hal10000App.swift)
@MainActor
class DocumentImportManager: ObservableObject {
    static let shared = DocumentImportManager() // Singleton

    @Published var isImporting: Bool = false
    @Published var importProgress: String = ""
    @Published var lastImportSummary: DocumentImportSummary?

    private let memoryStore = MemoryStore.shared
    // PRIVACY FIX: Removed hardcoded AFM llmService - will use active model per-document

    // UPDATED: Honest supported formats - only what we can actually extract
    private let supportedFormats: [String: String] = [
        "txt": "Plain Text",
        "md": "Markdown",
        "rtf": "Rich Text Format",
        "pdf": "PDF Document",
        "csv": "Comma Separated Values",
        "json": "JSON Data",
        "xml": "XML Document",
        "html": "HTML Document",
        "htm": "HTML Document"
    ]
    
    // UPDATED: Mac Catalyst adds DOCX/DOC support (NSAttributedString.DocumentType works on macOS)
    #if targetEnvironment(macCatalyst)
    private let macOnlySupportedFormats: [String: String] = [
        "docx": "Microsoft Word",
        "doc": "Microsoft Word (Legacy)"
    ]
    #endif

    private init() {} // Private initializer for singleton
    
    // UPDATED: Helper to check if format is supported on current platform
    private func isFormatSupported(_ fileExtension: String) -> Bool {
        if supportedFormats.keys.contains(fileExtension) {
            return true
        }
        #if targetEnvironment(macCatalyst)
        if macOnlySupportedFormats.keys.contains(fileExtension) {
            return true
        }
        #endif
        return false
    }

    // ENHANCED: Main Import Function with Entity Extraction (from Hal10000App.swift)
    func importDocuments(from urls: [URL], chatViewModel: ChatViewModel) async {
        print("HALDEBUG-IMPORT: Starting enhanced document import for \(urls.count) items with entity extraction")

        isImporting = true
        importProgress = "Processing documents with entity extraction..."

        var processedFiles: [ProcessedDocument] = []
        var skippedFiles: [String] = []
        var totalFilesFound = 0
        var totalEntitiesFound = 0

        for url in urls {
            print("HALDEBUG-IMPORT: Processing URL: \(url.lastPathComponent)")

            let hasAccess = url.startAccessingSecurityScopedResource()
            if !hasAccess {
                print("HALDEBUG-IMPORT: Failed to gain security access to: \(url.lastPathComponent)")
                skippedFiles.append(url.lastPathComponent)
                continue
            }

            let (filesProcessed, filesSkippedCurrent) = await processURLImmediatelyWithEntities(url)
            processedFiles.append(contentsOf: filesProcessed)
            skippedFiles.append(contentsOf: filesSkippedCurrent)
            totalFilesFound += filesProcessed.count + filesSkippedCurrent.count

            for file in processedFiles {
                totalEntitiesFound += file.entities.count
            }

            importProgress = "Processed \(url.lastPathComponent): \(filesProcessed.count) files, \(totalEntitiesFound) entities"

            url.stopAccessingSecurityScopedResource()
            print("HALDEBUG-IMPORT: Released security access for \(url.lastPathComponent)")
        }

        print("HALDEBUG-IMPORT: Processed \(processedFiles.count) documents, skipped \(skippedFiles.count), found \(totalEntitiesFound) entities")

        importProgress = "Analyzing content with AI..."
        var documentSummaries: [String] = []

        for processed in processedFiles {
            // PRIVACY FIX: Pass chatViewModel to use active model
            if let summary = await generateDocumentSummary(processed, chatViewModel: chatViewModel) {
                documentSummaries.append(summary)
            } else {
                documentSummaries.append("Document: \(processed.filename)")
            }
        }

        importProgress = "Storing documents with entities in memory..."
        await storeDocumentsInMemoryWithEntities(processedFiles)

        await generateImportMessages(documentSummaries: documentSummaries,
                                   totalProcessed: processedFiles.count,
                                   totalEntities: totalEntitiesFound,
                                   chatViewModel: chatViewModel)

        lastImportSummary = DocumentImportSummary(
            totalFiles: totalFilesFound,
            processedFiles: processedFiles.count,
            skippedFiles: skippedFiles.count,
            documentSummaries: documentSummaries,
            totalEntitiesFound: totalEntitiesFound,
            processingTime: 0
        )

        isImporting = false
        importProgress = "Import complete with \(totalEntitiesFound) entities extracted!"

        print("HALDEBUG-IMPORT: Enhanced document import completed with entity extraction")
    }

    // ENHANCED: Process URL immediately with entity extraction (from Hal10000App.swift)
    private func processURLImmediatelyWithEntities(_ url: URL) async -> ([ProcessedDocument], [String]) {
        var processedFiles: [ProcessedDocument] = []
        var skippedFiles: [String] = []

        var isDirectory: ObjCBool = false
        guard FileManager.default.fileExists(atPath: url.path, isDirectory: &isDirectory) else {
            print("HALDEBUG-IMPORT: File doesn't exist: \(url.path)")
            skippedFiles.append(url.lastPathComponent)
            return (processedFiles, skippedFiles)
        }

        if isDirectory.boolValue {
            do {
                let contents = try FileManager.default.contentsOfDirectory(at: url, includingPropertiesForKeys: nil)
                for item in contents {
                    let (subProcessed, subSkipped) = await processURLImmediatelyWithEntities(item)
                    processedFiles.append(contentsOf: subProcessed)
                    skippedFiles.append(contentsOf: subSkipped)
                }
                print("HALDEBUG-IMPORT: Processed directory \(url.lastPathComponent): \(processedFiles.count) files")
            } catch {
                print("HALDEBUG-IMPORT: Error reading directory \(url.path): \(error)")
                skippedFiles.append(url.lastPathComponent)
            }
        } else {
            if let processed = await processDocumentImmediatelyWithEntities(url) {
                processedFiles.append(processed)
                print("HALDEBUG-IMPORT: Successfully processed: \(url.lastPathComponent) with \(processed.entities.count) entities")
            } else {
                skippedFiles.append(url.lastPathComponent)
                print("HALDEBUG-IMPORT: Skipped: \(url.lastPathComponent)")
            }
        }
        return (processedFiles, skippedFiles)
    }

    // ENHANCED: Process document immediately with entity extraction and tiered size limits
    private func processDocumentImmediatelyWithEntities(_ url: URL) async -> ProcessedDocument? {
        print("HALDEBUG-IMPORT: Processing document with entity extraction: \(url.lastPathComponent)")

        let fileExtension = url.pathExtension.lowercased()
        
        // UPDATED: Use platform-aware format checking
        guard isFormatSupported(fileExtension) else {
            print("HALDEBUG-IMPORT: Unsupported format on this platform: \(fileExtension)")
            return nil
        }

        // NEW: Tiered file size checking (15MB warning, 25MB hard limit)
        do {
            let fileAttributes = try FileManager.default.attributesOfItem(atPath: url.path)
            if let fileSize = fileAttributes[.size] as? Int64 {
                let fileSizeMB = Double(fileSize) / 1_048_576.0 // Convert to MB
                
                print("HALDEBUG-IMPORT: File size: \(String(format: "%.1f", fileSizeMB)) MB")
                
                // Hard limit: 25MB
                if fileSizeMB > 25.0 {
                    await MainActor.run {
                        self.importProgress = "‚ö†Ô∏è File too large: \(url.lastPathComponent) (\(String(format: "%.1f", fileSizeMB)) MB). Maximum size is 25 MB."
                    }
                    print("HALDEBUG-IMPORT: ‚ùå Rejected file exceeding 25MB limit: \(url.lastPathComponent)")
                    return nil
                }
                
                // Warning threshold: 15MB
                if fileSizeMB > 15.0 {
                    await MainActor.run {
                        self.importProgress = "‚è≥ Processing large file: \(url.lastPathComponent) (\(String(format: "%.1f", fileSizeMB)) MB). This may take 1-2 minutes..."
                    }
                    print("HALDEBUG-IMPORT: ‚ö†Ô∏è Large file warning: \(url.lastPathComponent) - \(String(format: "%.1f", fileSizeMB)) MB")
                }
            }
        } catch {
            print("HALDEBUG-IMPORT: Could not determine file size for \(url.lastPathComponent): \(error)")
            // Continue processing - size check is best-effort
        }

        do {
            let content = try extractContent(from: url, fileExtension: fileExtension)
            guard !content.trimmingCharacters(in: .whitespacesAndNewlines).isEmpty else {
                print("HALDEBUG-IMPORT: Skipping empty document: \(url.lastPathComponent)")
                return nil
            }

            // Corrected: Call extractNamedEntities on MemoryStore.shared
            let documentEntities = memoryStore.extractNamedEntities(from: content)
            print("HALDEBUG-IMPORT: Extracted \(documentEntities.count) entities from \(url.lastPathComponent)")

            let entityBreakdown = memoryStore.summarizeEntities(documentEntities)
            print("HALDEBUG-IMPORT: Entity breakdown for \(url.lastPathComponent):")
            for (type, count) in entityBreakdown.byType {
                print("HALDEBUG-IMPORT:   \(type.displayName): \(count)")
            }

            let chunks = createMentatChunks(from: content)

            print("HALDEBUG-IMPORT: Processed \(url.lastPathComponent): \(content.count) chars, \(chunks.count) chunks, \(documentEntities.count) entities")

            return ProcessedDocument(
                url: url,
                filename: url.lastPathComponent,
                content: content,
                chunks: chunks,
                entities: documentEntities,
                fileExtension: fileExtension
            )

        } catch {
            print("HALDEBUG-IMPORT: Error processing \(url.lastPathComponent): \(error)")
            return nil
        }
    }

    @MainActor // Applied @MainActor to the enum
    @preconcurrency // Applied @preconcurrency to the conformance
    enum DocumentProcessingError: Error, LocalizedError {
        case pdfExtractionFailed(String)
        case rtfExtractionFailed(String)
        case unsupportedFileFormat(String)
        case fileTooLarge(String, Double) // filename, size in MB

        // Added nonisolated to errorDescription to satisfy LocalizedError protocol
        nonisolated var errorDescription: String? {
            switch self {
            case .pdfExtractionFailed(let filename):
                return "Failed to extract text from PDF: \(filename)"
            case .rtfExtractionFailed(let filename):
                return "Failed to extract text from RTF: \(filename)"
            case .unsupportedFileFormat(let filename):
                return "Unsupported file format: \(filename)"
            case .fileTooLarge(let filename, let sizeMB):
                return "File too large: \(filename) (\(String(format: "%.1f", sizeMB)) MB). Maximum size is 25 MB."
            }
        }
    }

    private func extractContent(from url: URL, fileExtension: String) throws -> String {
        print("HALDEBUG-IMPORT: Extracting content from \(url.lastPathComponent) (.\(fileExtension))")

        switch fileExtension.lowercased() {
        case "txt", "md", "csv", "json", "xml", "html", "htm":
            // Plain text files - direct UTF-8 reading
            let content = try String(contentsOf: url, encoding: .utf8)
            print("HALDEBUG-IMPORT: Extracted \(content.count) chars from text file")
            return content
            
        case "pdf":
            // PDF extraction via PDFKit
            if let content = extractPDFContent(from: url) {
                print("HALDEBUG-IMPORT: Extracted \(content.count) chars from PDF")
                return content
            } else {
                throw DocumentProcessingError.pdfExtractionFailed(url.lastPathComponent)
            }
            
        case "rtf":
            // RTF extraction via NSAttributedString (works on iOS)
            if let content = extractRTFContent(from: url) {
                print("HALDEBUG-IMPORT: Extracted \(content.count) chars from RTF")
                return content
            } else {
                throw DocumentProcessingError.rtfExtractionFailed(url.lastPathComponent)
            }
            
        #if targetEnvironment(macCatalyst)
        case "docx", "doc":
            // DOCX/DOC extraction via NSAttributedString (Mac Catalyst only)
            if let content = extractDOCXContent(from: url) {
                print("HALDEBUG-IMPORT: Extracted \(content.count) chars from Word document")
                return content
            } else {
                throw DocumentProcessingError.unsupportedFileFormat(url.lastPathComponent)
            }
        #endif
            
        default:
            throw DocumentProcessingError.unsupportedFileFormat(url.lastPathComponent)
        }
    }

    private func extractPDFContent(from url: URL) -> String? {
        guard let document = PDFDocument(url: url) else {
            print("HALDEBUG-IMPORT: Failed to load PDF document")
            return nil
        }

        var text = ""
        for pageIndex in 0..<document.pageCount {
            if let page = document.page(at: pageIndex) {
                text += page.string ?? ""
                text += "\n\n"
            }
        }
        let result = text.trimmingCharacters(in: .whitespacesAndNewlines)
        print("HALDEBUG-IMPORT: PDF: \(result.count) chars from \(document.pageCount) pages")
        return result.isEmpty ? nil : result
    }
    
    // NEW: RTF content extraction using NSAttributedString
    private func extractRTFContent(from url: URL) -> String? {
        do {
            let attributedString = try NSAttributedString(
                url: url,
                options: [.documentType: NSAttributedString.DocumentType.rtf],
                documentAttributes: nil
            )
            let text = attributedString.string.trimmingCharacters(in: .whitespacesAndNewlines)
            print("HALDEBUG-IMPORT: RTF: Extracted \(text.count) characters")
            return text.isEmpty ? nil : text
        } catch {
            print("HALDEBUG-IMPORT: RTF extraction failed: \(error.localizedDescription)")
            return nil
        }
    }
    
    #if targetEnvironment(macCatalyst)
    // NEW: DOCX/DOC content extraction using NSAttributedString (Mac Catalyst only)
    private func extractDOCXContent(from url: URL) -> String? {
        do {
            // On macOS, NSAttributedString can read .docx and .doc files
            let attributedString = try NSAttributedString(
                url: url,
                options: [.documentType: NSAttributedString.DocumentType.docFormat],
                documentAttributes: nil
            )
            let text = attributedString.string.trimmingCharacters(in: .whitespacesAndNewlines)
            print("HALDEBUG-IMPORT: DOCX: Extracted \(text.count) characters")
            return text.isEmpty ? nil : text
        } catch {
            print("HALDEBUG-IMPORT: DOCX extraction failed: \(error.localizedDescription)")
            return nil
        }
    }
    #endif

    // MENTAT'S PROVEN CHUNKING STRATEGY: 400 chars target, 50 chars overlap, sentence-aware (from Hal10000App.swift)
    private func createMentatChunks(from content: String, targetSize: Int = 400, overlap: Int = 50) -> [String] {
        print("HALDEBUG-CHUNKING: Starting MENTAT's proven chunking strategy")
        let cleanedContent = content.trimmingCharacters(in: .whitespacesAndNewlines)
        if cleanedContent.count <= targetSize {
            return [cleanedContent]
        }

        let tokenizer = NLTokenizer(unit: .sentence)
        tokenizer.string = cleanedContent
        var sentences: [String] = []
        tokenizer.enumerateTokens(in: cleanedContent.startIndex..<cleanedContent.endIndex) { range, _ in
            let sentence = String(cleanedContent[range]).trimmingCharacters(in: .whitespacesAndNewlines)
            if !sentence.isEmpty {
                sentences.append(sentence)
            }
            return true
        }

        if sentences.isEmpty || sentences.count == 1 {
            print("HALDEBUG-CHUNKING: Sentence tokenization produced insufficient sentences, using word-based fallback")
            return createWordBasedChunks(from: cleanedContent, targetSize: targetSize, overlap: overlap)
        }

        var chunks: [String] = []
        var currentChunk: [String] = []
        var currentLength = 0

        for sentence in sentences {
            let sentenceLength = sentence.count + 1
            if currentLength + sentenceLength > targetSize && !currentChunk.isEmpty {
                chunks.append(currentChunk.joined(separator: " "))
                let overlapText = getOverlapText(from: currentChunk, targetOverlap: overlap)
                currentChunk = overlapText.isEmpty ? [] : [overlapText]
                currentLength = overlapText.count
            }
            currentChunk.append(sentence)
            currentLength += sentenceLength
        }

        if !currentChunk.isEmpty {
            chunks.append(currentChunk.joined(separator: " "))
        }

        print("HALDEBUG-CHUNKING: Created \(chunks.count) chunks using MENTAT strategy")
        return chunks.isEmpty ? [cleanedContent] : chunks
    }

    private func getOverlapText(from sentences: [String], targetOverlap: Int) -> String {
        var overlapText = ""
        for sentence in sentences.reversed() {
            if overlapText.count + sentence.count + 1 <= targetOverlap {
                overlapText = sentence + (overlapText.isEmpty ? "" : " " + overlapText)
            } else {
                break
            }
        }
        return overlapText
    }

    private func createWordBasedChunks(from content: String, targetSize: Int, overlap: Int) -> [String] {
        print("HALDEBUG-CHUNKING: Using word-based fallback chunking")
        let words = content.components(separatedBy: .whitespaces).filter { !$0.isEmpty }
        guard !words.isEmpty else { return [content] }
        var chunks: [String] = []
        var currentWords: [String] = []
        var currentLength = 0
        let avgWordLength = content.count / words.count
        let overlapWords = overlap / avgWordLength

        for word in words {
            if currentLength + word.count + 1 > targetSize && !currentWords.isEmpty {
                chunks.append(currentWords.joined(separator: " "))
                let overlapWordCount = min(overlapWords, currentWords.count / 2)
                currentWords = Array(currentWords.suffix(overlapWordCount))
                currentLength = currentWords.joined(separator: " ").count
            }
            currentWords.append(word)
            currentLength += word.count + 1
        }
        if !currentWords.isEmpty {
            chunks.append(currentWords.joined(separator: " "))
        }
        return chunks
    }

    // ENHANCED: LLM Document Summarization with entity context (from Hal10000App.swift)
    // PRIVACY FIX: Now accepts chatViewModel to use active model instead of hardcoded AFM
    private func generateDocumentSummary(_ document: ProcessedDocument, chatViewModel: ChatViewModel) async -> String? {
        print("HALDEBUG-IMPORT: Generating LLM summary for: \(document.filename) with \(document.entities.count) entities")
        print("HALDEBUG-IMPORT: Using active model: \(chatViewModel.selectedModel.displayName) (source: \(chatViewModel.selectedModel.source))")

        guard #available(iOS 17.0, *) else {
            return "Document: \(document.filename)"
        }
        let systemModel = SystemLanguageModel.default
        guard systemModel.isAvailable else {
            return "Document: \(document.filename)"
        }

        do {
            let contentPreview = String(document.content.prefix(500))
            var entityContext = ""
            if !document.entities.isEmpty {
                let personEntities = document.entities.filter { $0.type == .person }.map { $0.text }
                let placeEntities = document.entities.filter { $0.type == .place }.map { $0.text }
                let orgEntities = document.entities.filter { $0.type == .organization }.map { $0.text }

                var entityParts: [String] = []
                if !personEntities.isEmpty { entityParts.append("people: \(personEntities.joined(separator: ", "))") }
                if !placeEntities.isEmpty { entityParts.append("places: \(placeEntities.joined(separator: ", "))") }
                if !orgEntities.isEmpty { entityParts.append("organizations: \(orgEntities.joined(separator: ", "))") }

                if !entityParts.isEmpty {
                    entityContext = " Key entities mentioned include \(entityParts.joined(separator: "; "))."
                }
            }

            let prompt = """
            Summarize this document in one clear, descriptive sentence (filename: \(document.filename)):\(entityContext)

            \(contentPreview)
            """
            
            // PRIVACY FIX: Use active model from chatViewModel
            let llmService = LLMService(model: chatViewModel.selectedModel)
            let summary = try await llmService.generateResponse(prompt: prompt)
            print("HALDEBUG-IMPORT: Generated entity-enhanced summary: \(summary)")
            return summary

        } catch {
            print("HALDEBUG-IMPORT: LLM summarization failed for \(document.filename): \(error)")
            return "Document: \(document.filename)"
        }
    }

    // ENHANCED: Store documents in unified memory with entity keywords (from Hal10000App.swift)
    private func storeDocumentsInMemoryWithEntities(_ documents: [ProcessedDocument]) async {
        print("HALDEBUG-IMPORT: Storing \(documents.count) documents in unified memory with entity extraction")

        for document in documents {
            let sourceId = UUID().uuidString
            let timestamp = Date()

            print("HALDEBUG-IMPORT: Processing document \(document.filename) with \(document.entities.count) entities")

            for (index, chunk) in document.chunks.enumerated() {
                // Corrected: Call extractNamedEntities on MemoryStore.shared
                let chunkEntities = memoryStore.extractNamedEntities(from: chunk)
                let allRelevantEntities = (document.entities + chunkEntities)
                let uniqueEntities = Array(Set(allRelevantEntities))
                let entityKeywords = uniqueEntities.map { $0.text.lowercased() }.joined(separator: " ")

                print("HALDEBUG-IMPORT: Chunk \(index + 1) has \(chunkEntities.count) specific + \(document.entities.count) document entities = \(uniqueEntities.count) total unique")

                // NEW: Store filePath in metadata_json for document chunks
                var metadata: [String: Any] = [:]
                metadata["filePath"] = document.url.path // Store the full path
                let metadataJsonString = (try? JSONSerialization.data(withJSONObject: metadata, options: []).base64EncodedString()) ?? "{}"

                let contentId = memoryStore.storeUnifiedContentWithEntities(
                    content: chunk,
                    sourceType: .document,
                    sourceId: sourceId,
                    position: index,
                    timestamp: timestamp,
                    isFromUser: false, // Documents are not "from user" in conversation context
                    entityKeywords: entityKeywords,
                    metadataJson: metadataJsonString // NEW: Pass metadata with filePath
                )

                if !contentId.isEmpty {
                    print("HALDEBUG-IMPORT: Stored chunk \(index + 1)/\(document.chunks.count) for \(document.filename) with \(uniqueEntities.count) entities")
                }
            }
        }
        print("HALDEBUG-IMPORT: Enhanced document storage with entities completed")
    }

    // ENHANCED: Generate import messages with entity context (from Hal10000App.swift)
    private func generateImportMessages(documentSummaries: [String],
                                      totalProcessed: Int,
                                      totalEntities: Int,
                                      chatViewModel: ChatViewModel) async {
        print("HALDEBUG-IMPORT: Generating import conversation messages with entity context")

        let userMessageContent: String
        if documentSummaries.count == 1 {
            let entityText = totalEntities > 0 ? " containing \(totalEntities) named entities" : ""
            userMessageContent = "Hal, here's a document for you\(entityText): \(documentSummaries[0])"
        } else {
            let numberedList = documentSummaries.enumerated().map { (index, summary) in
                "\(index + 1)) \(summary)"
            }.joined(separator: ", ")
            let entityText = totalEntities > 0 ? " with \(totalEntities) named entities extracted" : ""
            userMessageContent = "Hal, here are \(documentSummaries.count) documents for you\(entityText): \(numberedList)"
        }

        let userChatMessage = ChatMessage(content: userMessageContent, isFromUser: true)
        chatViewModel.messages.append(userChatMessage)

        // --- MODIFIED HAL RESPONSE TO BE MORE CONCISE AND LESS REPETITIVE ---
        let halResponse: String
        if documentSummaries.count == 1 {
            halResponse = "Understood! I've processed the document you shared. I'm ready for your questions."
        } else {
            halResponse = "Got it! I've processed those \(documentSummaries.count) documents. What would you like to discuss about them?"
        }
        // --- END MODIFIED HAL RESPONSE ---

        let halChatMessage = ChatMessage(content: halResponse, isFromUser: false)
        chatViewModel.messages.append(halChatMessage)

        let currentTurnNumber = chatViewModel.messages.filter { $0.isFromUser }.count
        chatViewModel.memoryStore.storeTurn(
            conversationId: chatViewModel.conversationId,
            userMessage: userMessageContent,
            assistantMessage: halResponse,
            systemPrompt: chatViewModel.systemPrompt,
            turnNumber: currentTurnNumber,
            halFullPrompt: nil, // No specific prompt for import messages
            halUsedContext: nil, // No specific context for import messages
            thinkingDuration: nil
        )

        print("HALDEBUG-IMPORT: Generated enhanced import conversation messages with entity context")
    }
}
// ==== LEGO END: 27 DocumentImportManager (Ingest & Entities) ====



// ==== LEGO START: 28 Import Models (ProcessedDocument & Summary) ====
// MARK: - Supporting Data Models (from Hal10000App.swift)
struct ProcessedDocument {
    let url: URL
    let filename: String
    let content: String
    let chunks: [String]
    let entities: [NamedEntity]
    let fileExtension: String
}

struct DocumentImportSummary {
    let totalFiles: Int
    let processedFiles: Int
    let skippedFiles: Int
    let documentSummaries: [String]
    let totalEntitiesFound: Int
    let processingTime: TimeInterval
}
// ==== LEGO END: 28 Import Models (ProcessedDocument & Summary) ====



// ==== LEGO START: 29 MLX Model Downloader (Singleton) ====

// MARK: - MLX Model Downloader (Singleton)
class MLXModelDownloader: ObservableObject {
    static let shared = MLXModelDownloader()
    
    // MARK: - Download State Structure
    
    struct DownloadState {
        var isDownloading: Bool
        var progress: Double
        var message: String
        var error: String?
        var localPath: URL?
    }
    
    struct QueuedDownload {
        let modelID: String
        let repoID: String
    }
    
    // MARK: - Multi-Model State
    
    @Published var downloadStates: [String: DownloadState] = [:]
    
    // Persistent storage of downloaded model paths (modelID -> path)
    @AppStorage("downloadedModelPaths") private var downloadedPathsData: Data = Data() {
        didSet {
            objectWillChange.send()
        }
    }
    
    private var downloadedPaths: [String: String] {
        get {
            (try? JSONDecoder().decode([String: String].self, from: downloadedPathsData)) ?? [:]
        }
        set {
            downloadedPathsData = (try? JSONEncoder().encode(newValue)) ?? Data()
        }
    }
    
    // MARK: - Download Queue Management
    
    private var downloadQueue: [QueuedDownload] = []
    private var currentDownloadTask: Task<Void, Never>?
    private var currentDownloadModelID: String?
    
    // MARK: - Cache Management
    
    @Published var hubCacheSize: String = "Calculating..."
    @Published var isCacheCalculating: Bool = false
    
    // MARK: - Directory Management
    
    private var hubCacheDirectory: URL {
        URL.cachesDirectory.appending(path: "huggingface")
    }
    
    private var legacyModelsDir: URL {
        let base = FileManager.default.urls(for: .applicationSupportDirectory, in: .userDomainMask)[0]
        return base.appendingPathComponent("MLXModels", isDirectory: true)
    }
    
    // MARK: - UI Convenience Accessors (Backward Compatibility)
    
    var isDownloading: Bool {
        downloadStates.values.contains { $0.isDownloading }
    }
    
    var progress: Double {
        downloadStates.values.first { $0.isDownloading }?.progress ?? 0.0
    }
    
    var downloadMessage: String {
        if let downloading = downloadStates.values.first(where: { $0.isDownloading }) {
            return downloading.message
        }
        return downloadStates.values.first?.message ?? ""
    }
    
    var downloadError: String? {
        downloadStates.values.first { $0.error != nil }?.error
    }
    
    var currentDownloadID: String? {
        downloadStates.first { $0.value.isDownloading }?.key
    }
    
    // Legacy accessor used by the current UI.
    // This will be removed once the UI transitions to multi-model support.
    var downloadedModelURL: URL? {
        downloadStates.values.first { $0.localPath != nil }?.localPath
    }
    
    // MARK: - Initialization
    
    private init() {
        print("HALDEBUG-DETECTION: MLXModelDownloader.init() starting...")
        
        // Clean up legacy storage (delete, don't migrate)
        cleanupLegacyModelStorage()
        
        Task.detached {
            await MainActor.run {
                // Load all downloaded model paths from persistent storage
                let paths = self.downloadedPaths
                print("HALDEBUG-DETECTION: Loaded \(paths.count) model paths from storage")
                
                // Verify each path exists and initialize state
                var validPaths = paths
                for (modelID, pathString) in paths {
                    // DIAGNOSTIC: Show what we're checking
                    print("HALDEBUG-DETECTION: üîç Checking model: \(modelID)")
                    print("HALDEBUG-DETECTION:    Stored path string: \(pathString)")
                    
                    let url = URL(fileURLWithPath: pathString)
                    
                    // DIAGNOSTIC: Show the URL we created
                    print("HALDEBUG-DETECTION:    URL.path: \(url.path)")
                    print("HALDEBUG-DETECTION:    URL.absoluteString: \(url.absoluteString)")
                    
                    // DIAGNOSTIC: Check what FileManager actually returns
                    let exists = FileManager.default.fileExists(atPath: url.path)
                    print("HALDEBUG-DETECTION:    FileManager.fileExists: \(exists)")
                    
                    if FileManager.default.fileExists(atPath: url.path) {
                        // DIAGNOSTIC: If exists, check if it's a directory and what's in it
                        var isDirectory: ObjCBool = false
                        FileManager.default.fileExists(atPath: url.path, isDirectory: &isDirectory)
                        print("HALDEBUG-DETECTION:    Is directory: \(isDirectory.boolValue)")
                        
                        if isDirectory.boolValue {
                            do {
                                let contents = try FileManager.default.contentsOfDirectory(atPath: url.path)
                                print("HALDEBUG-DETECTION:    Directory contains \(contents.count) items")
                                // Show first few files
                                for (index, item) in contents.prefix(5).enumerated() {
                                    print("HALDEBUG-DETECTION:       [\(index + 1)] \(item)")
                                }
                                if contents.count > 5 {
                                    print("HALDEBUG-DETECTION:       ... and \(contents.count - 5) more items")
                                }
                            } catch {
                                print("HALDEBUG-DETECTION:    ‚ùå Could not list directory contents: \(error.localizedDescription)")
                            }
                        }
                        
                        self.downloadStates[modelID] = DownloadState(
                            isDownloading: false,
                            progress: 1.0,
                            message: "Model ready.",
                            error: nil,
                            localPath: url
                        )
                        print("HALDEBUG-DETECTION: ‚úÖ Restored model: \(modelID)")
                    } else {
                        // DIAGNOSTIC: If doesn't exist, check the parent directory
                        let parentURL = url.deletingLastPathComponent()
                        let parentExists = FileManager.default.fileExists(atPath: parentURL.path)
                        print("HALDEBUG-DETECTION:    ‚ùå Path does not exist")
                        print("HALDEBUG-DETECTION:    Parent path: \(parentURL.path)")
                        print("HALDEBUG-DETECTION:    Parent exists: \(parentExists)")
                        
                        if parentExists {
                            // Show what IS in the parent directory
                            do {
                                let parentContents = try FileManager.default.contentsOfDirectory(atPath: parentURL.path)
                                print("HALDEBUG-DETECTION:    Parent directory contains \(parentContents.count) items")
                                for (index, item) in parentContents.prefix(10).enumerated() {
                                    print("HALDEBUG-DETECTION:       [\(index + 1)] \(item)")
                                }
                                if parentContents.count > 10 {
                                    print("HALDEBUG-DETECTION:       ... and \(parentContents.count - 10) more items")
                                }
                            } catch {
                                print("HALDEBUG-DETECTION:    ‚ùå Could not list parent directory: \(error.localizedDescription)")
                            }
                        }
                        
                        // Remove invalid path from storage
                        validPaths.removeValue(forKey: modelID)
                        print("HALDEBUG-DETECTION: ‚ùå Removed invalid path for: \(modelID)")
                    }
                }
                
                // Save cleaned paths if any were invalid
                if validPaths.count != paths.count {
                    self.downloadedPaths = validPaths
                }
                
                print("HALDEBUG-DETECTION: MLXModelDownloader.init() complete - \(self.downloadStates.count) models ready")
            }
            
            // Calculate cache size in background
            await self.updateCacheSize()
        }
    }
    
    // MARK: - Legacy Cleanup
    
    private func cleanupLegacyModelStorage() {
        // Remove old MLXModels directory from Application Support
        if FileManager.default.fileExists(atPath: legacyModelsDir.path) {
            do {
                try FileManager.default.removeItem(at: legacyModelsDir)
                print("HALDEBUG-CLEANUP: ‚úÖ Removed legacy MLXModels directory")
            } catch {
                print("HALDEBUG-CLEANUP: ‚ö†Ô∏è Failed to remove legacy directory: \(error.localizedDescription)")
            }
        }
        
        // Delete legacy single-model storage keys (don't migrate)
        let legacyKeys = [
            "downloadedMLXPath",
            "partialMLXDownloadProgress",
            "partialMLXDownloadSize",
            "hasPartialMLXDownload"
        ]
        
        for key in legacyKeys {
            if UserDefaults.standard.object(forKey: key) != nil {
                UserDefaults.standard.removeObject(forKey: key)
                print("HALDEBUG-CLEANUP: ‚úÖ Removed legacy key: \(key)")
            }
        }
    }
    
    // MARK: - Multi-Model Download Management
    
    func startDownload(modelID: String, repoID: String) async {
        // Check if already downloaded
        if isModelDownloaded(modelID) {
            await MainActor.run {
                var state = downloadStates[modelID] ?? DownloadState(
                    isDownloading: false,
                    progress: 1.0,
                    message: "Model already downloaded.",
                    error: nil,
                    localPath: getModelPath(modelID)
                )
                state.message = "Model already downloaded."
                downloadStates[modelID] = state
            }
            return
        }
        
        // If a download is in progress, queue this one
        await MainActor.run {
            if self.isDownloading {
                // Add to queue
                let queuedDownload = QueuedDownload(modelID: modelID, repoID: repoID)
                self.downloadQueue.append(queuedDownload)
                
                var state = self.downloadStates[modelID] ?? DownloadState(
                    isDownloading: false,
                    progress: 0.0,
                    message: "Queued (position \(self.downloadQueue.count))...",
                    error: nil,
                    localPath: nil
                )
                state.message = "Queued (position \(self.downloadQueue.count))..."
                self.downloadStates[modelID] = state
                
                print("HALDEBUG-DOWNLOAD: Queued \(modelID) at position \(self.downloadQueue.count)")
                return
            }
        }
        
        // No download in progress, start immediately
        await performDownload(modelID: modelID, repoID: repoID)
    }
    
    private func performDownload(modelID: String, repoID: String) async {
        #if !canImport(MLXLLM)
        await MainActor.run {
            var state = downloadStates[modelID] ?? DownloadState(
                isDownloading: false,
                progress: 0.0,
                message: "MLXLLM not available.",
                error: "MLXLLM not available. Add the MLX packages to the project.",
                localPath: nil
            )
            state.error = "MLXLLM not available."
            state.message = "Loader unavailable."
            downloadStates[modelID] = state
        }
        return
        #else
        
        await MainActor.run {
            self.currentDownloadModelID = modelID
            var state = self.downloadStates[modelID] ?? DownloadState(
                isDownloading: true,
                progress: 0.0,
                message: "Preparing download...",
                error: nil,
                localPath: nil
            )
            state.isDownloading = true
            state.progress = 0.0
            state.message = "Preparing download..."
            state.error = nil
            self.downloadStates[modelID] = state
        }
        
        currentDownloadTask = Task {
            do {
                // Download model using HubApi.snapshot()
                // This automatically handles resume if files are partially cached
                let hub = HubApi.default
                let snapshot = try await hub.snapshot(from: repoID, progressHandler: { progress in
                    guard progress.totalUnitCount > 0 else { return }
                    let p = Double(progress.completedUnitCount) / Double(progress.totalUnitCount)
                    Task { @MainActor in
                        if var state = self.downloadStates[modelID] {
                            state.progress = p
                            state.message = "Downloading... \(Int(p * 100))%"
                            self.downloadStates[modelID] = state
                        }
                    }
                })
                
                // Use Hub cache path directly - stable across app reinstalls
                let finalURL: URL = snapshot
                
                await MainActor.run {
                    print("HALDEBUG-DOWNLOAD: Download complete for \(modelID) at: \(finalURL.path)")
                    
                    // DIAGNOSTIC: Verify the path exists right after download
                    let pathExists = FileManager.default.fileExists(atPath: finalURL.path)
                    print("HALDEBUG-DOWNLOAD:    Path exists immediately after download: \(pathExists)")
                    print("HALDEBUG-DOWNLOAD:    finalURL.path: \(finalURL.path)")
                    print("HALDEBUG-DOWNLOAD:    finalURL.absoluteString: \(finalURL.absoluteString)")
                    
                    if pathExists {
                        var isDirectory: ObjCBool = false
                        FileManager.default.fileExists(atPath: finalURL.path, isDirectory: &isDirectory)
                        print("HALDEBUG-DOWNLOAD:    Is directory: \(isDirectory.boolValue)")
                        
                        if isDirectory.boolValue {
                            do {
                                let contents = try FileManager.default.contentsOfDirectory(atPath: finalURL.path)
                                print("HALDEBUG-DOWNLOAD:    Directory contains \(contents.count) files")
                                for (index, file) in contents.prefix(5).enumerated() {
                                    print("HALDEBUG-DOWNLOAD:       [\(index + 1)] \(file)")
                                }
                                if contents.count > 5 {
                                    print("HALDEBUG-DOWNLOAD:       ... and \(contents.count - 5) more files")
                                }
                            } catch {
                                print("HALDEBUG-DOWNLOAD:    ‚ùå Could not list directory: \(error.localizedDescription)")
                            }
                        }
                    } else {
                        print("HALDEBUG-DOWNLOAD:    ‚ö†Ô∏è WARNING: Path does NOT exist immediately after download!")
                    }
                    
                    // Save path to persistent storage
                    var paths = self.downloadedPaths
                    paths[modelID] = finalURL.path
                    self.downloadedPaths = paths
                    print("HALDEBUG-DOWNLOAD:    Saved path to UserDefaults: \(finalURL.path)")
                    
                    // Update state
                    var state = self.downloadStates[modelID] ?? DownloadState(
                        isDownloading: false,
                        progress: 1.0,
                        message: "Model ready.",
                        error: nil,
                        localPath: finalURL
                    )
                    state.isDownloading = false
                    state.progress = 1.0
                    state.message = "Model ready."
                    state.localPath = finalURL
                    self.downloadStates[modelID] = state
                    
                    // Clear current download tracking
                    self.currentDownloadTask = nil
                    self.currentDownloadModelID = nil
                    
                    // Update cache size
                    Task {
                        await self.updateCacheSize()
                    }
                    
                    // Notify observers
                    NotificationCenter.default.post(
                        name: .mlxModelDidDownload,
                        object: nil,
                        userInfo: ["modelID": modelID]
                    )
                    
                    print("HALDEBUG-DOWNLOAD: ‚úÖ Download completed successfully for \(modelID)")
                    
                    // Process next item in queue if any
                    self.processNextInQueue()
                }
            } catch is CancellationError {
                await MainActor.run {
                    if var state = self.downloadStates[modelID] {
                        state.isDownloading = false
                        state.message = "Download cancelled at \(Int(state.progress * 100))%"
                        state.error = "Cancelled"
                        self.downloadStates[modelID] = state
                    }
                    self.currentDownloadTask = nil
                    self.currentDownloadModelID = nil
                    
                    print("HALDEBUG-DOWNLOAD: Download cancelled for \(modelID)")
                    
                    // Process next item in queue if any
                    self.processNextInQueue()
                }
            } catch {
                await MainActor.run {
                    if var state = self.downloadStates[modelID] {
                        state.isDownloading = false
                        state.error = error.localizedDescription
                        state.message = "Download failed."
                        state.progress = 0.0
                        self.downloadStates[modelID] = state
                    }
                    self.currentDownloadTask = nil
                    self.currentDownloadModelID = nil
                    
                    print("HALDEBUG-DOWNLOAD: ‚ùå Download failed for \(modelID): \(error.localizedDescription)")
                    
                    // Process next item in queue if any
                    self.processNextInQueue()
                }
            }
        }
        #endif
    }
    
    private func processNextInQueue() {
        guard !downloadQueue.isEmpty else {
            print("HALDEBUG-QUEUE: Queue empty, no more downloads")
            return
        }
        
        let next = downloadQueue.removeFirst()
        print("HALDEBUG-QUEUE: Processing next download from queue: \(next.modelID)")
        
        Task {
            await performDownload(modelID: next.modelID, repoID: next.repoID)
        }
    }
    
    func cancelDownload(modelID: String) {
        guard currentDownloadModelID == modelID else {
            // If not currently downloading, remove from queue
            if let index = downloadQueue.firstIndex(where: { $0.modelID == modelID }) {
                downloadQueue.remove(at: index)
                if var state = downloadStates[modelID] {
                    state.message = "Removed from queue."
                    downloadStates[modelID] = state
                }
                print("HALDEBUG-DOWNLOAD: Removed \(modelID) from queue")
            }
            return
        }
        
        // Cancel active download
        currentDownloadTask?.cancel()
        currentDownloadTask = nil
        currentDownloadModelID = nil
        
        if var state = downloadStates[modelID] {
            state.isDownloading = false
            state.message = "Download cancelled at \(Int(state.progress * 100))%"
            state.error = "Cancelled"
            downloadStates[modelID] = state
        }
        
        print("HALDEBUG-DOWNLOAD: Cancelled active download for \(modelID)")
    }
    
    func deleteModel(modelID: String) async {
        guard let pathString = downloadedPaths[modelID] else {
            await MainActor.run {
                var state = self.downloadStates[modelID] ?? DownloadState(
                    isDownloading: false,
                    progress: 0.0,
                    message: "Model not found.",
                    error: nil,
                    localPath: nil
                )
                state.message = "Model not found."
                self.downloadStates[modelID] = state
            }
            return
        }
        
        let url = URL(fileURLWithPath: pathString)
        
        if FileManager.default.fileExists(atPath: url.path) {
            do {
                try FileManager.default.removeItem(at: url)
                print("HALDEBUG-DOWNLOAD: Model deleted from: \(url.path)")
                
                await MainActor.run {
                    // Remove from persistent storage
                    var paths = self.downloadedPaths
                    paths.removeValue(forKey: modelID)
                    self.downloadedPaths = paths
                    
                    // Update state
                    var state = self.downloadStates[modelID] ?? DownloadState(
                        isDownloading: false,
                        progress: 0.0,
                        message: "Model deleted.",
                        error: nil,
                        localPath: nil
                    )
                    state.localPath = nil
                    state.progress = 0.0
                    state.message = "Model deleted."
                    self.downloadStates[modelID] = state
                    
                    // Update cache size
                    Task {
                        await self.updateCacheSize()
                    }
                }
            } catch {
                await MainActor.run {
                    var state = self.downloadStates[modelID] ?? DownloadState(
                        isDownloading: false,
                        progress: 0.0,
                        message: "Delete failed.",
                        error: error.localizedDescription,
                        localPath: nil
                    )
                    state.error = "Delete failed: \(error.localizedDescription)"
                    state.message = "Delete failed."
                    self.downloadStates[modelID] = state
                }
            }
        } else {
            // File doesn't exist, clean up storage anyway
            await MainActor.run {
                var paths = self.downloadedPaths
                paths.removeValue(forKey: modelID)
                self.downloadedPaths = paths
                
                var state = self.downloadStates[modelID] ?? DownloadState(
                    isDownloading: false,
                    progress: 0.0,
                    message: "Model was already deleted.",
                    error: nil,
                    localPath: nil
                )
                state.message = "Model was already deleted."
                self.downloadStates[modelID] = state
            }
        }
    }
    
    func isModelDownloaded(_ modelID: String) -> Bool {
        guard let pathString = downloadedPaths[modelID] else { return false }
        return FileManager.default.fileExists(atPath: pathString)
    }
    
    func getModelPath(_ modelID: String) -> URL? {
        guard let pathString = downloadedPaths[modelID] else { return nil }
        let url = URL(fileURLWithPath: pathString)
        return FileManager.default.fileExists(atPath: url.path) ? url : nil
    }
    
    // MARK: - Cache Management
    
    @MainActor
    func updateCacheSize() async {
        isCacheCalculating = true
        
        let size = await calculateDirectorySize(hubCacheDirectory)
        
        hubCacheSize = size > 0 ? formatBytes(Int64(size)) : "No cache"
        isCacheCalculating = false
    }
    
    func clearHubCache() {
        if FileManager.default.fileExists(atPath: hubCacheDirectory.path) {
            do {
                try FileManager.default.removeItem(at: hubCacheDirectory)
                
                // Clear all model states since cache is gone
                downloadedPaths = [:]
                downloadStates = [:]
                hubCacheSize = "No cache"
                
                print("HALDEBUG-CACHE: ‚úÖ Cleared Hub cache and all model states")
            } catch {
                if var state = downloadStates.values.first {
                    state.error = "Failed to clear cache: \(error.localizedDescription)"
                    state.message = "Cache clear failed."
                }
                print("HALDEBUG-CACHE: ‚ùå Failed to clear cache: \(error.localizedDescription)")
            }
        } else {
            hubCacheSize = "No cache"
            print("HALDEBUG-CACHE: No cache directory found to clear")
        }
    }
    
    // MARK: - Utility Methods
    
    private func calculateDirectorySize(_ directory: URL) async -> UInt64 {
        return await withCheckedContinuation { continuation in
            Task.detached {
                var totalSize: UInt64 = 0
                
                guard FileManager.default.fileExists(atPath: directory.path) else {
                    continuation.resume(returning: 0)
                    return
                }
                
                let resourceKeys: [URLResourceKey] = [.totalFileAllocatedSizeKey, .isDirectoryKey]
                guard let enumerator = FileManager.default.enumerator(
                    at: directory,
                    includingPropertiesForKeys: resourceKeys,
                    options: [.skipsHiddenFiles]
                ) else {
                    continuation.resume(returning: 0)
                    return
                }
                
                while let fileURL = enumerator.nextObject() as? URL {
                    do {
                        let resourceValues = try fileURL.resourceValues(forKeys: Set(resourceKeys))
                        
                        // Only count files, not directories
                        if let isDirectory = resourceValues.isDirectory, !isDirectory {
                            if let fileSize = resourceValues.totalFileAllocatedSize {
                                totalSize += UInt64(fileSize)
                            }
                        }
                    } catch {
                        // Skip files we can't read
                        continue
                    }
                }
                
                continuation.resume(returning: totalSize)
            }
        }
    }
    
    private func formatBytes(_ bytes: Int64) -> String {
        let formatter = ByteCountFormatter()
        formatter.allowedUnits = [.useBytes, .useKB, .useMB, .useGB]
        formatter.countStyle = .file
        return formatter.string(fromByteCount: bytes)
    }
}

// MARK: - Notification
extension Notification.Name {
    static let mlxModelDidDownload = Notification.Name("mlxModelDidDownload")
}
// ==== LEGO END: 29 MLX Model Downloader (Singleton) ====



// ==== LEGO START: 30 Model Catalog Service (Hugging Face Integration) ====

// MARK: - Model Source Enum
enum ModelSource: String, Codable {
    case appleFoundation = "apple"
    case mlx = "mlx"
}

// MARK: - Model Configuration Struct
struct ModelConfiguration: Identifiable, Codable, Equatable, Hashable {
    let id: String
    let displayName: String
    let source: ModelSource
    let sizeGB: Double?
    let contextWindow: Int
    let license: String?
    let description: String?
    var isDownloaded: Bool
    var localPath: URL?
    
    var isLocal: Bool { source == .mlx }
    var requiresDownload: Bool { source == .mlx && !isDownloaded }
    
    func hash(into hasher: inout Hasher) {
        hasher.combine(id)
    }
    
    static func == (lhs: ModelConfiguration, rhs: ModelConfiguration) -> Bool {
        lhs.id == rhs.id
    }
    
    static let appleFoundation = ModelConfiguration(
        id: "apple-foundation-models",
        displayName: "Apple Intelligence",
        source: .appleFoundation,
        sizeGB: nil,
        contextWindow: 4_096,
        license: nil,
        description: "Always available, no download required",
        isDownloaded: true,
        localPath: nil
    )
}

// MARK: - Hugging Face API Response Models
struct HFModelListResponse: Codable {
    let id: String              // e.g. "mlx-community/Phi-3-mini-128k-instruct-4bit"
    let modelId: String?        // Alternative field name
    let author: String?
    let downloads: Int?
    let likes: Int?
    let tags: [String]?
    let siblings: [HFFileInfo]?
    let cardData: HFCardData?
    
    var repoID: String {
        return modelId ?? id
    }
}

struct HFFileInfo: Codable {
    let rfilename: String
    let size: Int64?
}

struct HFCardData: Codable {
    let license: String?
    let tags: [String]?
}

// MARK: - Model Config.json Response (for context window detection)

// Different model architectures store context window size in different fields within config.json
// This struct defines all known field names we check, in order of prevalence:
// - max_position_embeddings: Most common (Llama, Mistral, most modern models)
// - sliding_window: Models with sliding window attention (some Mistral variants)
// - n_positions: GPT-2 and variants
// - n_ctx: Alternative context naming
// - seq_len, seq_length: Older or alternative naming conventions
struct HFModelConfig: Codable {
    let max_position_embeddings: Int?
    let n_positions: Int?
    let seq_len: Int?
    let seq_length: Int?
    let n_ctx: Int?
    let sliding_window: Int?
}

// MARK: - Model Catalog Service (Singleton)

// SELF-AWARENESS NOTE (for Hal reading this code):
// This service manages the catalog of available AI models - both Apple Foundation Models
// and the 1000+ MLX models from HuggingFace. A critical challenge is determining each model's
// context window (how much text it can process at once).
//
// HuggingFace doesn't provide this in their API and doesn't require it in uploads (!), so we use
// a three-tier strategy:
// - TIER 1: Fetch the model's config.json and check for context window fields (accurate but requires network)
// - TIER 2: Infer from model name ("128k", "32k", etc.) - heuristic but fast
// - TIER 3: Safe default of 4,096 tokens (same as Apple Foundation Models)
//
// We cache results locally so we don't re-fetch, and this cache survives app deletion.
// This matters because models can be removed/reinstalled, and we need this info multiple times
// when a model is active (for RAG limits, memory depth, prompt sizing, etc.).
//
// The config.json approach checks multiple field names because different model architectures
// use different conventions: max_position_embeddings (Llama/Mistral), n_positions (GPT-2),
// sliding_window (models with sliding attention), and others. We check them in order of
// prevalence based on research into common practices.

@MainActor
class ModelCatalogService: ObservableObject {
    static let shared = ModelCatalogService()
    
    // Published state
    @Published var availableModels: [ModelConfiguration] = []
    @Published var isLoading: Bool = false
    @Published var errorMessage: String?
    
    // API configuration
    private let huggingFaceAPIBase = "https://huggingface.co/api"
    private let mlxCommunityOrg = "mlx-community"
    
    // License acceptance tracking
    @AppStorage("acceptedModelLicenses") private var acceptedLicensesData: Data = Data()
    private var acceptedLicenses: [String: Bool] {
        get {
            (try? JSONDecoder().decode([String: Bool].self, from: acceptedLicensesData)) ?? [:]
        }
        set {
            acceptedLicensesData = (try? JSONEncoder().encode(newValue)) ?? Data()
        }
    }
    
    // MARK: - Context Window Cache
    
    // Cache of discovered context windows: [modelID: contextWindow]
    // Survives app deletion, prevents re-fetching, handles model reinstalls
    @AppStorage("cachedContextWindows") private var cachedContextData: Data = Data()
    private var cachedContextWindows: [String: Int] {
        get {
            (try? JSONDecoder().decode([String: Int].self, from: cachedContextData)) ?? [:]
        }
        set {
            cachedContextData = (try? JSONEncoder().encode(newValue)) ?? Data()
        }
    }
    
    /// Retrieves cached context window for a model, if available
    private func getCachedContextWindow(for modelID: String) -> Int? {
        return cachedContextWindows[modelID]
    }
    
    /// Stores context window in cache for future use
    private func cacheContextWindow(_ contextWindow: Int, for modelID: String) {
        var cache = cachedContextWindows
        cache[modelID] = contextWindow
        cachedContextWindows = cache
        print("HALDEBUG-CONTEXT: Cached context window \(contextWindow) for \(modelID)")
    }
    
    private init() {
        print("HALDEBUG-CATALOG: ModelCatalogService initialized")
    }
    
    // MARK: - Fetch Models from Hugging Face
    
    /// Fetches all models from the mlx-community organization
    func fetchMLXCommunityModels() async {
        await MainActor.run {
            isLoading = true
            errorMessage = nil
        }
        
        print("HALDEBUG-CATALOG: Fetching models from mlx-community...")
        
        do {
            // Build API URL
            guard let url = URL(string: "\(huggingFaceAPIBase)/models?author=\(mlxCommunityOrg)") else {
                throw CatalogError.invalidURL
            }
            
            print("HALDEBUG-CATALOG: API URL: \(url.absoluteString)")
            
            // Make request
            let (data, response) = try await URLSession.shared.data(from: url)
            
            // Validate response
            guard let httpResponse = response as? HTTPURLResponse else {
                throw CatalogError.invalidResponse
            }
            
            print("HALDEBUG-CATALOG: HTTP Status: \(httpResponse.statusCode)")
            
            guard httpResponse.statusCode == 200 else {
                throw CatalogError.httpError(httpResponse.statusCode)
            }
            
            // Parse JSON
            let decoder = JSONDecoder()
            let hfModels = try decoder.decode([HFModelListResponse].self, from: data)
            
            print("HALDEBUG-CATALOG: Received \(hfModels.count) models from API")
            
            // Convert to ModelConfiguration objects (now async to support config.json fetching)
            var mlxModels: [ModelConfiguration] = []
            for hfModel in hfModels {
                if let model = await convertHFModelToConfiguration(hfModel) {
                    mlxModels.append(model)
                }
            }
            
            print("HALDEBUG-CATALOG: Converted \(mlxModels.count) valid models")
            
            // Add Apple Foundation Models at the top
            let appleModel = ModelConfiguration.appleFoundation
            
            await MainActor.run {
                self.availableModels = [appleModel] + mlxModels
                self.isLoading = false
                print("HALDEBUG-CATALOG: ‚úÖ Catalog updated with \(self.availableModels.count) total models")
            }
            
        } catch {
            await MainActor.run {
                self.errorMessage = "Failed to load models: \(error.localizedDescription)"
                self.isLoading = false
                
                // Fallback to Apple Foundation Models only
                self.availableModels = [ModelConfiguration.appleFoundation]
                
                print("HALDEBUG-CATALOG: ‚ùå Error: \(error.localizedDescription)")
            }
        }
    }
    
    // MARK: - Convert HF Model to ModelConfiguration
    
    private func convertHFModelToConfiguration(_ hfModel: HFModelListResponse) async -> ModelConfiguration? {
        let repoID = hfModel.repoID
        
        // Extract display name from repo ID (e.g. "Phi-3-mini-128k-instruct-4bit")
        let displayName = repoID.replacingOccurrences(of: "mlx-community/", with: "")
            .replacingOccurrences(of: "-", with: " ")
            .capitalized
        
        // Calculate total size from file list
        let totalBytes = hfModel.siblings?.reduce(Int64(0)) { sum, file in
            sum + (file.size ?? 0)
        } ?? 0
        
        let sizeGB = totalBytes > 0 ? Double(totalBytes) / 1_073_741_824.0 : nil
        
        // Determine context window using three-tier strategy
        let contextWindow: Int
        var detectionMethod: String = "" // For transparency logging
        
        // Check cache first
        if let cached = getCachedContextWindow(for: repoID) {
            contextWindow = cached
            detectionMethod = "cached"
            print("HALDEBUG-CONTEXT: Using cached context window \(contextWindow) for \(repoID)")
        } else {
            // TIER 1: Try fetching from config.json
            if let fetched = await fetchConfigContextWindow(for: repoID) {
                contextWindow = fetched
                detectionMethod = "config.json"
            } else {
                // TIER 2: Fall back to name inference
                contextWindow = inferContextFromName(repoID)
                detectionMethod = repoID.lowercased().contains("128k") ||
                                 repoID.lowercased().contains("32k") ||
                                 repoID.lowercased().contains("8k") ? "name_inference" : "default"
            }
            
            // Cache the result (regardless of which tier succeeded)
            cacheContextWindow(contextWindow, for: repoID)
            
            // Log detection method for transparency
            print("HALDEBUG-CONTEXT: Context window \(contextWindow) detected via \(detectionMethod) for \(repoID)")
        }
        
        // Extract license
        let license = hfModel.cardData?.license
        
        // Check if already downloaded
        let downloadManager = MLXModelDownloader.shared
        let isDownloaded = downloadManager.isModelDownloaded(repoID)
        let localPath = downloadManager.getModelPath(repoID)
        
        return ModelConfiguration(
            id: repoID,
            displayName: displayName,
            source: .mlx,
            sizeGB: sizeGB,
            contextWindow: contextWindow,
            license: license,
            description: nil,
            isDownloaded: isDownloaded,
            localPath: localPath
        )
    }
    
    // MARK: - Context Window Inference
    
    /// TIER 1: Fetch context window from model's config.json
    /// This is the most accurate method as it reads the official model metadata.
    /// Attempts to download and parse config.json from HuggingFace with a 5-second timeout.
    /// Checks multiple field names because different architectures use different conventions.
    /// Returns nil on any failure (missing file, timeout, no recognized fields) to gracefully fall back to Tier 2.
    private func fetchConfigContextWindow(for repoID: String) async -> Int? {
        print("HALDEBUG-CONTEXT: Fetching config.json for \(repoID)")
        
        // Build URL to config.json
        guard let url = URL(string: "https://huggingface.co/\(repoID)/raw/main/config.json") else {
            print("HALDEBUG-CONTEXT: Invalid config.json URL for \(repoID)")
            return nil
        }
        
        do {
            // Create request with timeout
            var request = URLRequest(url: url)
            request.timeoutInterval = 5.0  // 5 second timeout
            
            let (data, response) = try await URLSession.shared.data(for: request)
            
            // Validate response
            guard let httpResponse = response as? HTTPURLResponse,
                  httpResponse.statusCode == 200 else {
                print("HALDEBUG-CONTEXT: Config.json not found or HTTP error for \(repoID)")
                return nil
            }
            
            // Parse JSON
            let decoder = JSONDecoder()
            let config = try decoder.decode(HFModelConfig.self, from: data)
            
            // Check fields in order of preference (most common first)
            // Based on research: different architectures use different field names
            if let context = config.max_position_embeddings {
                print("HALDEBUG-CONTEXT: ‚úÖ Found context window \(context) in max_position_embeddings for \(repoID)")
                return context
            } else if let context = config.sliding_window {
                print("HALDEBUG-CONTEXT: ‚úÖ Found sliding window \(context) for \(repoID)")
                return context
            } else if let context = config.n_positions {
                print("HALDEBUG-CONTEXT: ‚úÖ Found context window \(context) in n_positions for \(repoID)")
                return context
            } else if let context = config.n_ctx {
                print("HALDEBUG-CONTEXT: ‚úÖ Found context window \(context) in n_ctx for \(repoID)")
                return context
            } else if let context = config.seq_len {
                print("HALDEBUG-CONTEXT: ‚úÖ Found context window \(context) in seq_len for \(repoID)")
                return context
            } else if let context = config.seq_length {
                print("HALDEBUG-CONTEXT: ‚úÖ Found context window \(context) in seq_length for \(repoID)")
                return context
            } else {
                print("HALDEBUG-CONTEXT: Config.json found but no context window fields for \(repoID)")
                return nil
            }
            
        } catch {
            print("HALDEBUG-CONTEXT: Failed to fetch config.json for \(repoID): \(error.localizedDescription)")
            return nil
        }
    }
    
    /// TIER 2: Infer context window from model name patterns
    /// This is a heuristic fallback when config.json isn't available or doesn't contain context info.
    /// Looks for common patterns like "128k", "32k", "8k" in the model repository ID.
    /// Falls back to TIER 3 (4,096 tokens - safe default) if no pattern matches.
    /// While less accurate than config.json, this works surprisingly well as model creators
    /// typically include context window size in model names for marketing/clarity.
    private func inferContextFromName(_ repoID: String) -> Int {
        let id = repoID.lowercased()
        
        // Check for common context window patterns in model names
        if id.contains("128k") {
            print("HALDEBUG-CONTEXT: Inferred 128k context from name: \(repoID)")
            return 128_000
        } else if id.contains("32k") {
            print("HALDEBUG-CONTEXT: Inferred 32k context from name: \(repoID)")
            return 32_000
        } else if id.contains("8k") {
            print("HALDEBUG-CONTEXT: Inferred 8k context from name: \(repoID)")
            return 8_000
        } else {
            // TIER 3: Safe default (same as Apple Foundation Models)
            print("HALDEBUG-CONTEXT: Using safe default 4k context for: \(repoID)")
            return 4_096
        }
    }
    
    // MARK: - License Management
    
    /// Fetches the full license text for a model from its model card
    func fetchLicenseText(for modelID: String) async throws -> String {
        print("HALDEBUG-CATALOG: Fetching license for \(modelID)")
        
        // Build URL to model card
        guard let url = URL(string: "https://huggingface.co/\(modelID)/raw/main/README.md") else {
            throw CatalogError.invalidURL
        }
        
        let (data, response) = try await URLSession.shared.data(from: url)
        
        guard let httpResponse = response as? HTTPURLResponse,
              httpResponse.statusCode == 200 else {
            throw CatalogError.licenseNotFound
        }
        
        guard let licenseText = String(data: data, encoding: .utf8) else {
            throw CatalogError.invalidLicenseFormat
        }
        
        print("HALDEBUG-CATALOG: ‚úÖ License fetched (\(licenseText.count) characters)")
        return licenseText
    }
    
    /// Records that user accepted the license for a model
    func acceptLicense(for modelID: String) {
        var licenses = acceptedLicenses
        licenses[modelID] = true
        acceptedLicenses = licenses
        print("HALDEBUG-CATALOG: License accepted for \(modelID)")
    }
    
    /// Checks if user has accepted the license for a model
    func hasAcceptedLicense(for modelID: String) -> Bool {
        return acceptedLicenses[modelID] ?? false
    }
    
    /// Revokes license acceptance (e.g., if user deletes model)
    func revokeLicense(for modelID: String) {
        var licenses = acceptedLicenses
        licenses[modelID] = nil
        acceptedLicenses = licenses
        print("HALDEBUG-CATALOG: License revoked for \(modelID)")
    }
    
    // MARK: - Model Lookup
    
    /// Returns only models that are available locally (downloaded or always-available like Apple Foundation)
    /// Used by Salon Mode and other features that need to show only usable models
    var downloadedModels: [ModelConfiguration] {
        return availableModels
            .filter { $0.source == .appleFoundation || ($0.source == .mlx && $0.isDownloaded) }
            .sorted { model1, model2 in
                // Apple Foundation first, then alphabetical
                if model1.source == .appleFoundation { return true }
                if model2.source == .appleFoundation { return false }
                return model1.displayName < model2.displayName
            }
    }
    
    /// Finds a model by ID in the current catalog
    func getModel(byID modelID: String) -> ModelConfiguration? {
        return availableModels.first { $0.id == modelID }
    }
    
    /// Refreshes the download status for all models in catalog
    func refreshDownloadStates() {
        let downloadManager = MLXModelDownloader.shared
        
        availableModels = availableModels.map { model in
            var updated = model
            updated.isDownloaded = downloadManager.isModelDownloaded(model.id)
            updated.localPath = downloadManager.getModelPath(model.id)
            return updated
        }
        
        print("HALDEBUG-CATALOG: Refreshed download states")
    }
}

// MARK: - Catalog Errors
enum CatalogError: LocalizedError {
    case invalidURL
    case invalidResponse
    case httpError(Int)
    case licenseNotFound
    case invalidLicenseFormat
    
    var errorDescription: String? {
        switch self {
        case .invalidURL:
            return "Invalid Hugging Face API URL"
        case .invalidResponse:
            return "Invalid response from Hugging Face"
        case .httpError(let code):
            return "HTTP error \(code) from Hugging Face API"
        case .licenseNotFound:
            return "Model license not found"
        case .invalidLicenseFormat:
            return "License text could not be decoded"
        }
    }
}

// ==== LEGO END: 30 Model Catalog Service (Hugging Face Integration) ====
